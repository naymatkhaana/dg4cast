Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=8, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.647162675857544
Epoch: 1, Steps: 46 | Train Loss: 0.3734261 Vali Loss: 0.1858417 Test Loss: 0.1858417
Validation loss decreased (inf --> 0.185842).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0678668022155762
Epoch: 2, Steps: 46 | Train Loss: 0.2860136 Vali Loss: 0.1635630 Test Loss: 0.1635630
Validation loss decreased (0.185842 --> 0.163563).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9790072441101074
Epoch: 3, Steps: 46 | Train Loss: 0.2599951 Vali Loss: 0.1574723 Test Loss: 0.1574723
Validation loss decreased (0.163563 --> 0.157472).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0270628929138184
Epoch: 4, Steps: 46 | Train Loss: 0.2419937 Vali Loss: 0.1549953 Test Loss: 0.1549953
Validation loss decreased (0.157472 --> 0.154995).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0905861854553223
Epoch: 5, Steps: 46 | Train Loss: 0.2379785 Vali Loss: 0.1526527 Test Loss: 0.1526527
Validation loss decreased (0.154995 --> 0.152653).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9978973865509033
Epoch: 6, Steps: 46 | Train Loss: 0.2459004 Vali Loss: 0.1521527 Test Loss: 0.1521527
Validation loss decreased (0.152653 --> 0.152153).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9964087009429932
Epoch: 7, Steps: 46 | Train Loss: 0.2424484 Vali Loss: 0.1527306 Test Loss: 0.1527306
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.1137454509735107
Epoch: 8, Steps: 46 | Train Loss: 0.2318761 Vali Loss: 0.1522626 Test Loss: 0.1522626
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0915441513061523
Epoch: 9, Steps: 46 | Train Loss: 0.2380266 Vali Loss: 0.1514337 Test Loss: 0.1514337
Validation loss decreased (0.152153 --> 0.151434).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9943497180938721
Epoch: 10, Steps: 46 | Train Loss: 0.2425553 Vali Loss: 0.1517018 Test Loss: 0.1517018
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.019397130236029625, mae:0.10302376002073288, smape:10.279785096645355, dtw:not calculated
horizon:2 mse:0.044004276394844055, mae:0.16216343641281128, smape:16.119632124900818, dtw:not calculated
horizon:3 mse:0.07790233194828033, mae:0.20196279883384705, smape:19.910137355327606, dtw:not calculated
horizon:4 mse:0.10511849820613861, mae:0.23827551305294037, smape:23.37314933538437, dtw:not calculated
horizon:5 mse:0.129404217004776, mae:0.270454078912735, smape:26.43590271472931, dtw:not calculated
horizon:6 mse:0.14554855227470398, mae:0.28545472025871277, smape:27.854937314987183, dtw:not calculated
horizon:7 mse:0.16782796382904053, mae:0.31516844034194946, smape:30.67328631877899, dtw:not calculated
horizon:8 mse:0.2005932629108429, mae:0.34467169642448425, smape:33.40727090835571, dtw:not calculated
horizon:9 mse:0.20667371153831482, mae:0.34620746970176697, smape:33.495694398880005, dtw:not calculated
horizon:10 mse:0.23531433939933777, mae:0.36967337131500244, smape:35.62404811382294, dtw:not calculated
horizon:11 mse:0.23742352426052094, mae:0.37452957034111023, smape:36.062291264534, dtw:not calculated
horizon:12 mse:0.24799686670303345, mae:0.3800671696662903, smape:36.51416599750519, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08689583092927933, mae:0.21022239327430725, smape:20.662258565425873, dtw:not calculated
average metrics: horizon upto:12 mse:0.15143372118473053, mae:0.28263765573501587, smape:27.47919261455536, dtw:not calculated
===============================================================================
average of horizons: mse:0.15143372118473053, mae:0.28263765573501587, smape:27.47919261455536, dtw:not calculated
mean smape over horizons:  27.479191745320957
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=8, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.6073780059814453
Epoch: 1, Steps: 46 | Train Loss: 0.4565473 Vali Loss: 0.1926210 Test Loss: 0.1926210
Validation loss decreased (inf --> 0.192621).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0291309356689453
Epoch: 2, Steps: 46 | Train Loss: 0.3107285 Vali Loss: 0.1810686 Test Loss: 0.1810686
Validation loss decreased (0.192621 --> 0.181069).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1148536205291748
Epoch: 3, Steps: 46 | Train Loss: 0.2870801 Vali Loss: 0.1693965 Test Loss: 0.1693965
Validation loss decreased (0.181069 --> 0.169396).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0774266719818115
Epoch: 4, Steps: 46 | Train Loss: 0.2720874 Vali Loss: 0.1676490 Test Loss: 0.1676490
Validation loss decreased (0.169396 --> 0.167649).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.1192224025726318
Epoch: 5, Steps: 46 | Train Loss: 0.2617879 Vali Loss: 0.1679452 Test Loss: 0.1679452
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0608530044555664
Epoch: 6, Steps: 46 | Train Loss: 0.2671488 Vali Loss: 0.1697155 Test Loss: 0.1697155
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.060598373413086
Epoch: 7, Steps: 46 | Train Loss: 0.2619267 Vali Loss: 0.1677312 Test Loss: 0.1677312
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.024767639115452766, mae:0.12151439487934113, smape:12.122932076454163, dtw:not calculated
horizon:2 mse:0.0751536637544632, mae:0.21449023485183716, smape:21.227319538593292, dtw:not calculated
horizon:3 mse:0.09698744863271713, mae:0.2334926873445511, smape:22.968322038650513, dtw:not calculated
horizon:4 mse:0.14977997541427612, mae:0.3054969608783722, smape:29.87966537475586, dtw:not calculated
horizon:5 mse:0.15615327656269073, mae:0.3046065866947174, smape:29.707875847816467, dtw:not calculated
horizon:6 mse:0.1803406924009323, mae:0.31956690549850464, smape:31.020578742027283, dtw:not calculated
horizon:7 mse:0.19733218848705292, mae:0.34827110171318054, smape:33.81099998950958, dtw:not calculated
horizon:8 mse:0.21056802570819855, mae:0.35003525018692017, smape:33.84363055229187, dtw:not calculated
horizon:9 mse:0.21911558508872986, mae:0.3668746054172516, smape:35.51577031612396, dtw:not calculated
horizon:10 mse:0.21472561359405518, mae:0.3645266890525818, smape:35.31034290790558, dtw:not calculated
horizon:11 mse:0.22713324427604675, mae:0.3793925642967224, smape:36.72136068344116, dtw:not calculated
horizon:12 mse:0.2597309648990631, mae:0.39443090558052063, smape:37.862780690193176, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.11386378109455109, mae:0.24986132979393005, smape:24.487780034542084, dtw:not calculated
average metrics: horizon upto:12 mse:0.167649045586586, mae:0.30855825543403625, smape:29.999297857284546, dtw:not calculated
===============================================================================
average of horizons: mse:0.167649045586586, mae:0.30855825543403625, smape:29.999297857284546, dtw:not calculated
mean smape over horizons:  29.999298229813576
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=8, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4375617504119873
Epoch: 1, Steps: 46 | Train Loss: 0.4424028 Vali Loss: 0.1848863 Test Loss: 0.1848863
Validation loss decreased (inf --> 0.184886).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1000869274139404
Epoch: 2, Steps: 46 | Train Loss: 0.2947006 Vali Loss: 0.1795062 Test Loss: 0.1795062
Validation loss decreased (0.184886 --> 0.179506).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1254746913909912
Epoch: 3, Steps: 46 | Train Loss: 0.2858176 Vali Loss: 0.1746440 Test Loss: 0.1746440
Validation loss decreased (0.179506 --> 0.174644).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1445887088775635
Epoch: 4, Steps: 46 | Train Loss: 0.2663960 Vali Loss: 0.1752040 Test Loss: 0.1752040
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0999174118041992
Epoch: 5, Steps: 46 | Train Loss: 0.2621738 Vali Loss: 0.1740747 Test Loss: 0.1740747
Validation loss decreased (0.174644 --> 0.174075).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9645335674285889
Epoch: 6, Steps: 46 | Train Loss: 0.2711594 Vali Loss: 0.1718689 Test Loss: 0.1718689
Validation loss decreased (0.174075 --> 0.171869).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0122528076171875
Epoch: 7, Steps: 46 | Train Loss: 0.2593971 Vali Loss: 0.1711729 Test Loss: 0.1711729
Validation loss decreased (0.171869 --> 0.171173).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.1105999946594238
Epoch: 8, Steps: 46 | Train Loss: 0.2578057 Vali Loss: 0.1711371 Test Loss: 0.1711371
Validation loss decreased (0.171173 --> 0.171137).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.090348482131958
Epoch: 9, Steps: 46 | Train Loss: 0.2483971 Vali Loss: 0.1704692 Test Loss: 0.1704692
Validation loss decreased (0.171137 --> 0.170469).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.071899175643921
Epoch: 10, Steps: 46 | Train Loss: 0.2536469 Vali Loss: 0.1706736 Test Loss: 0.1706736
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.036710068583488464, mae:0.14351758360862732, smape:14.279986917972565, dtw:not calculated
horizon:2 mse:0.07137957960367203, mae:0.20105841755867004, smape:19.870594143867493, dtw:not calculated
horizon:3 mse:0.10538046807050705, mae:0.24663451313972473, smape:24.233023822307587, dtw:not calculated
horizon:4 mse:0.1124410405755043, mae:0.23817691206932068, smape:23.293979465961456, dtw:not calculated
horizon:5 mse:0.15827976167201996, mae:0.29919061064720154, smape:29.129189252853394, dtw:not calculated
horizon:6 mse:0.1976429671049118, mae:0.33600500226020813, smape:32.49874711036682, dtw:not calculated
horizon:7 mse:0.20720148086547852, mae:0.34253737330436707, smape:33.07945132255554, dtw:not calculated
horizon:8 mse:0.21578773856163025, mae:0.35132598876953125, smape:33.91466438770294, dtw:not calculated
horizon:9 mse:0.22355452179908752, mae:0.36779138445854187, smape:35.533225536346436, dtw:not calculated
horizon:10 mse:0.23894102871418, mae:0.38666802644729614, smape:37.32137978076935, dtw:not calculated
horizon:11 mse:0.23713353276252747, mae:0.3787035346031189, smape:36.53047978878021, dtw:not calculated
horizon:12 mse:0.24117806553840637, mae:0.3842656910419464, smape:37.015751004219055, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.1136389747262001, mae:0.24409717321395874, smape:23.884253203868866, dtw:not calculated
average metrics: horizon upto:12 mse:0.17046917974948883, mae:0.30632293224334717, smape:29.72503900527954, dtw:not calculated
===============================================================================
average of horizons: mse:0.17046917974948883, mae:0.30632293224334717, smape:29.72503900527954, dtw:not calculated
mean smape over horizons:  29.72503937780857
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=10, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4601871967315674
Epoch: 1, Steps: 46 | Train Loss: 0.4436420 Vali Loss: 0.2034803 Test Loss: 0.2034803
Validation loss decreased (inf --> 0.203480).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.2002639770507812
Epoch: 2, Steps: 46 | Train Loss: 0.2916798 Vali Loss: 0.2128850 Test Loss: 0.2128850
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0461971759796143
Epoch: 3, Steps: 46 | Train Loss: 0.2652239 Vali Loss: 0.1715655 Test Loss: 0.1715655
Validation loss decreased (0.203480 --> 0.171566).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0297214984893799
Epoch: 4, Steps: 46 | Train Loss: 0.2604135 Vali Loss: 0.1723769 Test Loss: 0.1723769
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0180637836456299
Epoch: 5, Steps: 46 | Train Loss: 0.2611414 Vali Loss: 0.1695638 Test Loss: 0.1695638
Validation loss decreased (0.171566 --> 0.169564).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0537819862365723
Epoch: 6, Steps: 46 | Train Loss: 0.2619888 Vali Loss: 0.1709291 Test Loss: 0.1709291
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9601342678070068
Epoch: 7, Steps: 46 | Train Loss: 0.2584807 Vali Loss: 0.1709569 Test Loss: 0.1709569
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.1346631050109863
Epoch: 8, Steps: 46 | Train Loss: 0.2480614 Vali Loss: 0.1700651 Test Loss: 0.1700651
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03922922536730766, mae:0.15086524188518524, smape:15.00953584909439, dtw:not calculated
horizon:2 mse:0.0657174214720726, mae:0.19119778275489807, smape:18.92731487751007, dtw:not calculated
horizon:3 mse:0.11521297693252563, mae:0.24331346154212952, smape:23.82127046585083, dtw:not calculated
horizon:4 mse:0.12211665511131287, mae:0.2664894163608551, smape:26.140379905700684, dtw:not calculated
horizon:5 mse:0.15468820929527283, mae:0.3008672595024109, smape:29.339700937271118, dtw:not calculated
horizon:6 mse:0.17003251612186432, mae:0.31710124015808105, smape:30.876410007476807, dtw:not calculated
horizon:7 mse:0.20522786676883698, mae:0.3587454557418823, smape:34.8208487033844, dtw:not calculated
horizon:8 mse:0.21663261950016022, mae:0.3584747612476349, smape:34.66203212738037, dtw:not calculated
horizon:9 mse:0.22962547838687897, mae:0.3714136779308319, smape:35.863760113716125, dtw:not calculated
horizon:10 mse:0.22718805074691772, mae:0.3775775134563446, smape:36.53629422187805, dtw:not calculated
horizon:11 mse:0.2412462830543518, mae:0.39698389172554016, smape:38.39396834373474, dtw:not calculated
horizon:12 mse:0.24784836173057556, mae:0.3929552435874939, smape:37.888747453689575, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.11116616427898407, mae:0.24497239291667938, smape:24.01910275220871, dtw:not calculated
average metrics: horizon upto:12 mse:0.16956380009651184, mae:0.31049874424934387, smape:30.19002377986908, dtw:not calculated
===============================================================================
average of horizons: mse:0.16956380009651184, mae:0.31049874424934387, smape:30.19002377986908, dtw:not calculated
mean smape over horizons:  30.19002191722393
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=10, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3860106468200684
Epoch: 1, Steps: 46 | Train Loss: 0.4190796 Vali Loss: 0.1941327 Test Loss: 0.1941327
Validation loss decreased (inf --> 0.194133).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0811891555786133
Epoch: 2, Steps: 46 | Train Loss: 0.3014321 Vali Loss: 0.1769290 Test Loss: 0.1769290
Validation loss decreased (0.194133 --> 0.176929).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0040385723114014
Epoch: 3, Steps: 46 | Train Loss: 0.2888125 Vali Loss: 0.1688649 Test Loss: 0.1688649
Validation loss decreased (0.176929 --> 0.168865).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9970724582672119
Epoch: 4, Steps: 46 | Train Loss: 0.2583191 Vali Loss: 0.1620237 Test Loss: 0.1620237
Validation loss decreased (0.168865 --> 0.162024).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0514118671417236
Epoch: 5, Steps: 46 | Train Loss: 0.2463298 Vali Loss: 0.1622635 Test Loss: 0.1622635
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0701045989990234
Epoch: 6, Steps: 46 | Train Loss: 0.2543376 Vali Loss: 0.1623141 Test Loss: 0.1623141
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0705146789550781
Epoch: 7, Steps: 46 | Train Loss: 0.2526738 Vali Loss: 0.1624491 Test Loss: 0.1624491
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02370998077094555, mae:0.11688729375600815, smape:11.65621429681778, dtw:not calculated
horizon:2 mse:0.05359876900911331, mae:0.17415757477283478, smape:17.269618809223175, dtw:not calculated
horizon:3 mse:0.08721813559532166, mae:0.22456979751586914, smape:22.150972485542297, dtw:not calculated
horizon:4 mse:0.12344478815793991, mae:0.27152517437934875, smape:26.63217782974243, dtw:not calculated
horizon:5 mse:0.1387816220521927, mae:0.27765271067619324, smape:27.10111141204834, dtw:not calculated
horizon:6 mse:0.1761927753686905, mae:0.3197437822818756, smape:31.075695157051086, dtw:not calculated
horizon:7 mse:0.1906535029411316, mae:0.3218168616294861, smape:31.13696277141571, dtw:not calculated
horizon:8 mse:0.20966699719429016, mae:0.3550676703453064, smape:34.38746631145477, dtw:not calculated
horizon:9 mse:0.20359642803668976, mae:0.3409327268600464, smape:32.9890251159668, dtw:not calculated
horizon:10 mse:0.25260162353515625, mae:0.3908630907535553, smape:37.62657344341278, dtw:not calculated
horizon:11 mse:0.24203155934810638, mae:0.3882105052471161, smape:37.469881772994995, dtw:not calculated
horizon:12 mse:0.2427883744239807, mae:0.38785862922668457, smape:37.40188777446747, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.1004910096526146, mae:0.23075607419013977, smape:22.64763116836548, dtw:not calculated
average metrics: horizon upto:12 mse:0.16202370822429657, mae:0.29744046926498413, smape:28.908130526542664, dtw:not calculated
===============================================================================
average of horizons: mse:0.16202370822429657, mae:0.29744046926498413, smape:28.908130526542664, dtw:not calculated
mean smape over horizons:  28.90813226501147
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=10, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4153542518615723
Epoch: 1, Steps: 46 | Train Loss: 0.4303357 Vali Loss: 0.1781115 Test Loss: 0.1781115
Validation loss decreased (inf --> 0.178112).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.035315752029419
Epoch: 2, Steps: 46 | Train Loss: 0.2971984 Vali Loss: 0.1703304 Test Loss: 0.1703304
Validation loss decreased (0.178112 --> 0.170330).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0750126838684082
Epoch: 3, Steps: 46 | Train Loss: 0.2714120 Vali Loss: 0.1654979 Test Loss: 0.1654979
Validation loss decreased (0.170330 --> 0.165498).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0899889469146729
Epoch: 4, Steps: 46 | Train Loss: 0.2649361 Vali Loss: 0.1588336 Test Loss: 0.1588336
Validation loss decreased (0.165498 --> 0.158834).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0333902835845947
Epoch: 5, Steps: 46 | Train Loss: 0.2625914 Vali Loss: 0.1618866 Test Loss: 0.1618866
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.20033860206604
Epoch: 6, Steps: 46 | Train Loss: 0.2633665 Vali Loss: 0.1628349 Test Loss: 0.1628349
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0398516654968262
Epoch: 7, Steps: 46 | Train Loss: 0.2678960 Vali Loss: 0.1614816 Test Loss: 0.1614816
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.035158585757017136, mae:0.13624697923660278, smape:13.54781985282898, dtw:not calculated
horizon:2 mse:0.05610574036836624, mae:0.17648546397686005, smape:17.49263107776642, dtw:not calculated
horizon:3 mse:0.09302552044391632, mae:0.22351853549480438, smape:21.96807861328125, dtw:not calculated
horizon:4 mse:0.12192793190479279, mae:0.2582329213619232, smape:25.273731350898743, dtw:not calculated
horizon:5 mse:0.1310112327337265, mae:0.27421218156814575, smape:26.833796501159668, dtw:not calculated
horizon:6 mse:0.1596430540084839, mae:0.3036264181137085, smape:29.586556553840637, dtw:not calculated
horizon:7 mse:0.1853107213973999, mae:0.34188300371170044, smape:33.249831199645996, dtw:not calculated
horizon:8 mse:0.225942924618721, mae:0.3783150911331177, smape:36.61904335021973, dtw:not calculated
horizon:9 mse:0.21300610899925232, mae:0.3638772666454315, smape:35.26564836502075, dtw:not calculated
horizon:10 mse:0.2281751036643982, mae:0.3719179034233093, smape:35.93262434005737, dtw:not calculated
horizon:11 mse:0.22793735563755035, mae:0.3781494200229645, smape:36.58204972743988, dtw:not calculated
horizon:12 mse:0.22875867784023285, mae:0.36764413118362427, smape:35.497769713401794, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09947867691516876, mae:0.22872039675712585, smape:22.450435161590576, dtw:not calculated
average metrics: horizon upto:12 mse:0.1588335633277893, mae:0.297842413187027, smape:28.987467288970947, dtw:not calculated
===============================================================================
average of horizons: mse:0.1588335633277893, mae:0.297842413187027, smape:28.987467288970947, dtw:not calculated
mean smape over horizons:  28.987465053796768
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=10, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4240989685058594
Epoch: 1, Steps: 46 | Train Loss: 0.4813015 Vali Loss: 0.1739470 Test Loss: 0.1739470
Validation loss decreased (inf --> 0.173947).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1194815635681152
Epoch: 2, Steps: 46 | Train Loss: 0.3016738 Vali Loss: 0.1687786 Test Loss: 0.1687786
Validation loss decreased (0.173947 --> 0.168779).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1205482482910156
Epoch: 3, Steps: 46 | Train Loss: 0.2732190 Vali Loss: 0.1649381 Test Loss: 0.1649381
Validation loss decreased (0.168779 --> 0.164938).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.124678373336792
Epoch: 4, Steps: 46 | Train Loss: 0.2546523 Vali Loss: 0.1663718 Test Loss: 0.1663718
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0199689865112305
Epoch: 5, Steps: 46 | Train Loss: 0.2558602 Vali Loss: 0.1658731 Test Loss: 0.1658731
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0332062244415283
Epoch: 6, Steps: 46 | Train Loss: 0.2586487 Vali Loss: 0.1651067 Test Loss: 0.1651067
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.028895467519760132, mae:0.13154691457748413, smape:13.114748895168304, dtw:not calculated
horizon:2 mse:0.05003907158970833, mae:0.17386285960674286, smape:17.271263897418976, dtw:not calculated
horizon:3 mse:0.0855737179517746, mae:0.22229313850402832, smape:21.92167192697525, dtw:not calculated
horizon:4 mse:0.10061497986316681, mae:0.23529052734375, smape:23.125752806663513, dtw:not calculated
horizon:5 mse:0.1456824541091919, mae:0.28818273544311523, smape:28.126472234725952, dtw:not calculated
horizon:6 mse:0.2005656361579895, mae:0.3538805842399597, smape:34.33066010475159, dtw:not calculated
horizon:7 mse:0.20641502737998962, mae:0.36253029108047485, smape:35.181352496147156, dtw:not calculated
horizon:8 mse:0.1937689334154129, mae:0.34171202778816223, smape:33.17022919654846, dtw:not calculated
horizon:9 mse:0.2534369230270386, mae:0.4035784602165222, smape:38.90445828437805, dtw:not calculated
horizon:10 mse:0.24673958122730255, mae:0.37482285499572754, smape:35.93769669532776, dtw:not calculated
horizon:11 mse:0.24138939380645752, mae:0.3925573229789734, smape:37.940940260887146, dtw:not calculated
horizon:12 mse:0.22613559663295746, mae:0.3802213966846466, smape:36.74734830856323, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10189522057771683, mae:0.2341761291027069, smape:22.981762886047363, dtw:not calculated
average metrics: horizon upto:12 mse:0.1649380624294281, mae:0.3050399422645569, smape:29.647713899612427, dtw:not calculated
===============================================================================
average of horizons: mse:0.1649380624294281, mae:0.3050399422645569, smape:29.647713899612427, dtw:not calculated
mean smape over horizons:  29.647716258962948
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=12, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4040014743804932
Epoch: 1, Steps: 46 | Train Loss: 0.4383112 Vali Loss: 0.1855970 Test Loss: 0.1855970
Validation loss decreased (inf --> 0.185597).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0637671947479248
Epoch: 2, Steps: 46 | Train Loss: 0.2928497 Vali Loss: 0.1803737 Test Loss: 0.1803737
Validation loss decreased (0.185597 --> 0.180374).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0289101600646973
Epoch: 3, Steps: 46 | Train Loss: 0.2666598 Vali Loss: 0.1715837 Test Loss: 0.1715837
Validation loss decreased (0.180374 --> 0.171584).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1129124164581299
Epoch: 4, Steps: 46 | Train Loss: 0.2569947 Vali Loss: 0.1655691 Test Loss: 0.1655691
Validation loss decreased (0.171584 --> 0.165569).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.021592140197754
Epoch: 5, Steps: 46 | Train Loss: 0.2543849 Vali Loss: 0.1647241 Test Loss: 0.1647241
Validation loss decreased (0.165569 --> 0.164724).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0456898212432861
Epoch: 6, Steps: 46 | Train Loss: 0.2583991 Vali Loss: 0.1645999 Test Loss: 0.1645999
Validation loss decreased (0.164724 --> 0.164600).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0619070529937744
Epoch: 7, Steps: 46 | Train Loss: 0.2489264 Vali Loss: 0.1654873 Test Loss: 0.1654873
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.035083532333374
Epoch: 8, Steps: 46 | Train Loss: 0.2461327 Vali Loss: 0.1650407 Test Loss: 0.1650407
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.128190279006958
Epoch: 9, Steps: 46 | Train Loss: 0.2578912 Vali Loss: 0.1649554 Test Loss: 0.1649554
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.023012425750494003, mae:0.11335458606481552, smape:11.305752396583557, dtw:not calculated
horizon:2 mse:0.057852305471897125, mae:0.18704693019390106, smape:18.554212152957916, dtw:not calculated
horizon:3 mse:0.07845568656921387, mae:0.20944084227085114, smape:20.675911009311676, dtw:not calculated
horizon:4 mse:0.11706308275461197, mae:0.25197017192840576, smape:24.692794680595398, dtw:not calculated
horizon:5 mse:0.15175428986549377, mae:0.2922138273715973, smape:28.480851650238037, dtw:not calculated
horizon:6 mse:0.18167228996753693, mae:0.3150211274623871, smape:30.530112981796265, dtw:not calculated
horizon:7 mse:0.1850176751613617, mae:0.3205875754356384, smape:31.075677275657654, dtw:not calculated
horizon:8 mse:0.22078004479408264, mae:0.3673098087310791, smape:35.53780913352966, dtw:not calculated
horizon:9 mse:0.23633509874343872, mae:0.37315502762794495, smape:35.983845591545105, dtw:not calculated
horizon:10 mse:0.22712397575378418, mae:0.38216832280158997, smape:37.01034486293793, dtw:not calculated
horizon:11 mse:0.2586551606655121, mae:0.39471980929374695, smape:37.91559934616089, dtw:not calculated
horizon:12 mse:0.2374763935804367, mae:0.3785683214664459, smape:36.47768199443817, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10163502395153046, mae:0.2281745970249176, smape:22.373273968696594, dtw:not calculated
average metrics: horizon upto:12 mse:0.16459986567497253, mae:0.2987963855266571, smape:29.020050168037415, dtw:not calculated
===============================================================================
average of horizons: mse:0.16459986567497253, mae:0.2987963855266571, smape:29.020050168037415, dtw:not calculated
mean smape over horizons:  29.020049422979355
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=12, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4187445640563965
Epoch: 1, Steps: 46 | Train Loss: 0.4362948 Vali Loss: 0.1846561 Test Loss: 0.1846561
Validation loss decreased (inf --> 0.184656).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.021089792251587
Epoch: 2, Steps: 46 | Train Loss: 0.2878601 Vali Loss: 0.1619649 Test Loss: 0.1619649
Validation loss decreased (0.184656 --> 0.161965).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0532326698303223
Epoch: 3, Steps: 46 | Train Loss: 0.2595770 Vali Loss: 0.1737012 Test Loss: 0.1737012
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0466976165771484
Epoch: 4, Steps: 46 | Train Loss: 0.2416882 Vali Loss: 0.1617779 Test Loss: 0.1617779
Validation loss decreased (0.161965 --> 0.161778).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9464592933654785
Epoch: 5, Steps: 46 | Train Loss: 0.2428072 Vali Loss: 0.1639353 Test Loss: 0.1639353
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1270036697387695
Epoch: 6, Steps: 46 | Train Loss: 0.2494155 Vali Loss: 0.1630322 Test Loss: 0.1630322
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.026003122329712
Epoch: 7, Steps: 46 | Train Loss: 0.2401182 Vali Loss: 0.1632100 Test Loss: 0.1632100
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.022403711453080177, mae:0.11240966618061066, smape:11.212292313575745, dtw:not calculated
horizon:2 mse:0.05814967304468155, mae:0.176112100481987, smape:17.43224412202835, dtw:not calculated
horizon:3 mse:0.08364187180995941, mae:0.21139341592788696, smape:20.83990126848221, dtw:not calculated
horizon:4 mse:0.11777248978614807, mae:0.2482052892446518, smape:24.29511547088623, dtw:not calculated
horizon:5 mse:0.1566736400127411, mae:0.2911550998687744, smape:28.315606713294983, dtw:not calculated
horizon:6 mse:0.16334952414035797, mae:0.3053250014781952, smape:29.717084765434265, dtw:not calculated
horizon:7 mse:0.17380917072296143, mae:0.3192010819911957, smape:31.058725714683533, dtw:not calculated
horizon:8 mse:0.19877277314662933, mae:0.34894299507141113, smape:33.88531804084778, dtw:not calculated
horizon:9 mse:0.24371224641799927, mae:0.3867378234863281, smape:37.27595508098602, dtw:not calculated
horizon:10 mse:0.224821075797081, mae:0.38652026653289795, smape:37.5043123960495, dtw:not calculated
horizon:11 mse:0.2329476922750473, mae:0.3871726095676422, smape:37.44228482246399, dtw:not calculated
horizon:12 mse:0.26528117060661316, mae:0.4039905369281769, smape:38.80712389945984, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10033182054758072, mae:0.22410011291503906, smape:21.96870893239975, dtw:not calculated
average metrics: horizon upto:12 mse:0.16177792847156525, mae:0.298097163438797, smape:28.982162475585938, dtw:not calculated
===============================================================================
average of horizons: mse:0.16177792847156525, mae:0.298097163438797, smape:28.982162475585938, dtw:not calculated
mean smape over horizons:  28.98216371734937
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=12, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3251988887786865
Epoch: 1, Steps: 46 | Train Loss: 0.3965361 Vali Loss: 0.1915270 Test Loss: 0.1915270
Validation loss decreased (inf --> 0.191527).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9731299877166748
Epoch: 2, Steps: 46 | Train Loss: 0.2914524 Vali Loss: 0.1635848 Test Loss: 0.1635848
Validation loss decreased (0.191527 --> 0.163585).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1238281726837158
Epoch: 3, Steps: 46 | Train Loss: 0.2609674 Vali Loss: 0.1563223 Test Loss: 0.1563223
Validation loss decreased (0.163585 --> 0.156322).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0412473678588867
Epoch: 4, Steps: 46 | Train Loss: 0.2495227 Vali Loss: 0.1544994 Test Loss: 0.1544994
Validation loss decreased (0.156322 --> 0.154499).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.132843255996704
Epoch: 5, Steps: 46 | Train Loss: 0.2414132 Vali Loss: 0.1535561 Test Loss: 0.1535561
Validation loss decreased (0.154499 --> 0.153556).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1069376468658447
Epoch: 6, Steps: 46 | Train Loss: 0.2518573 Vali Loss: 0.1538442 Test Loss: 0.1538442
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0666098594665527
Epoch: 7, Steps: 46 | Train Loss: 0.2563511 Vali Loss: 0.1538042 Test Loss: 0.1538042
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9609320163726807
Epoch: 8, Steps: 46 | Train Loss: 0.2378580 Vali Loss: 0.1533260 Test Loss: 0.1533260
Validation loss decreased (0.153556 --> 0.153326).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0407295227050781
Epoch: 9, Steps: 46 | Train Loss: 0.2540269 Vali Loss: 0.1540242 Test Loss: 0.1540242
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9924397468566895
Epoch: 10, Steps: 46 | Train Loss: 0.2419601 Vali Loss: 0.1539628 Test Loss: 0.1539628
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.029899151995778084, mae:0.12558330595493317, smape:12.506082653999329, dtw:not calculated
horizon:2 mse:0.0615847148001194, mae:0.18975241482257843, smape:18.80338490009308, dtw:not calculated
horizon:3 mse:0.0811285674571991, mae:0.20797426998615265, smape:20.514465868473053, dtw:not calculated
horizon:4 mse:0.13095490634441376, mae:0.27170664072036743, smape:26.5876442193985, dtw:not calculated
horizon:5 mse:0.1459488570690155, mae:0.2898832857608795, smape:28.298524022102356, dtw:not calculated
horizon:6 mse:0.17406952381134033, mae:0.30524104833602905, smape:29.602444171905518, dtw:not calculated
horizon:7 mse:0.17567762732505798, mae:0.3176501393318176, smape:30.867275595664978, dtw:not calculated
horizon:8 mse:0.193212628364563, mae:0.3362254798412323, smape:32.619354128837585, dtw:not calculated
horizon:9 mse:0.20135509967803955, mae:0.34426721930503845, smape:33.375388383865356, dtw:not calculated
horizon:10 mse:0.22442446649074554, mae:0.3639761209487915, smape:35.14887988567352, dtw:not calculated
horizon:11 mse:0.21925832331180573, mae:0.37008607387542725, smape:35.839131474494934, dtw:not calculated
horizon:12 mse:0.2023981511592865, mae:0.3580479919910431, smape:34.767523407936096, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10393095016479492, mae:0.2316901832818985, smape:22.71875888109207, dtw:not calculated
average metrics: horizon upto:12 mse:0.15332600474357605, mae:0.29003283381462097, smape:28.244176506996155, dtw:not calculated
===============================================================================
average of horizons: mse:0.15332600474357605, mae:0.29003283381462097, smape:28.244176506996155, dtw:not calculated
mean smape over horizons:  28.244174892703693
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=12, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4002389907836914
Epoch: 1, Steps: 46 | Train Loss: 0.3872525 Vali Loss: 0.1809647 Test Loss: 0.1809647
Validation loss decreased (inf --> 0.180965).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0472731590270996
Epoch: 2, Steps: 46 | Train Loss: 0.2988841 Vali Loss: 0.1679220 Test Loss: 0.1679220
Validation loss decreased (0.180965 --> 0.167922).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.133415937423706
Epoch: 3, Steps: 46 | Train Loss: 0.2709357 Vali Loss: 0.1626054 Test Loss: 0.1626054
Validation loss decreased (0.167922 --> 0.162605).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0865521430969238
Epoch: 4, Steps: 46 | Train Loss: 0.2532553 Vali Loss: 0.1619314 Test Loss: 0.1619314
Validation loss decreased (0.162605 --> 0.161931).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0185508728027344
Epoch: 5, Steps: 46 | Train Loss: 0.2580405 Vali Loss: 0.1613013 Test Loss: 0.1613013
Validation loss decreased (0.161931 --> 0.161301).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9285488128662109
Epoch: 6, Steps: 46 | Train Loss: 0.2589799 Vali Loss: 0.1610743 Test Loss: 0.1610743
Validation loss decreased (0.161301 --> 0.161074).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0573372840881348
Epoch: 7, Steps: 46 | Train Loss: 0.2612275 Vali Loss: 0.1608847 Test Loss: 0.1608847
Validation loss decreased (0.161074 --> 0.160885).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0454719066619873
Epoch: 8, Steps: 46 | Train Loss: 0.2476302 Vali Loss: 0.1607137 Test Loss: 0.1607137
Validation loss decreased (0.160885 --> 0.160714).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0681054592132568
Epoch: 9, Steps: 46 | Train Loss: 0.2506204 Vali Loss: 0.1606379 Test Loss: 0.1606379
Validation loss decreased (0.160714 --> 0.160638).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0219495296478271
Epoch: 10, Steps: 46 | Train Loss: 0.2597496 Vali Loss: 0.1609642 Test Loss: 0.1609642
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.022539421916007996, mae:0.1115226149559021, smape:11.121713370084763, dtw:not calculated
horizon:2 mse:0.054534826427698135, mae:0.1813114434480667, smape:17.995531857013702, dtw:not calculated
horizon:3 mse:0.08175186812877655, mae:0.20953182876110077, smape:20.66155970096588, dtw:not calculated
horizon:4 mse:0.10907085239887238, mae:0.2474949210882187, smape:24.300754070281982, dtw:not calculated
horizon:5 mse:0.1427197903394699, mae:0.2866596579551697, smape:28.00668478012085, dtw:not calculated
horizon:6 mse:0.17814883589744568, mae:0.3254815936088562, smape:31.63926601409912, dtw:not calculated
horizon:7 mse:0.18903027474880219, mae:0.3400614261627197, smape:33.04673433303833, dtw:not calculated
horizon:8 mse:0.2168998122215271, mae:0.37198472023010254, smape:36.06221675872803, dtw:not calculated
horizon:9 mse:0.20876234769821167, mae:0.34971362352371216, smape:33.84746611118317, dtw:not calculated
horizon:10 mse:0.2445448637008667, mae:0.39600422978401184, smape:38.2560133934021, dtw:not calculated
horizon:11 mse:0.2295149862766266, mae:0.3779224455356598, smape:36.50752902030945, dtw:not calculated
horizon:12 mse:0.25013652443885803, mae:0.39173218607902527, smape:37.74620294570923, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09812759608030319, mae:0.22700034081935883, smape:22.287584841251373, dtw:not calculated
average metrics: horizon upto:12 mse:0.16063787043094635, mae:0.29911836981773376, smape:29.09930646419525, dtw:not calculated
===============================================================================
average of horizons: mse:0.16063787043094635, mae:0.29911836981773376, smape:29.09930646419525, dtw:not calculated
mean smape over horizons:  29.09930602957805
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=12, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4677815437316895
Epoch: 1, Steps: 46 | Train Loss: 0.3699274 Vali Loss: 0.1763675 Test Loss: 0.1763675
Validation loss decreased (inf --> 0.176368).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0345945358276367
Epoch: 2, Steps: 46 | Train Loss: 0.2916980 Vali Loss: 0.1745984 Test Loss: 0.1745984
Validation loss decreased (0.176368 --> 0.174598).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0755505561828613
Epoch: 3, Steps: 46 | Train Loss: 0.2687157 Vali Loss: 0.1701859 Test Loss: 0.1701859
Validation loss decreased (0.174598 --> 0.170186).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1427326202392578
Epoch: 4, Steps: 46 | Train Loss: 0.2535494 Vali Loss: 0.1671681 Test Loss: 0.1671681
Validation loss decreased (0.170186 --> 0.167168).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.066948652267456
Epoch: 5, Steps: 46 | Train Loss: 0.2601968 Vali Loss: 0.1672876 Test Loss: 0.1672876
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9969923496246338
Epoch: 6, Steps: 46 | Train Loss: 0.2572351 Vali Loss: 0.1664193 Test Loss: 0.1664193
Validation loss decreased (0.167168 --> 0.166419).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9725747108459473
Epoch: 7, Steps: 46 | Train Loss: 0.2595783 Vali Loss: 0.1654300 Test Loss: 0.1654300
Validation loss decreased (0.166419 --> 0.165430).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0429983139038086
Epoch: 8, Steps: 46 | Train Loss: 0.2431625 Vali Loss: 0.1659883 Test Loss: 0.1659883
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.1441094875335693
Epoch: 9, Steps: 46 | Train Loss: 0.2518207 Vali Loss: 0.1655104 Test Loss: 0.1655104
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9726943969726562
Epoch: 10, Steps: 46 | Train Loss: 0.2629322 Vali Loss: 0.1668870 Test Loss: 0.1668870
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.023794373497366905, mae:0.11648716032505035, smape:11.61910966038704, dtw:not calculated
horizon:2 mse:0.05593334510922432, mae:0.18043746054172516, smape:17.88989007472992, dtw:not calculated
horizon:3 mse:0.0873756855726242, mae:0.22154034674167633, smape:21.840900182724, dtw:not calculated
horizon:4 mse:0.11046285927295685, mae:0.25283029675483704, smape:24.844786524772644, dtw:not calculated
horizon:5 mse:0.14116255939006805, mae:0.2870006263256073, smape:28.050902485847473, dtw:not calculated
horizon:6 mse:0.167885884642601, mae:0.3188921809196472, smape:31.089770793914795, dtw:not calculated
horizon:7 mse:0.1987757682800293, mae:0.3486393988132477, smape:33.829158544540405, dtw:not calculated
horizon:8 mse:0.21766157448291779, mae:0.37275001406669617, smape:36.13244295120239, dtw:not calculated
horizon:9 mse:0.22699913382530212, mae:0.3701622784137726, smape:35.75166165828705, dtw:not calculated
horizon:10 mse:0.25630050897598267, mae:0.407992422580719, smape:39.37033414840698, dtw:not calculated
horizon:11 mse:0.23414044082164764, mae:0.38543567061424255, smape:37.236207723617554, dtw:not calculated
horizon:12 mse:0.264667809009552, mae:0.4085371494293213, smape:39.343541860580444, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.0977691188454628, mae:0.22953133285045624, smape:22.555892169475555, dtw:not calculated
average metrics: horizon upto:12 mse:0.1654299944639206, mae:0.30589208006858826, smape:29.74989414215088, dtw:not calculated
===============================================================================
average of horizons: mse:0.1654299944639206, mae:0.30589208006858826, smape:29.74989414215088, dtw:not calculated
mean smape over horizons:  29.749892217417557
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=14, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4413561820983887
Epoch: 1, Steps: 46 | Train Loss: 0.4303028 Vali Loss: 0.2134432 Test Loss: 0.2134432
Validation loss decreased (inf --> 0.213443).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0206730365753174
Epoch: 2, Steps: 46 | Train Loss: 0.3043889 Vali Loss: 0.1983939 Test Loss: 0.1983939
Validation loss decreased (0.213443 --> 0.198394).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0494444370269775
Epoch: 3, Steps: 46 | Train Loss: 0.2858218 Vali Loss: 0.1826912 Test Loss: 0.1826912
Validation loss decreased (0.198394 --> 0.182691).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1370856761932373
Epoch: 4, Steps: 46 | Train Loss: 0.2680750 Vali Loss: 0.1791118 Test Loss: 0.1791118
Validation loss decreased (0.182691 --> 0.179112).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0774509906768799
Epoch: 5, Steps: 46 | Train Loss: 0.2622240 Vali Loss: 0.1784796 Test Loss: 0.1784796
Validation loss decreased (0.179112 --> 0.178480).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9610376358032227
Epoch: 6, Steps: 46 | Train Loss: 0.2629000 Vali Loss: 0.1762644 Test Loss: 0.1762644
Validation loss decreased (0.178480 --> 0.176264).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0786182880401611
Epoch: 7, Steps: 46 | Train Loss: 0.2616637 Vali Loss: 0.1757940 Test Loss: 0.1757940
Validation loss decreased (0.176264 --> 0.175794).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0965988636016846
Epoch: 8, Steps: 46 | Train Loss: 0.2576247 Vali Loss: 0.1738633 Test Loss: 0.1738633
Validation loss decreased (0.175794 --> 0.173863).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0579757690429688
Epoch: 9, Steps: 46 | Train Loss: 0.2593935 Vali Loss: 0.1745584 Test Loss: 0.1745584
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9682507514953613
Epoch: 10, Steps: 46 | Train Loss: 0.2585862 Vali Loss: 0.1735552 Test Loss: 0.1735552
Validation loss decreased (0.173863 --> 0.173555).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.038266316056251526, mae:0.152788445353508, smape:15.210425853729248, dtw:not calculated
horizon:2 mse:0.07832930982112885, mae:0.21262124180793762, smape:21.01454734802246, dtw:not calculated
horizon:3 mse:0.10718786716461182, mae:0.2519591748714447, smape:24.78666603565216, dtw:not calculated
horizon:4 mse:0.15327517688274384, mae:0.30201417207717896, smape:29.49593961238861, dtw:not calculated
horizon:5 mse:0.15084116160869598, mae:0.2981112599372864, smape:29.12229895591736, dtw:not calculated
horizon:6 mse:0.17138202488422394, mae:0.31930822134017944, smape:31.089863181114197, dtw:not calculated
horizon:7 mse:0.21415111422538757, mae:0.3567754030227661, smape:34.5133513212204, dtw:not calculated
horizon:8 mse:0.218123659491539, mae:0.35574910044670105, smape:34.353479743003845, dtw:not calculated
horizon:9 mse:0.23545655608177185, mae:0.36861294507980347, smape:35.483765602111816, dtw:not calculated
horizon:10 mse:0.22764885425567627, mae:0.36951732635498047, smape:35.682520270347595, dtw:not calculated
horizon:11 mse:0.24838535487651825, mae:0.3824828863143921, smape:36.79284751415253, dtw:not calculated
horizon:12 mse:0.23961499333381653, mae:0.37993839383125305, smape:36.609235405921936, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.11654697358608246, mae:0.2561337649822235, smape:25.119957327842712, dtw:not calculated
average metrics: horizon upto:12 mse:0.1735551953315735, mae:0.31248989701271057, smape:30.34624457359314, dtw:not calculated
===============================================================================
average of horizons: mse:0.1735551953315735, mae:0.31248989701271057, smape:30.34624457359314, dtw:not calculated
mean smape over horizons:  30.34624507029851
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=14, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3550584316253662
Epoch: 1, Steps: 46 | Train Loss: 0.4283770 Vali Loss: 0.1897480 Test Loss: 0.1897480
Validation loss decreased (inf --> 0.189748).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0372579097747803
Epoch: 2, Steps: 46 | Train Loss: 0.3078643 Vali Loss: 0.1767069 Test Loss: 0.1767069
Validation loss decreased (0.189748 --> 0.176707).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9933338165283203
Epoch: 3, Steps: 46 | Train Loss: 0.2840417 Vali Loss: 0.1666205 Test Loss: 0.1666205
Validation loss decreased (0.176707 --> 0.166620).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9754452705383301
Epoch: 4, Steps: 46 | Train Loss: 0.2659555 Vali Loss: 0.1608905 Test Loss: 0.1608905
Validation loss decreased (0.166620 --> 0.160891).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0361952781677246
Epoch: 5, Steps: 46 | Train Loss: 0.2708061 Vali Loss: 0.1616370 Test Loss: 0.1616370
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0888824462890625
Epoch: 6, Steps: 46 | Train Loss: 0.2621376 Vali Loss: 0.1598113 Test Loss: 0.1598113
Validation loss decreased (0.160891 --> 0.159811).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.1326093673706055
Epoch: 7, Steps: 46 | Train Loss: 0.2673787 Vali Loss: 0.1602679 Test Loss: 0.1602679
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.1054470539093018
Epoch: 8, Steps: 46 | Train Loss: 0.2612525 Vali Loss: 0.1600523 Test Loss: 0.1600523
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9973480701446533
Epoch: 9, Steps: 46 | Train Loss: 0.2613633 Vali Loss: 0.1611596 Test Loss: 0.1611596
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.029680728912353516, mae:0.13361138105392456, smape:13.3172407746315, dtw:not calculated
horizon:2 mse:0.06489820778369904, mae:0.19915179908275604, smape:19.73772943019867, dtw:not calculated
horizon:3 mse:0.09891363233327866, mae:0.2373640239238739, smape:23.36495816707611, dtw:not calculated
horizon:4 mse:0.11864497512578964, mae:0.2566687762737274, smape:25.159794092178345, dtw:not calculated
horizon:5 mse:0.12782204151153564, mae:0.2791118919849396, smape:27.381709218025208, dtw:not calculated
horizon:6 mse:0.17618927359580994, mae:0.32168135046958923, smape:31.272706389427185, dtw:not calculated
horizon:7 mse:0.16414834558963776, mae:0.31545940041542053, smape:30.769851803779602, dtw:not calculated
horizon:8 mse:0.2114039808511734, mae:0.36684417724609375, smape:35.59443652629852, dtw:not calculated
horizon:9 mse:0.223088800907135, mae:0.3665415942668915, smape:35.4248970746994, dtw:not calculated
horizon:10 mse:0.21418839693069458, mae:0.36867669224739075, smape:35.75581908226013, dtw:not calculated
horizon:11 mse:0.24072512984275818, mae:0.3827356994152069, smape:36.91815733909607, dtw:not calculated
horizon:12 mse:0.24803222715854645, mae:0.38578200340270996, smape:37.12148070335388, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10269147902727127, mae:0.2379315346479416, smape:23.372356593608856, dtw:not calculated
average metrics: horizon upto:12 mse:0.15981130301952362, mae:0.30113571882247925, smape:29.318228363990784, dtw:not calculated
===============================================================================
average of horizons: mse:0.15981130301952362, mae:0.30113571882247925, smape:29.318228363990784, dtw:not calculated
mean smape over horizons:  29.318231716752052
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=14, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3836467266082764
Epoch: 1, Steps: 46 | Train Loss: 0.4425934 Vali Loss: 0.1845059 Test Loss: 0.1845059
Validation loss decreased (inf --> 0.184506).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.06923508644104
Epoch: 2, Steps: 46 | Train Loss: 0.2965075 Vali Loss: 0.1678671 Test Loss: 0.1678671
Validation loss decreased (0.184506 --> 0.167867).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.00193190574646
Epoch: 3, Steps: 46 | Train Loss: 0.2865403 Vali Loss: 0.1620969 Test Loss: 0.1620969
Validation loss decreased (0.167867 --> 0.162097).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9474964141845703
Epoch: 4, Steps: 46 | Train Loss: 0.2771054 Vali Loss: 0.1615616 Test Loss: 0.1615616
Validation loss decreased (0.162097 --> 0.161562).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0049521923065186
Epoch: 5, Steps: 46 | Train Loss: 0.2708337 Vali Loss: 0.1604528 Test Loss: 0.1604528
Validation loss decreased (0.161562 --> 0.160453).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0834076404571533
Epoch: 6, Steps: 46 | Train Loss: 0.2684444 Vali Loss: 0.1602262 Test Loss: 0.1602262
Validation loss decreased (0.160453 --> 0.160226).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0081713199615479
Epoch: 7, Steps: 46 | Train Loss: 0.2668062 Vali Loss: 0.1601303 Test Loss: 0.1601303
Validation loss decreased (0.160226 --> 0.160130).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0304419994354248
Epoch: 8, Steps: 46 | Train Loss: 0.2608234 Vali Loss: 0.1600026 Test Loss: 0.1600026
Validation loss decreased (0.160130 --> 0.160003).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0587048530578613
Epoch: 9, Steps: 46 | Train Loss: 0.2629161 Vali Loss: 0.1597703 Test Loss: 0.1597703
Validation loss decreased (0.160003 --> 0.159770).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0351004600524902
Epoch: 10, Steps: 46 | Train Loss: 0.2614094 Vali Loss: 0.1605645 Test Loss: 0.1605645
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.025657761842012405, mae:0.12154354900121689, smape:12.120269984006882, dtw:not calculated
horizon:2 mse:0.07058269530534744, mae:0.20283034443855286, smape:20.065394043922424, dtw:not calculated
horizon:3 mse:0.08286890387535095, mae:0.21697765588760376, smape:21.409615874290466, dtw:not calculated
horizon:4 mse:0.12935271859169006, mae:0.2788909673690796, smape:27.34789252281189, dtw:not calculated
horizon:5 mse:0.1543898582458496, mae:0.3065067231655121, smape:29.932215809822083, dtw:not calculated
horizon:6 mse:0.16584140062332153, mae:0.3254927396774292, smape:31.787285208702087, dtw:not calculated
horizon:7 mse:0.2049732804298401, mae:0.3561326265335083, smape:34.54138934612274, dtw:not calculated
horizon:8 mse:0.19789597392082214, mae:0.3484514653682709, smape:33.84261429309845, dtw:not calculated
horizon:9 mse:0.2014390081167221, mae:0.35814154148101807, smape:34.804221987724304, dtw:not calculated
horizon:10 mse:0.20520657300949097, mae:0.3581239581108093, smape:34.76421236991882, dtw:not calculated
horizon:11 mse:0.23550088703632355, mae:0.3797494173049927, smape:36.641138792037964, dtw:not calculated
horizon:12 mse:0.24353407323360443, mae:0.39026695489883423, smape:37.65954673290253, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10478222370147705, mae:0.24204033613204956, smape:23.777110874652863, dtw:not calculated
average metrics: horizon upto:12 mse:0.15977025032043457, mae:0.303592324256897, smape:29.576316475868225, dtw:not calculated
===============================================================================
average of horizons: mse:0.15977025032043457, mae:0.303592324256897, smape:29.576316475868225, dtw:not calculated
mean smape over horizons:  29.576316413780052
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=14, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5104069709777832
Epoch: 1, Steps: 46 | Train Loss: 0.4211114 Vali Loss: 0.1688083 Test Loss: 0.1688083
Validation loss decreased (inf --> 0.168808).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9772861003875732
Epoch: 2, Steps: 46 | Train Loss: 0.2873938 Vali Loss: 0.1626089 Test Loss: 0.1626089
Validation loss decreased (0.168808 --> 0.162609).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0356268882751465
Epoch: 3, Steps: 46 | Train Loss: 0.2782849 Vali Loss: 0.1550114 Test Loss: 0.1550114
Validation loss decreased (0.162609 --> 0.155011).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0138726234436035
Epoch: 4, Steps: 46 | Train Loss: 0.2610289 Vali Loss: 0.1541109 Test Loss: 0.1541109
Validation loss decreased (0.155011 --> 0.154111).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9506866931915283
Epoch: 5, Steps: 46 | Train Loss: 0.2634944 Vali Loss: 0.1526450 Test Loss: 0.1526450
Validation loss decreased (0.154111 --> 0.152645).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9782619476318359
Epoch: 6, Steps: 46 | Train Loss: 0.2581332 Vali Loss: 0.1513966 Test Loss: 0.1513966
Validation loss decreased (0.152645 --> 0.151397).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0039732456207275
Epoch: 7, Steps: 46 | Train Loss: 0.2587577 Vali Loss: 0.1516141 Test Loss: 0.1516141
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9861927032470703
Epoch: 8, Steps: 46 | Train Loss: 0.2476444 Vali Loss: 0.1518889 Test Loss: 0.1518889
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.025975227355957
Epoch: 9, Steps: 46 | Train Loss: 0.2552296 Vali Loss: 0.1513322 Test Loss: 0.1513322
Validation loss decreased (0.151397 --> 0.151332).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0082437992095947
Epoch: 10, Steps: 46 | Train Loss: 0.2500625 Vali Loss: 0.1515694 Test Loss: 0.1515694
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0302566010504961, mae:0.12601320445537567, smape:12.54500150680542, dtw:not calculated
horizon:2 mse:0.04871223866939545, mae:0.16192664206027985, smape:16.063226759433746, dtw:not calculated
horizon:3 mse:0.08820567280054092, mae:0.22893956303596497, smape:22.58690893650055, dtw:not calculated
horizon:4 mse:0.11827115714550018, mae:0.2585413157939911, smape:25.340476632118225, dtw:not calculated
horizon:5 mse:0.13007165491580963, mae:0.26970499753952026, smape:26.38808786869049, dtw:not calculated
horizon:6 mse:0.14966624975204468, mae:0.2951662242412567, smape:28.79815697669983, dtw:not calculated
horizon:7 mse:0.17655999958515167, mae:0.3229391574859619, smape:31.415340304374695, dtw:not calculated
horizon:8 mse:0.21566282212734222, mae:0.3596917390823364, smape:34.79596674442291, dtw:not calculated
horizon:9 mse:0.21291707456111908, mae:0.3659122884273529, smape:35.471588373184204, dtw:not calculated
horizon:10 mse:0.20580826699733734, mae:0.3519611060619354, smape:34.126079082489014, dtw:not calculated
horizon:11 mse:0.22996391355991364, mae:0.37550538778305054, smape:36.27593517303467, dtw:not calculated
horizon:12 mse:0.20989109575748444, mae:0.3599230945110321, smape:34.86091494560242, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09419727325439453, mae:0.22338199615478516, smape:21.95364087820053, dtw:not calculated
average metrics: horizon upto:12 mse:0.15133222937583923, mae:0.2896853983402252, smape:28.222310543060303, dtw:not calculated
===============================================================================
average of horizons: mse:0.15133222937583923, mae:0.2896853983402252, smape:28.222310543060303, dtw:not calculated
mean smape over horizons:  28.222306941946346
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=14, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3592369556427002
Epoch: 1, Steps: 46 | Train Loss: 0.4173328 Vali Loss: 0.1873822 Test Loss: 0.1873822
Validation loss decreased (inf --> 0.187382).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1153790950775146
Epoch: 2, Steps: 46 | Train Loss: 0.2951837 Vali Loss: 0.1749722 Test Loss: 0.1749722
Validation loss decreased (0.187382 --> 0.174972).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0753233432769775
Epoch: 3, Steps: 46 | Train Loss: 0.2703273 Vali Loss: 0.1709729 Test Loss: 0.1709729
Validation loss decreased (0.174972 --> 0.170973).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0536713600158691
Epoch: 4, Steps: 46 | Train Loss: 0.2597370 Vali Loss: 0.1678384 Test Loss: 0.1678384
Validation loss decreased (0.170973 --> 0.167838).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0837984085083008
Epoch: 5, Steps: 46 | Train Loss: 0.2500733 Vali Loss: 0.1628153 Test Loss: 0.1628153
Validation loss decreased (0.167838 --> 0.162815).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0750703811645508
Epoch: 6, Steps: 46 | Train Loss: 0.2586508 Vali Loss: 0.1617346 Test Loss: 0.1617346
Validation loss decreased (0.162815 --> 0.161735).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0129194259643555
Epoch: 7, Steps: 46 | Train Loss: 0.2593583 Vali Loss: 0.1617997 Test Loss: 0.1617997
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9866459369659424
Epoch: 8, Steps: 46 | Train Loss: 0.2410098 Vali Loss: 0.1610491 Test Loss: 0.1610491
Validation loss decreased (0.161735 --> 0.161049).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.064793348312378
Epoch: 9, Steps: 46 | Train Loss: 0.2580757 Vali Loss: 0.1605996 Test Loss: 0.1605996
Validation loss decreased (0.161049 --> 0.160600).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0734851360321045
Epoch: 10, Steps: 46 | Train Loss: 0.2485998 Vali Loss: 0.1608230 Test Loss: 0.1608230
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.025224201381206512, mae:0.1248057410120964, smape:12.453952431678772, dtw:not calculated
horizon:2 mse:0.05018260329961777, mae:0.1710572987794876, smape:16.987349092960358, dtw:not calculated
horizon:3 mse:0.09234123677015305, mae:0.22320441901683807, smape:21.96081578731537, dtw:not calculated
horizon:4 mse:0.11868740618228912, mae:0.2551836669445038, smape:24.98638927936554, dtw:not calculated
horizon:5 mse:0.15402083098888397, mae:0.29814228415489197, smape:29.072514176368713, dtw:not calculated
horizon:6 mse:0.1722193956375122, mae:0.32273390889167786, smape:31.421953439712524, dtw:not calculated
horizon:7 mse:0.19113267958164215, mae:0.33918190002441406, smape:32.94457793235779, dtw:not calculated
horizon:8 mse:0.20631054043769836, mae:0.3644653260707855, smape:35.385969281196594, dtw:not calculated
horizon:9 mse:0.22253045439720154, mae:0.36775174736976624, smape:35.54753363132477, dtw:not calculated
horizon:10 mse:0.25205421447753906, mae:0.3987139165401459, smape:38.4524792432785, dtw:not calculated
horizon:11 mse:0.2220851331949234, mae:0.38534682989120483, smape:37.406930327415466, dtw:not calculated
horizon:12 mse:0.22040702402591705, mae:0.3781140446662903, smape:36.67881786823273, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10211261361837387, mae:0.23252122104167938, smape:22.81382977962494, dtw:not calculated
average metrics: horizon upto:12 mse:0.16059964895248413, mae:0.30239173769950867, smape:29.441606998443604, dtw:not calculated
===============================================================================
average of horizons: mse:0.16059964895248413, mae:0.30239173769950867, smape:29.441606998443604, dtw:not calculated
mean smape over horizons:  29.44160687426726
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=14, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4310157299041748
Epoch: 1, Steps: 46 | Train Loss: 0.4097567 Vali Loss: 0.1812978 Test Loss: 0.1812978
Validation loss decreased (inf --> 0.181298).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9787287712097168
Epoch: 2, Steps: 46 | Train Loss: 0.2882520 Vali Loss: 0.1654092 Test Loss: 0.1654092
Validation loss decreased (0.181298 --> 0.165409).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0116875171661377
Epoch: 3, Steps: 46 | Train Loss: 0.2670037 Vali Loss: 0.1646446 Test Loss: 0.1646446
Validation loss decreased (0.165409 --> 0.164645).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1047999858856201
Epoch: 4, Steps: 46 | Train Loss: 0.2591557 Vali Loss: 0.1600014 Test Loss: 0.1600014
Validation loss decreased (0.164645 --> 0.160001).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.071732759475708
Epoch: 5, Steps: 46 | Train Loss: 0.2494083 Vali Loss: 0.1546082 Test Loss: 0.1546082
Validation loss decreased (0.160001 --> 0.154608).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0460917949676514
Epoch: 6, Steps: 46 | Train Loss: 0.2577450 Vali Loss: 0.1543197 Test Loss: 0.1543197
Validation loss decreased (0.154608 --> 0.154320).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.000520944595337
Epoch: 7, Steps: 46 | Train Loss: 0.2592603 Vali Loss: 0.1543271 Test Loss: 0.1543271
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9090471267700195
Epoch: 8, Steps: 46 | Train Loss: 0.2418368 Vali Loss: 0.1537919 Test Loss: 0.1537919
Validation loss decreased (0.154320 --> 0.153792).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0716619491577148
Epoch: 9, Steps: 46 | Train Loss: 0.2528365 Vali Loss: 0.1538119 Test Loss: 0.1538119
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0659515857696533
Epoch: 10, Steps: 46 | Train Loss: 0.2474932 Vali Loss: 0.1538143 Test Loss: 0.1538143
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.021189508959650993, mae:0.11158707737922668, smape:11.138681322336197, dtw:not calculated
horizon:2 mse:0.046445585787296295, mae:0.16540513932704926, smape:16.436201333999634, dtw:not calculated
horizon:3 mse:0.08152516186237335, mae:0.21086707711219788, smape:20.793679356575012, dtw:not calculated
horizon:4 mse:0.10941553860902786, mae:0.24793298542499542, smape:24.32856261730194, dtw:not calculated
horizon:5 mse:0.14973144233226776, mae:0.29117023944854736, smape:28.406986594200134, dtw:not calculated
horizon:6 mse:0.16038276255130768, mae:0.30380529165267944, smape:29.600408673286438, dtw:not calculated
horizon:7 mse:0.17849335074424744, mae:0.3214326500892639, smape:31.23762011528015, dtw:not calculated
horizon:8 mse:0.18781790137290955, mae:0.3390151560306549, smape:32.95034170150757, dtw:not calculated
horizon:9 mse:0.221140056848526, mae:0.3641863763332367, smape:35.21850407123566, dtw:not calculated
horizon:10 mse:0.23986543715000153, mae:0.38868406414985657, smape:37.558043003082275, dtw:not calculated
horizon:11 mse:0.22384488582611084, mae:0.37679529190063477, smape:36.496299505233765, dtw:not calculated
horizon:12 mse:0.22565077245235443, mae:0.376315176486969, smape:36.411088705062866, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09478167444467545, mae:0.22179463505744934, smape:21.784086525440216, dtw:not calculated
average metrics: horizon upto:12 mse:0.1537918746471405, mae:0.29143303632736206, smape:28.38137149810791, dtw:not calculated
===============================================================================
average of horizons: mse:0.1537918746471405, mae:0.29143303632736206, smape:28.38137149810791, dtw:not calculated
mean smape over horizons:  28.38136808325847
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=16, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3730995655059814
Epoch: 1, Steps: 46 | Train Loss: 0.4408835 Vali Loss: 0.2014333 Test Loss: 0.2014333
Validation loss decreased (inf --> 0.201433).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.010573387145996
Epoch: 2, Steps: 46 | Train Loss: 0.3067865 Vali Loss: 0.1768443 Test Loss: 0.1768443
Validation loss decreased (0.201433 --> 0.176844).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1010377407073975
Epoch: 3, Steps: 46 | Train Loss: 0.2909904 Vali Loss: 0.1710471 Test Loss: 0.1710471
Validation loss decreased (0.176844 --> 0.171047).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9581818580627441
Epoch: 4, Steps: 46 | Train Loss: 0.2686571 Vali Loss: 0.1647263 Test Loss: 0.1647263
Validation loss decreased (0.171047 --> 0.164726).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0786421298980713
Epoch: 5, Steps: 46 | Train Loss: 0.2561995 Vali Loss: 0.1602747 Test Loss: 0.1602747
Validation loss decreased (0.164726 --> 0.160275).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9918196201324463
Epoch: 6, Steps: 46 | Train Loss: 0.2587928 Vali Loss: 0.1620878 Test Loss: 0.1620878
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0189714431762695
Epoch: 7, Steps: 46 | Train Loss: 0.2591888 Vali Loss: 0.1593839 Test Loss: 0.1593839
Validation loss decreased (0.160275 --> 0.159384).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0382091999053955
Epoch: 8, Steps: 46 | Train Loss: 0.2516936 Vali Loss: 0.1616512 Test Loss: 0.1616512
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0190081596374512
Epoch: 9, Steps: 46 | Train Loss: 0.2565308 Vali Loss: 0.1602288 Test Loss: 0.1602288
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0142903327941895
Epoch: 10, Steps: 46 | Train Loss: 0.2473723 Vali Loss: 0.1597744 Test Loss: 0.1597744
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0244367104023695, mae:0.11762748658657074, smape:11.732862144708633, dtw:not calculated
horizon:2 mse:0.05504569411277771, mae:0.179854616522789, smape:17.847204208374023, dtw:not calculated
horizon:3 mse:0.07949095219373703, mae:0.20746658742427826, smape:20.470525324344635, dtw:not calculated
horizon:4 mse:0.11830101162195206, mae:0.26379066705703735, smape:25.90010166168213, dtw:not calculated
horizon:5 mse:0.13550707697868347, mae:0.2830537259578705, smape:27.69823670387268, dtw:not calculated
horizon:6 mse:0.15783870220184326, mae:0.3079424500465393, smape:30.058589577674866, dtw:not calculated
horizon:7 mse:0.178587406873703, mae:0.32988059520721436, smape:32.11910128593445, dtw:not calculated
horizon:8 mse:0.21501880884170532, mae:0.3679638206958771, smape:35.67829430103302, dtw:not calculated
horizon:9 mse:0.21045418083667755, mae:0.3605135679244995, smape:34.96432900428772, dtw:not calculated
horizon:10 mse:0.25407636165618896, mae:0.39190807938575745, smape:37.71321177482605, dtw:not calculated
horizon:11 mse:0.2297961413860321, mae:0.37561488151550293, smape:36.29564642906189, dtw:not calculated
horizon:12 mse:0.25405386090278625, mae:0.3940504789352417, smape:37.917402386665344, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09510336816310883, mae:0.2266225963830948, smape:22.28458672761917, dtw:not calculated
average metrics: horizon upto:12 mse:0.1593838930130005, mae:0.29830557107925415, smape:29.032960534095764, dtw:not calculated
===============================================================================
average of horizons: mse:0.1593838930130005, mae:0.29830557107925415, smape:29.032960534095764, dtw:not calculated
mean smape over horizons:  29.032958733538788
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=16, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.414581537246704
Epoch: 1, Steps: 46 | Train Loss: 0.4727852 Vali Loss: 0.2026112 Test Loss: 0.2026112
Validation loss decreased (inf --> 0.202611).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0734202861785889
Epoch: 2, Steps: 46 | Train Loss: 0.3046997 Vali Loss: 0.1765984 Test Loss: 0.1765984
Validation loss decreased (0.202611 --> 0.176598).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0601918697357178
Epoch: 3, Steps: 46 | Train Loss: 0.2871968 Vali Loss: 0.1711793 Test Loss: 0.1711793
Validation loss decreased (0.176598 --> 0.171179).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.058845043182373
Epoch: 4, Steps: 46 | Train Loss: 0.2619397 Vali Loss: 0.1709708 Test Loss: 0.1709708
Validation loss decreased (0.171179 --> 0.170971).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9787533283233643
Epoch: 5, Steps: 46 | Train Loss: 0.2665774 Vali Loss: 0.1717218 Test Loss: 0.1717218
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1688711643218994
Epoch: 6, Steps: 46 | Train Loss: 0.2600432 Vali Loss: 0.1705118 Test Loss: 0.1705118
Validation loss decreased (0.170971 --> 0.170512).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0501937866210938
Epoch: 7, Steps: 46 | Train Loss: 0.2737262 Vali Loss: 0.1696537 Test Loss: 0.1696537
Validation loss decreased (0.170512 --> 0.169654).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.091888666152954
Epoch: 8, Steps: 46 | Train Loss: 0.2491608 Vali Loss: 0.1691241 Test Loss: 0.1691241
Validation loss decreased (0.169654 --> 0.169124).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.1151773929595947
Epoch: 9, Steps: 46 | Train Loss: 0.2716175 Vali Loss: 0.1688204 Test Loss: 0.1688204
Validation loss decreased (0.169124 --> 0.168820).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.113996982574463
Epoch: 10, Steps: 46 | Train Loss: 0.2561924 Vali Loss: 0.1686242 Test Loss: 0.1686242
Validation loss decreased (0.168820 --> 0.168624).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03329494222998619, mae:0.14028693735599518, smape:13.971443474292755, dtw:not calculated
horizon:2 mse:0.0824563130736351, mae:0.21519407629966736, smape:21.235129237174988, dtw:not calculated
horizon:3 mse:0.10151651501655579, mae:0.24461504817008972, smape:24.079374969005585, dtw:not calculated
horizon:4 mse:0.11086948215961456, mae:0.25393298268318176, smape:24.945776164531708, dtw:not calculated
horizon:5 mse:0.16984006762504578, mae:0.3252311646938324, smape:31.713467836380005, dtw:not calculated
horizon:6 mse:0.16678278148174286, mae:0.3123790919780731, smape:30.421748757362366, dtw:not calculated
horizon:7 mse:0.17966359853744507, mae:0.33393362164497375, smape:32.53013789653778, dtw:not calculated
horizon:8 mse:0.21359913051128387, mae:0.3697372376918793, smape:35.86969971656799, dtw:not calculated
horizon:9 mse:0.23173049092292786, mae:0.3804078698158264, smape:36.76703870296478, dtw:not calculated
horizon:10 mse:0.2531387507915497, mae:0.40024957060813904, smape:38.60388994216919, dtw:not calculated
horizon:11 mse:0.23637884855270386, mae:0.38570278882980347, smape:37.25070655345917, dtw:not calculated
horizon:12 mse:0.24421948194503784, mae:0.38842126727104187, smape:37.436315417289734, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.1107933521270752, mae:0.24860653281211853, smape:24.394491314888, dtw:not calculated
average metrics: horizon upto:12 mse:0.16862419247627258, mae:0.31250765919685364, smape:30.402058362960815, dtw:not calculated
===============================================================================
average of horizons: mse:0.16862419247627258, mae:0.31250765919685364, smape:30.402058362960815, dtw:not calculated
mean smape over horizons:  30.402060722311337
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=16, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3948943614959717
Epoch: 1, Steps: 46 | Train Loss: 0.4173835 Vali Loss: 0.2162737 Test Loss: 0.2162737
Validation loss decreased (inf --> 0.216274).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0113048553466797
Epoch: 2, Steps: 46 | Train Loss: 0.2936046 Vali Loss: 0.1848391 Test Loss: 0.1848391
Validation loss decreased (0.216274 --> 0.184839).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9529497623443604
Epoch: 3, Steps: 46 | Train Loss: 0.2761389 Vali Loss: 0.1769322 Test Loss: 0.1769322
Validation loss decreased (0.184839 --> 0.176932).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0275216102600098
Epoch: 4, Steps: 46 | Train Loss: 0.2552395 Vali Loss: 0.1710219 Test Loss: 0.1710219
Validation loss decreased (0.176932 --> 0.171022).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0310814380645752
Epoch: 5, Steps: 46 | Train Loss: 0.2649216 Vali Loss: 0.1700115 Test Loss: 0.1700115
Validation loss decreased (0.171022 --> 0.170012).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0168683528900146
Epoch: 6, Steps: 46 | Train Loss: 0.2650045 Vali Loss: 0.1718063 Test Loss: 0.1718063
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.002382516860962
Epoch: 7, Steps: 46 | Train Loss: 0.2743886 Vali Loss: 0.1700157 Test Loss: 0.1700157
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0662033557891846
Epoch: 8, Steps: 46 | Train Loss: 0.2486781 Vali Loss: 0.1689763 Test Loss: 0.1689763
Validation loss decreased (0.170012 --> 0.168976).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0323333740234375
Epoch: 9, Steps: 46 | Train Loss: 0.2518751 Vali Loss: 0.1709635 Test Loss: 0.1709635
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0668699741363525
Epoch: 10, Steps: 46 | Train Loss: 0.2540666 Vali Loss: 0.1692290 Test Loss: 0.1692290
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.020807035267353058, mae:0.11110043525695801, smape:11.09161376953125, dtw:not calculated
horizon:2 mse:0.052860867232084274, mae:0.1722794473171234, smape:17.092706263065338, dtw:not calculated
horizon:3 mse:0.08996731787919998, mae:0.22243793308734894, smape:21.90639227628708, dtw:not calculated
horizon:4 mse:0.11233290284872055, mae:0.2509341537952423, smape:24.6439129114151, dtw:not calculated
horizon:5 mse:0.13840939104557037, mae:0.28136658668518066, smape:27.50527858734131, dtw:not calculated
horizon:6 mse:0.17155060172080994, mae:0.3137768805027008, smape:30.507823824882507, dtw:not calculated
horizon:7 mse:0.2497614324092865, mae:0.3683421313762665, smape:35.25687754154205, dtw:not calculated
horizon:8 mse:0.21721947193145752, mae:0.37082818150520325, smape:35.95142960548401, dtw:not calculated
horizon:9 mse:0.23283889889717102, mae:0.3735736310482025, smape:36.06563210487366, dtw:not calculated
horizon:10 mse:0.2608671188354492, mae:0.39668625593185425, smape:38.10829222202301, dtw:not calculated
horizon:11 mse:0.24503104388713837, mae:0.38728803396224976, smape:37.366774678230286, dtw:not calculated
horizon:12 mse:0.23606975376605988, mae:0.3798595070838928, smape:36.65694296360016, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09765467792749405, mae:0.2253158986568451, smape:22.124619781970978, dtw:not calculated
average metrics: horizon upto:12 mse:0.1689763069152832, mae:0.3023727834224701, smape:29.34613823890686, dtw:not calculated
===============================================================================
average of horizons: mse:0.1689763069152832, mae:0.3023727834224701, smape:29.34613823890686, dtw:not calculated
mean smape over horizons:  29.34613972902298
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=16, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4606599807739258
Epoch: 1, Steps: 46 | Train Loss: 0.4033622 Vali Loss: 0.1980249 Test Loss: 0.1980249
Validation loss decreased (inf --> 0.198025).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9879026412963867
Epoch: 2, Steps: 46 | Train Loss: 0.2882467 Vali Loss: 0.1834382 Test Loss: 0.1834382
Validation loss decreased (0.198025 --> 0.183438).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0077557563781738
Epoch: 3, Steps: 46 | Train Loss: 0.2731391 Vali Loss: 0.1750618 Test Loss: 0.1750618
Validation loss decreased (0.183438 --> 0.175062).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0756089687347412
Epoch: 4, Steps: 46 | Train Loss: 0.2522366 Vali Loss: 0.1719003 Test Loss: 0.1719003
Validation loss decreased (0.175062 --> 0.171900).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.012040376663208
Epoch: 5, Steps: 46 | Train Loss: 0.2579920 Vali Loss: 0.1703942 Test Loss: 0.1703942
Validation loss decreased (0.171900 --> 0.170394).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0559427738189697
Epoch: 6, Steps: 46 | Train Loss: 0.2587859 Vali Loss: 0.1708784 Test Loss: 0.1708784
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0680792331695557
Epoch: 7, Steps: 46 | Train Loss: 0.2723900 Vali Loss: 0.1699947 Test Loss: 0.1699947
Validation loss decreased (0.170394 --> 0.169995).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0188329219818115
Epoch: 8, Steps: 46 | Train Loss: 0.2507323 Vali Loss: 0.1696512 Test Loss: 0.1696512
Validation loss decreased (0.169995 --> 0.169651).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.041532039642334
Epoch: 9, Steps: 46 | Train Loss: 0.2484961 Vali Loss: 0.1703179 Test Loss: 0.1703179
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.024062156677246
Epoch: 10, Steps: 46 | Train Loss: 0.2501089 Vali Loss: 0.1695503 Test Loss: 0.1695503
Validation loss decreased (0.169651 --> 0.169550).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.021472491323947906, mae:0.10958665609359741, smape:10.937704145908356, dtw:not calculated
horizon:2 mse:0.04955001547932625, mae:0.16486766934394836, smape:16.359159350395203, dtw:not calculated
horizon:3 mse:0.08107925951480865, mae:0.21717220544815063, smape:21.445579826831818, dtw:not calculated
horizon:4 mse:0.10583433508872986, mae:0.2391493171453476, smape:23.482389748096466, dtw:not calculated
horizon:5 mse:0.13495099544525146, mae:0.27045056223869324, smape:26.39787495136261, dtw:not calculated
horizon:6 mse:0.1765371412038803, mae:0.3200230896472931, smape:31.099554896354675, dtw:not calculated
horizon:7 mse:0.22142411768436432, mae:0.3588712513446808, smape:34.61188077926636, dtw:not calculated
horizon:8 mse:0.23159697651863098, mae:0.38017863035202026, smape:36.74552142620087, dtw:not calculated
horizon:9 mse:0.23801784217357635, mae:0.3785932660102844, smape:36.52581572532654, dtw:not calculated
horizon:10 mse:0.29195690155029297, mae:0.42360225319862366, smape:40.52969217300415, dtw:not calculated
horizon:11 mse:0.23747043311595917, mae:0.38040801882743835, smape:36.71386241912842, dtw:not calculated
horizon:12 mse:0.24471324682235718, mae:0.38435330986976624, smape:37.0398223400116, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09490404278039932, mae:0.22020824253559113, smape:21.620377898216248, dtw:not calculated
average metrics: horizon upto:12 mse:0.1695503145456314, mae:0.30227136611938477, smape:29.32407259941101, dtw:not calculated
===============================================================================
average of horizons: mse:0.1695503145456314, mae:0.30227136611938477, smape:29.32407259941101, dtw:not calculated
mean smape over horizons:  29.32407148182392
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=16, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.42756986618042
Epoch: 1, Steps: 46 | Train Loss: 0.4498189 Vali Loss: 0.1681300 Test Loss: 0.1681300
Validation loss decreased (inf --> 0.168130).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.061971664428711
Epoch: 2, Steps: 46 | Train Loss: 0.2831276 Vali Loss: 0.1483048 Test Loss: 0.1483048
Validation loss decreased (0.168130 --> 0.148305).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9776835441589355
Epoch: 3, Steps: 46 | Train Loss: 0.2753720 Vali Loss: 0.1519415 Test Loss: 0.1519415
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0109226703643799
Epoch: 4, Steps: 46 | Train Loss: 0.2618646 Vali Loss: 0.1466809 Test Loss: 0.1466809
Validation loss decreased (0.148305 --> 0.146681).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0459468364715576
Epoch: 5, Steps: 46 | Train Loss: 0.2527558 Vali Loss: 0.1435580 Test Loss: 0.1435580
Validation loss decreased (0.146681 --> 0.143558).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0827293395996094
Epoch: 6, Steps: 46 | Train Loss: 0.2585681 Vali Loss: 0.1439041 Test Loss: 0.1439041
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.1488046646118164
Epoch: 7, Steps: 46 | Train Loss: 0.2628921 Vali Loss: 0.1435052 Test Loss: 0.1435052
Validation loss decreased (0.143558 --> 0.143505).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0921812057495117
Epoch: 8, Steps: 46 | Train Loss: 0.2452745 Vali Loss: 0.1434602 Test Loss: 0.1434602
Validation loss decreased (0.143505 --> 0.143460).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0606505870819092
Epoch: 9, Steps: 46 | Train Loss: 0.2557785 Vali Loss: 0.1442116 Test Loss: 0.1442116
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.066265344619751
Epoch: 10, Steps: 46 | Train Loss: 0.2548640 Vali Loss: 0.1440253 Test Loss: 0.1440253
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02369939349591732, mae:0.1127028539776802, smape:11.235152184963226, dtw:not calculated
horizon:2 mse:0.044873930513858795, mae:0.15825298428535461, smape:15.717074275016785, dtw:not calculated
horizon:3 mse:0.08536459505558014, mae:0.2244497388601303, smape:22.145669162273407, dtw:not calculated
horizon:4 mse:0.10109247267246246, mae:0.24246597290039062, smape:23.86229783296585, dtw:not calculated
horizon:5 mse:0.12451542913913727, mae:0.26903459429740906, smape:26.365798711776733, dtw:not calculated
horizon:6 mse:0.13531233370304108, mae:0.2851204574108124, smape:27.907952666282654, dtw:not calculated
horizon:7 mse:0.1673772633075714, mae:0.3232136070728302, smape:31.5077006816864, dtw:not calculated
horizon:8 mse:0.19054639339447021, mae:0.34808602929115295, smape:33.87987017631531, dtw:not calculated
horizon:9 mse:0.19170677661895752, mae:0.337682843208313, smape:32.80223906040192, dtw:not calculated
horizon:10 mse:0.1979677379131317, mae:0.35782480239868164, smape:34.81723666191101, dtw:not calculated
horizon:11 mse:0.22844772040843964, mae:0.38631853461265564, smape:37.442755699157715, dtw:not calculated
horizon:12 mse:0.23061831295490265, mae:0.3899282217025757, smape:37.78555691242218, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08580968528985977, mae:0.21533775329589844, smape:21.205656230449677, dtw:not calculated
average metrics: horizon upto:12 mse:0.1434601992368698, mae:0.28625673055648804, smape:27.95577347278595, dtw:not calculated
===============================================================================
average of horizons: mse:0.1434601992368698, mae:0.28625673055648804, smape:27.95577347278595, dtw:not calculated
mean smape over horizons:  27.9557753354311
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=16, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3140578269958496
Epoch: 1, Steps: 46 | Train Loss: 0.4451282 Vali Loss: 0.1678982 Test Loss: 0.1678982
Validation loss decreased (inf --> 0.167898).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1554973125457764
Epoch: 2, Steps: 46 | Train Loss: 0.2876542 Vali Loss: 0.1488681 Test Loss: 0.1488681
Validation loss decreased (0.167898 --> 0.148868).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0502736568450928
Epoch: 3, Steps: 46 | Train Loss: 0.2788345 Vali Loss: 0.1493123 Test Loss: 0.1493123
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9987645149230957
Epoch: 4, Steps: 46 | Train Loss: 0.2657170 Vali Loss: 0.1445968 Test Loss: 0.1445968
Validation loss decreased (0.148868 --> 0.144597).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.1184720993041992
Epoch: 5, Steps: 46 | Train Loss: 0.2562072 Vali Loss: 0.1407581 Test Loss: 0.1407581
Validation loss decreased (0.144597 --> 0.140758).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0870063304901123
Epoch: 6, Steps: 46 | Train Loss: 0.2668430 Vali Loss: 0.1401542 Test Loss: 0.1401542
Validation loss decreased (0.140758 --> 0.140154).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.036952018737793
Epoch: 7, Steps: 46 | Train Loss: 0.2653285 Vali Loss: 0.1397136 Test Loss: 0.1397136
Validation loss decreased (0.140154 --> 0.139714).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0420918464660645
Epoch: 8, Steps: 46 | Train Loss: 0.2485005 Vali Loss: 0.1393040 Test Loss: 0.1393040
Validation loss decreased (0.139714 --> 0.139304).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.125737190246582
Epoch: 9, Steps: 46 | Train Loss: 0.2597012 Vali Loss: 0.1398467 Test Loss: 0.1398467
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.073211669921875
Epoch: 10, Steps: 46 | Train Loss: 0.2574653 Vali Loss: 0.1404559 Test Loss: 0.1404559
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.022865429520606995, mae:0.11314224451780319, smape:11.286664009094238, dtw:not calculated
horizon:2 mse:0.044076159596443176, mae:0.1519155353307724, smape:15.079115331172943, dtw:not calculated
horizon:3 mse:0.081983283162117, mae:0.2073979675769806, smape:20.42950987815857, dtw:not calculated
horizon:4 mse:0.09642242640256882, mae:0.2360052615404129, smape:23.221218585968018, dtw:not calculated
horizon:5 mse:0.12170515209436417, mae:0.2638724148273468, smape:25.83937644958496, dtw:not calculated
horizon:6 mse:0.14385734498500824, mae:0.2918962836265564, smape:28.514328598976135, dtw:not calculated
horizon:7 mse:0.1527816206216812, mae:0.3075747787952423, smape:30.04443347454071, dtw:not calculated
horizon:8 mse:0.18676061928272247, mae:0.35010233521461487, smape:34.11775231361389, dtw:not calculated
horizon:9 mse:0.18753261864185333, mae:0.34081876277923584, smape:33.168286085128784, dtw:not calculated
horizon:10 mse:0.19323556125164032, mae:0.3471628427505493, smape:33.76155197620392, dtw:not calculated
horizon:11 mse:0.2210504710674286, mae:0.3812602162361145, smape:37.00414001941681, dtw:not calculated
horizon:12 mse:0.21937696635723114, mae:0.37642961740493774, smape:36.502912640571594, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08515162765979767, mae:0.21070496737957, smape:20.728369057178497, dtw:not calculated
average metrics: horizon upto:12 mse:0.13930396735668182, mae:0.28063151240348816, smape:27.41410732269287, dtw:not calculated
===============================================================================
average of horizons: mse:0.13930396735668182, mae:0.28063151240348816, smape:27.41410732269287, dtw:not calculated
mean smape over horizons:  27.414107446869213
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=16, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4711222648620605
Epoch: 1, Steps: 46 | Train Loss: 0.4369598 Vali Loss: 0.1684990 Test Loss: 0.1684990
Validation loss decreased (inf --> 0.168499).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0674376487731934
Epoch: 2, Steps: 46 | Train Loss: 0.2905034 Vali Loss: 0.1571868 Test Loss: 0.1571868
Validation loss decreased (0.168499 --> 0.157187).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9277575016021729
Epoch: 3, Steps: 46 | Train Loss: 0.2833810 Vali Loss: 0.1580865 Test Loss: 0.1580865
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0764787197113037
Epoch: 4, Steps: 46 | Train Loss: 0.2699565 Vali Loss: 0.1531501 Test Loss: 0.1531501
Validation loss decreased (0.157187 --> 0.153150).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0345537662506104
Epoch: 5, Steps: 46 | Train Loss: 0.2589581 Vali Loss: 0.1501341 Test Loss: 0.1501341
Validation loss decreased (0.153150 --> 0.150134).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.070694923400879
Epoch: 6, Steps: 46 | Train Loss: 0.2718654 Vali Loss: 0.1496105 Test Loss: 0.1496105
Validation loss decreased (0.150134 --> 0.149611).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0986394882202148
Epoch: 7, Steps: 46 | Train Loss: 0.2656747 Vali Loss: 0.1492579 Test Loss: 0.1492579
Validation loss decreased (0.149611 --> 0.149258).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.024137020111084
Epoch: 8, Steps: 46 | Train Loss: 0.2517079 Vali Loss: 0.1492963 Test Loss: 0.1492963
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0645451545715332
Epoch: 9, Steps: 46 | Train Loss: 0.2625540 Vali Loss: 0.1496002 Test Loss: 0.1496002
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0920298099517822
Epoch: 10, Steps: 46 | Train Loss: 0.2575592 Vali Loss: 0.1501444 Test Loss: 0.1501444
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02112250216305256, mae:0.10899786651134491, smape:10.87827980518341, dtw:not calculated
horizon:2 mse:0.050936851650476456, mae:0.1682632565498352, smape:16.689467430114746, dtw:not calculated
horizon:3 mse:0.08356035500764847, mae:0.2108360081911087, smape:20.758195221424103, dtw:not calculated
horizon:4 mse:0.10566755384206772, mae:0.24412308633327484, smape:23.975327610969543, dtw:not calculated
horizon:5 mse:0.13206754624843597, mae:0.2730150818824768, smape:26.67687237262726, dtw:not calculated
horizon:6 mse:0.16152119636535645, mae:0.3112267851829529, smape:30.335691571235657, dtw:not calculated
horizon:7 mse:0.16070394217967987, mae:0.3156236410140991, smape:30.80804944038391, dtw:not calculated
horizon:8 mse:0.1896049827337265, mae:0.35174065828323364, smape:34.27618443965912, dtw:not calculated
horizon:9 mse:0.1991414725780487, mae:0.3563038408756256, smape:34.638819098472595, dtw:not calculated
horizon:10 mse:0.21997541189193726, mae:0.36572813987731934, smape:35.39125919342041, dtw:not calculated
horizon:11 mse:0.2334723025560379, mae:0.3842831254005432, smape:37.1770977973938, dtw:not calculated
horizon:12 mse:0.2333199381828308, mae:0.389020174741745, smape:37.66183853149414, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09247932583093643, mae:0.21941034495830536, smape:21.552307903766632, dtw:not calculated
average metrics: horizon upto:12 mse:0.1492578238248825, mae:0.28993016481399536, smape:28.27225625514984, dtw:not calculated
===============================================================================
average of horizons: mse:0.1492578238248825, mae:0.28993016481399536, smape:28.27225625514984, dtw:not calculated
mean smape over horizons:  28.27225687603156
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3947455883026123
Epoch: 1, Steps: 46 | Train Loss: 0.4534887 Vali Loss: 0.2020475 Test Loss: 0.2020475
Validation loss decreased (inf --> 0.202047).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0506837368011475
Epoch: 2, Steps: 46 | Train Loss: 0.3055293 Vali Loss: 0.1947198 Test Loss: 0.1947198
Validation loss decreased (0.202047 --> 0.194720).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0126996040344238
Epoch: 3, Steps: 46 | Train Loss: 0.2814457 Vali Loss: 0.1807940 Test Loss: 0.1807940
Validation loss decreased (0.194720 --> 0.180794).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0549588203430176
Epoch: 4, Steps: 46 | Train Loss: 0.2759729 Vali Loss: 0.1735647 Test Loss: 0.1735647
Validation loss decreased (0.180794 --> 0.173565).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0616309642791748
Epoch: 5, Steps: 46 | Train Loss: 0.2752766 Vali Loss: 0.1716272 Test Loss: 0.1716272
Validation loss decreased (0.173565 --> 0.171627).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1084742546081543
Epoch: 6, Steps: 46 | Train Loss: 0.2722009 Vali Loss: 0.1728368 Test Loss: 0.1728368
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0577473640441895
Epoch: 7, Steps: 46 | Train Loss: 0.2631573 Vali Loss: 0.1717527 Test Loss: 0.1717527
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9587748050689697
Epoch: 8, Steps: 46 | Train Loss: 0.2666189 Vali Loss: 0.1723610 Test Loss: 0.1723610
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03292020782828331, mae:0.13551926612854004, smape:13.488997519016266, dtw:not calculated
horizon:2 mse:0.07100611180067062, mae:0.2036815732717514, smape:20.15652507543564, dtw:not calculated
horizon:3 mse:0.10577583312988281, mae:0.24345284700393677, smape:23.916976153850555, dtw:not calculated
horizon:4 mse:0.13158732652664185, mae:0.2720857262611389, smape:26.621010899543762, dtw:not calculated
horizon:5 mse:0.16842244565486908, mae:0.3109069764614105, smape:30.24381995201111, dtw:not calculated
horizon:6 mse:0.20112307369709015, mae:0.34609454870224, smape:33.530911803245544, dtw:not calculated
horizon:7 mse:0.19769255816936493, mae:0.34987568855285645, smape:33.98509919643402, dtw:not calculated
horizon:8 mse:0.21661272644996643, mae:0.36624300479888916, smape:35.4705274105072, dtw:not calculated
horizon:9 mse:0.2188611775636673, mae:0.3671840727329254, smape:35.549187660217285, dtw:not calculated
horizon:10 mse:0.24834802746772766, mae:0.38866424560546875, smape:37.435126304626465, dtw:not calculated
horizon:11 mse:0.2240474373102188, mae:0.3780989646911621, smape:36.63671314716339, dtw:not calculated
horizon:12 mse:0.24312952160835266, mae:0.3974388539791107, smape:38.40075433254242, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.11847250908613205, mae:0.2519568204879761, smape:24.65970665216446, dtw:not calculated
average metrics: horizon upto:12 mse:0.1716271936893463, mae:0.3132704794406891, smape:30.452972650527954, dtw:not calculated
===============================================================================
average of horizons: mse:0.1716271936893463, mae:0.3132704794406891, smape:30.452972650527954, dtw:not calculated
mean smape over horizons:  30.452970787882805
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4654066562652588
Epoch: 1, Steps: 46 | Train Loss: 0.4215380 Vali Loss: 0.1960886 Test Loss: 0.1960886
Validation loss decreased (inf --> 0.196089).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0144374370574951
Epoch: 2, Steps: 46 | Train Loss: 0.2871942 Vali Loss: 0.1762562 Test Loss: 0.1762562
Validation loss decreased (0.196089 --> 0.176256).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9635453224182129
Epoch: 3, Steps: 46 | Train Loss: 0.2673519 Vali Loss: 0.1677792 Test Loss: 0.1677792
Validation loss decreased (0.176256 --> 0.167779).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0778837203979492
Epoch: 4, Steps: 46 | Train Loss: 0.2680813 Vali Loss: 0.1644507 Test Loss: 0.1644507
Validation loss decreased (0.167779 --> 0.164451).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.004716157913208
Epoch: 5, Steps: 46 | Train Loss: 0.2614213 Vali Loss: 0.1643601 Test Loss: 0.1643601
Validation loss decreased (0.164451 --> 0.164360).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0575840473175049
Epoch: 6, Steps: 46 | Train Loss: 0.2608147 Vali Loss: 0.1628952 Test Loss: 0.1628952
Validation loss decreased (0.164360 --> 0.162895).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.054940938949585
Epoch: 7, Steps: 46 | Train Loss: 0.2532338 Vali Loss: 0.1623393 Test Loss: 0.1623393
Validation loss decreased (0.162895 --> 0.162339).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.050968885421753
Epoch: 8, Steps: 46 | Train Loss: 0.2574795 Vali Loss: 0.1627776 Test Loss: 0.1627776
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0690298080444336
Epoch: 9, Steps: 46 | Train Loss: 0.2611916 Vali Loss: 0.1643448 Test Loss: 0.1643448
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0294742584228516
Epoch: 10, Steps: 46 | Train Loss: 0.2560777 Vali Loss: 0.1633828 Test Loss: 0.1633828
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.021228833124041557, mae:0.10549203306436539, smape:10.522837936878204, dtw:not calculated
horizon:2 mse:0.06364884972572327, mae:0.19451192021369934, smape:19.276058673858643, dtw:not calculated
horizon:3 mse:0.08882571756839752, mae:0.22129522264003754, smape:21.811260282993317, dtw:not calculated
horizon:4 mse:0.10786714404821396, mae:0.24615536630153656, smape:24.184376001358032, dtw:not calculated
horizon:5 mse:0.1360168606042862, mae:0.27920904755592346, smape:27.30661928653717, dtw:not calculated
horizon:6 mse:0.17145366966724396, mae:0.32169947028160095, smape:31.325426697731018, dtw:not calculated
horizon:7 mse:0.17892810702323914, mae:0.3261309564113617, smape:31.736353039741516, dtw:not calculated
horizon:8 mse:0.22463680803775787, mae:0.37133705615997314, smape:35.90977489948273, dtw:not calculated
horizon:9 mse:0.2190416157245636, mae:0.36871832609176636, smape:35.71601212024689, dtw:not calculated
horizon:10 mse:0.23043900728225708, mae:0.377897709608078, smape:36.5459144115448, dtw:not calculated
horizon:11 mse:0.23884524405002594, mae:0.3900640308856964, smape:37.71927356719971, dtw:not calculated
horizon:12 mse:0.26713988184928894, mae:0.39963653683662415, smape:38.362523913383484, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09817350655794144, mae:0.2280605137348175, smape:22.40442931652069, dtw:not calculated
average metrics: horizon upto:12 mse:0.16233931481838226, mae:0.30017897486686707, smape:29.20137047767639, dtw:not calculated
===============================================================================
average of horizons: mse:0.16233931481838226, mae:0.30017897486686707, smape:29.20137047767639, dtw:not calculated
mean smape over horizons:  29.20136923591296
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4420702457427979
Epoch: 1, Steps: 46 | Train Loss: 0.4518481 Vali Loss: 0.1832688 Test Loss: 0.1832688
Validation loss decreased (inf --> 0.183269).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0248420238494873
Epoch: 2, Steps: 46 | Train Loss: 0.2959724 Vali Loss: 0.1761914 Test Loss: 0.1761914
Validation loss decreased (0.183269 --> 0.176191).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0616250038146973
Epoch: 3, Steps: 46 | Train Loss: 0.2717086 Vali Loss: 0.1667403 Test Loss: 0.1667403
Validation loss decreased (0.176191 --> 0.166740).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9864928722381592
Epoch: 4, Steps: 46 | Train Loss: 0.2579674 Vali Loss: 0.1660911 Test Loss: 0.1660911
Validation loss decreased (0.166740 --> 0.166091).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.026639461517334
Epoch: 5, Steps: 46 | Train Loss: 0.2638558 Vali Loss: 0.1617972 Test Loss: 0.1617972
Validation loss decreased (0.166091 --> 0.161797).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9333007335662842
Epoch: 6, Steps: 46 | Train Loss: 0.2630225 Vali Loss: 0.1605053 Test Loss: 0.1605053
Validation loss decreased (0.161797 --> 0.160505).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0604870319366455
Epoch: 7, Steps: 46 | Train Loss: 0.2638065 Vali Loss: 0.1621147 Test Loss: 0.1621147
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.057499885559082
Epoch: 8, Steps: 46 | Train Loss: 0.2496181 Vali Loss: 0.1608474 Test Loss: 0.1608474
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9896087646484375
Epoch: 9, Steps: 46 | Train Loss: 0.2565228 Vali Loss: 0.1612088 Test Loss: 0.1612088
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0214165560901165, mae:0.10878229886293411, smape:10.853452235460281, dtw:not calculated
horizon:2 mse:0.06415896117687225, mae:0.20062316954135895, smape:19.896233081817627, dtw:not calculated
horizon:3 mse:0.07740868628025055, mae:0.20885683596134186, smape:20.63066214323044, dtw:not calculated
horizon:4 mse:0.11089716851711273, mae:0.2500012218952179, smape:24.54020380973816, dtw:not calculated
horizon:5 mse:0.1559148132801056, mae:0.30507007241249084, smape:29.757142066955566, dtw:not calculated
horizon:6 mse:0.18588121235370636, mae:0.33709394931793213, smape:32.778507471084595, dtw:not calculated
horizon:7 mse:0.18207740783691406, mae:0.33327043056488037, smape:32.419875264167786, dtw:not calculated
horizon:8 mse:0.1903892159461975, mae:0.3473767638206482, smape:33.81006717681885, dtw:not calculated
horizon:9 mse:0.24400444328784943, mae:0.3933330178260803, smape:37.98215687274933, dtw:not calculated
horizon:10 mse:0.20382137596607208, mae:0.36409875750541687, smape:35.42064726352692, dtw:not calculated
horizon:11 mse:0.24872583150863647, mae:0.39972665905952454, smape:38.59109580516815, dtw:not calculated
horizon:12 mse:0.24136796593666077, mae:0.3932836353778839, smape:38.00540268421173, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10261289775371552, mae:0.23507125675678253, smape:23.07603359222412, dtw:not calculated
average metrics: horizon upto:12 mse:0.1605052947998047, mae:0.3034597635269165, smape:29.557117819786072, dtw:not calculated
===============================================================================
average of horizons: mse:0.1605052947998047, mae:0.3034597635269165, smape:29.557117819786072, dtw:not calculated
mean smape over horizons:  29.557120489577454
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4034028053283691
Epoch: 1, Steps: 46 | Train Loss: 0.4731513 Vali Loss: 0.1926688 Test Loss: 0.1926688
Validation loss decreased (inf --> 0.192669).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0429911613464355
Epoch: 2, Steps: 46 | Train Loss: 0.2912252 Vali Loss: 0.1631423 Test Loss: 0.1631423
Validation loss decreased (0.192669 --> 0.163142).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0352649688720703
Epoch: 3, Steps: 46 | Train Loss: 0.2641666 Vali Loss: 0.1575276 Test Loss: 0.1575276
Validation loss decreased (0.163142 --> 0.157528).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0369830131530762
Epoch: 4, Steps: 46 | Train Loss: 0.2570884 Vali Loss: 0.1538291 Test Loss: 0.1538291
Validation loss decreased (0.157528 --> 0.153829).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.057079792022705
Epoch: 5, Steps: 46 | Train Loss: 0.2699865 Vali Loss: 0.1542025 Test Loss: 0.1542025
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0658128261566162
Epoch: 6, Steps: 46 | Train Loss: 0.2665306 Vali Loss: 0.1553617 Test Loss: 0.1553617
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.054502010345459
Epoch: 7, Steps: 46 | Train Loss: 0.2629949 Vali Loss: 0.1541339 Test Loss: 0.1541339
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02282087132334709, mae:0.12014458328485489, smape:11.993435770273209, dtw:not calculated
horizon:2 mse:0.05509224906563759, mae:0.17950394749641418, smape:17.811131477355957, dtw:not calculated
horizon:3 mse:0.08723092079162598, mae:0.222447469830513, smape:21.934881806373596, dtw:not calculated
horizon:4 mse:0.10977155715227127, mae:0.24455508589744568, smape:23.977263271808624, dtw:not calculated
horizon:5 mse:0.14495261013507843, mae:0.2948554754257202, smape:28.835678100585938, dtw:not calculated
horizon:6 mse:0.17191924154758453, mae:0.3231342136859894, smape:31.476345658302307, dtw:not calculated
horizon:7 mse:0.17915473878383636, mae:0.3296159505844116, smape:32.055094838142395, dtw:not calculated
horizon:8 mse:0.18460452556610107, mae:0.33391913771629333, smape:32.49048888683319, dtw:not calculated
horizon:9 mse:0.21492226421833038, mae:0.3689514696598053, smape:35.777074098587036, dtw:not calculated
horizon:10 mse:0.22593837976455688, mae:0.3802492618560791, smape:36.826735734939575, dtw:not calculated
horizon:11 mse:0.215855672955513, mae:0.38187745213508606, smape:37.12875545024872, dtw:not calculated
horizon:12 mse:0.2336866706609726, mae:0.3852217495441437, smape:37.24571466445923, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09863124787807465, mae:0.230773463845253, smape:22.67145663499832, dtw:not calculated
average metrics: horizon upto:12 mse:0.15382912755012512, mae:0.297039657831192, smape:28.962719440460205, dtw:not calculated
===============================================================================
average of horizons: mse:0.15382912755012512, mae:0.297039657831192, smape:28.962719440460205, dtw:not calculated
mean smape over horizons:  28.96271664649248
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3761708736419678
Epoch: 1, Steps: 46 | Train Loss: 0.4589801 Vali Loss: 0.1647612 Test Loss: 0.1647612
Validation loss decreased (inf --> 0.164761).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9705162048339844
Epoch: 2, Steps: 46 | Train Loss: 0.2772662 Vali Loss: 0.1584488 Test Loss: 0.1584488
Validation loss decreased (0.164761 --> 0.158449).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1562292575836182
Epoch: 3, Steps: 46 | Train Loss: 0.2572220 Vali Loss: 0.1498764 Test Loss: 0.1498764
Validation loss decreased (0.158449 --> 0.149876).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0594091415405273
Epoch: 4, Steps: 46 | Train Loss: 0.2506085 Vali Loss: 0.1447073 Test Loss: 0.1447073
Validation loss decreased (0.149876 --> 0.144707).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9833478927612305
Epoch: 5, Steps: 46 | Train Loss: 0.2613555 Vali Loss: 0.1442060 Test Loss: 0.1442060
Validation loss decreased (0.144707 --> 0.144206).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0278699398040771
Epoch: 6, Steps: 46 | Train Loss: 0.2622412 Vali Loss: 0.1440262 Test Loss: 0.1440262
Validation loss decreased (0.144206 --> 0.144026).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0223579406738281
Epoch: 7, Steps: 46 | Train Loss: 0.2568139 Vali Loss: 0.1436906 Test Loss: 0.1436906
Validation loss decreased (0.144026 --> 0.143691).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0250818729400635
Epoch: 8, Steps: 46 | Train Loss: 0.2477437 Vali Loss: 0.1435859 Test Loss: 0.1435859
Validation loss decreased (0.143691 --> 0.143586).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0626084804534912
Epoch: 9, Steps: 46 | Train Loss: 0.2524149 Vali Loss: 0.1439228 Test Loss: 0.1439228
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0646605491638184
Epoch: 10, Steps: 46 | Train Loss: 0.2586777 Vali Loss: 0.1439581 Test Loss: 0.1439581
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.021503234282135963, mae:0.107884481549263, smape:10.764064639806747, dtw:not calculated
horizon:2 mse:0.04410106688737869, mae:0.16112157702445984, smape:16.017837822437286, dtw:not calculated
horizon:3 mse:0.07582234591245651, mae:0.20339038968086243, smape:20.076322555541992, dtw:not calculated
horizon:4 mse:0.10847148299217224, mae:0.23814356327056885, smape:23.318582773208618, dtw:not calculated
horizon:5 mse:0.12592390179634094, mae:0.27024462819099426, smape:26.468071341514587, dtw:not calculated
horizon:6 mse:0.1586378812789917, mae:0.3065705895423889, smape:29.898563027381897, dtw:not calculated
horizon:7 mse:0.16718627512454987, mae:0.31048575043678284, smape:30.19006848335266, dtw:not calculated
horizon:8 mse:0.17769652605056763, mae:0.3251648247241974, smape:31.646201014518738, dtw:not calculated
horizon:9 mse:0.21151326596736908, mae:0.36268165707588196, smape:35.14604568481445, dtw:not calculated
horizon:10 mse:0.221559539437294, mae:0.36880114674568176, smape:35.659632086753845, dtw:not calculated
horizon:11 mse:0.2062721848487854, mae:0.3677653968334198, smape:35.760438442230225, dtw:not calculated
horizon:12 mse:0.20434343814849854, mae:0.36029690504074097, smape:34.98981297016144, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08907665312290192, mae:0.21455919742584229, smape:21.09057456254959, dtw:not calculated
average metrics: horizon upto:12 mse:0.1435859352350235, mae:0.2818792462348938, smape:27.4946391582489, dtw:not calculated
===============================================================================
average of horizons: mse:0.1435859352350235, mae:0.2818792462348938, smape:27.4946391582489, dtw:not calculated
mean smape over horizons:  27.494636736810207
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5245065689086914
Epoch: 1, Steps: 46 | Train Loss: 0.4613172 Vali Loss: 0.1689452 Test Loss: 0.1689452
Validation loss decreased (inf --> 0.168945).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.8793685436248779
Epoch: 2, Steps: 46 | Train Loss: 0.2789365 Vali Loss: 0.1665360 Test Loss: 0.1665360
Validation loss decreased (0.168945 --> 0.166536).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9741277694702148
Epoch: 3, Steps: 46 | Train Loss: 0.2610265 Vali Loss: 0.1564251 Test Loss: 0.1564251
Validation loss decreased (0.166536 --> 0.156425).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9960250854492188
Epoch: 4, Steps: 46 | Train Loss: 0.2547776 Vali Loss: 0.1498834 Test Loss: 0.1498834
Validation loss decreased (0.156425 --> 0.149883).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0549366474151611
Epoch: 5, Steps: 46 | Train Loss: 0.2652036 Vali Loss: 0.1506826 Test Loss: 0.1506826
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0087645053863525
Epoch: 6, Steps: 46 | Train Loss: 0.2681332 Vali Loss: 0.1501272 Test Loss: 0.1501272
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0466008186340332
Epoch: 7, Steps: 46 | Train Loss: 0.2595392 Vali Loss: 0.1501918 Test Loss: 0.1501918
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.021114611998200417, mae:0.10912232846021652, smape:10.89048907160759, dtw:not calculated
horizon:2 mse:0.0491243414580822, mae:0.17042630910873413, smape:16.9336199760437, dtw:not calculated
horizon:3 mse:0.07846350967884064, mae:0.2068605273962021, smape:20.411765575408936, dtw:not calculated
horizon:4 mse:0.10690779983997345, mae:0.2411423921585083, smape:23.648729920387268, dtw:not calculated
horizon:5 mse:0.12785059213638306, mae:0.27108147740364075, smape:26.54309868812561, dtw:not calculated
horizon:6 mse:0.17434442043304443, mae:0.32078254222869873, smape:31.191793084144592, dtw:not calculated
horizon:7 mse:0.17098790407180786, mae:0.3146648108959198, smape:30.598056316375732, dtw:not calculated
horizon:8 mse:0.16998596489429474, mae:0.322540819644928, smape:31.463134288787842, dtw:not calculated
horizon:9 mse:0.22033964097499847, mae:0.37072616815567017, smape:35.88396906852722, dtw:not calculated
horizon:10 mse:0.23201201856136322, mae:0.37565332651138306, smape:36.2642765045166, dtw:not calculated
horizon:11 mse:0.22177036106586456, mae:0.3798955976963043, smape:36.86211109161377, dtw:not calculated
horizon:12 mse:0.22569969296455383, mae:0.3765754699707031, smape:36.40857636928558, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09296755492687225, mae:0.21990260481834412, smape:21.60324901342392, dtw:not calculated
average metrics: horizon upto:12 mse:0.14988338947296143, mae:0.288289338350296, smape:28.091633319854736, dtw:not calculated
===============================================================================
average of horizons: mse:0.14988338947296143, mae:0.288289338350296, smape:28.091633319854736, dtw:not calculated
mean smape over horizons:  28.09163499623537
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4779822826385498
Epoch: 1, Steps: 46 | Train Loss: 0.4529056 Vali Loss: 0.1684092 Test Loss: 0.1684092
Validation loss decreased (inf --> 0.168409).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.020653486251831
Epoch: 2, Steps: 46 | Train Loss: 0.2798136 Vali Loss: 0.1636430 Test Loss: 0.1636430
Validation loss decreased (0.168409 --> 0.163643).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.061063289642334
Epoch: 3, Steps: 46 | Train Loss: 0.2627811 Vali Loss: 0.1562510 Test Loss: 0.1562510
Validation loss decreased (0.163643 --> 0.156251).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9870293140411377
Epoch: 4, Steps: 46 | Train Loss: 0.2574386 Vali Loss: 0.1506167 Test Loss: 0.1506167
Validation loss decreased (0.156251 --> 0.150617).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0359554290771484
Epoch: 5, Steps: 46 | Train Loss: 0.2693496 Vali Loss: 0.1506248 Test Loss: 0.1506248
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0323703289031982
Epoch: 6, Steps: 46 | Train Loss: 0.2724701 Vali Loss: 0.1508226 Test Loss: 0.1508226
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9936156272888184
Epoch: 7, Steps: 46 | Train Loss: 0.2608796 Vali Loss: 0.1507323 Test Loss: 0.1507323
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.027125313878059387, mae:0.12131727486848831, smape:12.089519947767258, dtw:not calculated
horizon:2 mse:0.05332941561937332, mae:0.1746542900800705, smape:17.32773184776306, dtw:not calculated
horizon:3 mse:0.08185429871082306, mae:0.20681804418563843, smape:20.380041003227234, dtw:not calculated
horizon:4 mse:0.10305860638618469, mae:0.23765844106674194, smape:23.32942634820938, dtw:not calculated
horizon:5 mse:0.13625933229923248, mae:0.2747505009174347, smape:26.837828755378723, dtw:not calculated
horizon:6 mse:0.16668041050434113, mae:0.3132384717464447, smape:30.508312582969666, dtw:not calculated
horizon:7 mse:0.15793080627918243, mae:0.3060949444770813, smape:29.85541522502899, dtw:not calculated
horizon:8 mse:0.17419356107711792, mae:0.33196184039115906, smape:32.38908648490906, dtw:not calculated
horizon:9 mse:0.22348330914974213, mae:0.3769569396972656, smape:36.50786876678467, dtw:not calculated
horizon:10 mse:0.23491142690181732, mae:0.3885670006275177, smape:37.608227133750916, dtw:not calculated
horizon:11 mse:0.2229699194431305, mae:0.383565217256546, smape:37.23604083061218, dtw:not calculated
horizon:12 mse:0.22560377418994904, mae:0.3763737976551056, smape:36.37329041957855, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09471788257360458, mae:0.22140617668628693, smape:21.745476126670837, dtw:not calculated
average metrics: horizon upto:12 mse:0.15061667561531067, mae:0.29099640250205994, smape:28.37023437023163, dtw:not calculated
===============================================================================
average of horizons: mse:0.15061667561531067, mae:0.29099640250205994, smape:28.37023437023163, dtw:not calculated
mean smape over horizons:  28.370232445498306
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=18, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4149773120880127
Epoch: 1, Steps: 46 | Train Loss: 0.4401126 Vali Loss: 0.1662897 Test Loss: 0.1662897
Validation loss decreased (inf --> 0.166290).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1439433097839355
Epoch: 2, Steps: 46 | Train Loss: 0.2755197 Vali Loss: 0.1563491 Test Loss: 0.1563491
Validation loss decreased (0.166290 --> 0.156349).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0403857231140137
Epoch: 3, Steps: 46 | Train Loss: 0.2580254 Vali Loss: 0.1497245 Test Loss: 0.1497245
Validation loss decreased (0.156349 --> 0.149724).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0529096126556396
Epoch: 4, Steps: 46 | Train Loss: 0.2528575 Vali Loss: 0.1459215 Test Loss: 0.1459215
Validation loss decreased (0.149724 --> 0.145921).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.1152989864349365
Epoch: 5, Steps: 46 | Train Loss: 0.2647827 Vali Loss: 0.1456859 Test Loss: 0.1456859
Validation loss decreased (0.145921 --> 0.145686).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1451566219329834
Epoch: 6, Steps: 46 | Train Loss: 0.2674353 Vali Loss: 0.1465093 Test Loss: 0.1465093
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.009962797164917
Epoch: 7, Steps: 46 | Train Loss: 0.2554469 Vali Loss: 0.1464259 Test Loss: 0.1464259
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.1407763957977295
Epoch: 8, Steps: 46 | Train Loss: 0.2479534 Vali Loss: 0.1463315 Test Loss: 0.1463315
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.020104387775063515, mae:0.10673706233501434, smape:10.653973370790482, dtw:not calculated
horizon:2 mse:0.04291440546512604, mae:0.15862540900707245, smape:15.774288773536682, dtw:not calculated
horizon:3 mse:0.08011744916439056, mae:0.2032565027475357, smape:20.041285455226898, dtw:not calculated
horizon:4 mse:0.1036602035164833, mae:0.23592166602611542, smape:23.145076632499695, dtw:not calculated
horizon:5 mse:0.14090541005134583, mae:0.27827319502830505, smape:27.14996039867401, dtw:not calculated
horizon:6 mse:0.15369024872779846, mae:0.2988746762275696, smape:29.16235327720642, dtw:not calculated
horizon:7 mse:0.15140466392040253, mae:0.2987031936645508, smape:29.15845215320587, dtw:not calculated
horizon:8 mse:0.17783305048942566, mae:0.3321618437767029, smape:32.36267864704132, dtw:not calculated
horizon:9 mse:0.2100384533405304, mae:0.3623538911342621, smape:35.14642119407654, dtw:not calculated
horizon:10 mse:0.2315293550491333, mae:0.38233134150505066, smape:36.99549436569214, dtw:not calculated
horizon:11 mse:0.2143975794315338, mae:0.3734969198703766, smape:36.275407671928406, dtw:not calculated
horizon:12 mse:0.22163531184196472, mae:0.36628782749176025, smape:35.376161336898804, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.0902320146560669, mae:0.21361476182937622, smape:20.987822115421295, dtw:not calculated
average metrics: horizon upto:12 mse:0.14568588137626648, mae:0.28308531641960144, smape:27.60346531867981, dtw:not calculated
===============================================================================
average of horizons: mse:0.14568588137626648, mae:0.28308531641960144, smape:27.60346531867981, dtw:not calculated
mean smape over horizons:  27.603462773064773
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3785481452941895
Epoch: 1, Steps: 46 | Train Loss: 0.4320021 Vali Loss: 0.2078592 Test Loss: 0.2078592
Validation loss decreased (inf --> 0.207859).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1587131023406982
Epoch: 2, Steps: 46 | Train Loss: 0.3080214 Vali Loss: 0.1959467 Test Loss: 0.1959467
Validation loss decreased (0.207859 --> 0.195947).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.085240125656128
Epoch: 3, Steps: 46 | Train Loss: 0.2757864 Vali Loss: 0.1767722 Test Loss: 0.1767722
Validation loss decreased (0.195947 --> 0.176772).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9742128849029541
Epoch: 4, Steps: 46 | Train Loss: 0.2645867 Vali Loss: 0.1730388 Test Loss: 0.1730388
Validation loss decreased (0.176772 --> 0.173039).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0564565658569336
Epoch: 5, Steps: 46 | Train Loss: 0.2593966 Vali Loss: 0.1728117 Test Loss: 0.1728117
Validation loss decreased (0.173039 --> 0.172812).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1238162517547607
Epoch: 6, Steps: 46 | Train Loss: 0.2699483 Vali Loss: 0.1716953 Test Loss: 0.1716953
Validation loss decreased (0.172812 --> 0.171695).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9818143844604492
Epoch: 7, Steps: 46 | Train Loss: 0.2692877 Vali Loss: 0.1702434 Test Loss: 0.1702434
Validation loss decreased (0.171695 --> 0.170243).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0682566165924072
Epoch: 8, Steps: 46 | Train Loss: 0.2654266 Vali Loss: 0.1725607 Test Loss: 0.1725607
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9723043441772461
Epoch: 9, Steps: 46 | Train Loss: 0.2655442 Vali Loss: 0.1708499 Test Loss: 0.1708499
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.038156509399414
Epoch: 10, Steps: 46 | Train Loss: 0.2694340 Vali Loss: 0.1691149 Test Loss: 0.1691149
Validation loss decreased (0.170243 --> 0.169115).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02715177834033966, mae:0.12464527785778046, smape:12.42545023560524, dtw:not calculated
horizon:2 mse:0.05681092292070389, mae:0.179523766040802, smape:17.801527678966522, dtw:not calculated
horizon:3 mse:0.1025553047657013, mae:0.2509046792984009, smape:24.72052127122879, dtw:not calculated
horizon:4 mse:0.10933724790811539, mae:0.25012898445129395, smape:24.572665989398956, dtw:not calculated
horizon:5 mse:0.15471959114074707, mae:0.3010278344154358, smape:29.379737377166748, dtw:not calculated
horizon:6 mse:0.17415407299995422, mae:0.3282068371772766, smape:31.97612166404724, dtw:not calculated
horizon:7 mse:0.1989414542913437, mae:0.34228432178497314, smape:33.17667841911316, dtw:not calculated
horizon:8 mse:0.2330765426158905, mae:0.3751075267791748, smape:36.20724081993103, dtw:not calculated
horizon:9 mse:0.24772021174430847, mae:0.3946166932582855, smape:38.07473182678223, dtw:not calculated
horizon:10 mse:0.24071936309337616, mae:0.38628336787223816, smape:37.292614579200745, dtw:not calculated
horizon:11 mse:0.24408328533172607, mae:0.3890607953071594, smape:37.51414716243744, dtw:not calculated
horizon:12 mse:0.24010927975177765, mae:0.39352378249168396, smape:38.06904852390289, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10412148386240005, mae:0.23907288908958435, smape:23.479337990283966, dtw:not calculated
average metrics: horizon upto:12 mse:0.16911491751670837, mae:0.3096095025539398, smape:30.100873112678528, dtw:not calculated
===============================================================================
average of horizons: mse:0.16911491751670837, mae:0.3096095025539398, smape:30.100873112678528, dtw:not calculated
mean smape over horizons:  30.100873795648415
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.411374807357788
Epoch: 1, Steps: 46 | Train Loss: 0.4442616 Vali Loss: 0.2065476 Test Loss: 0.2065476
Validation loss decreased (inf --> 0.206548).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0016734600067139
Epoch: 2, Steps: 46 | Train Loss: 0.2971032 Vali Loss: 0.1788937 Test Loss: 0.1788937
Validation loss decreased (0.206548 --> 0.178894).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9889159202575684
Epoch: 3, Steps: 46 | Train Loss: 0.2855290 Vali Loss: 0.1746551 Test Loss: 0.1746551
Validation loss decreased (0.178894 --> 0.174655).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.037674903869629
Epoch: 4, Steps: 46 | Train Loss: 0.2687510 Vali Loss: 0.1762211 Test Loss: 0.1762211
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0053770542144775
Epoch: 5, Steps: 46 | Train Loss: 0.2651032 Vali Loss: 0.1768831 Test Loss: 0.1768831
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0286409854888916
Epoch: 6, Steps: 46 | Train Loss: 0.2693300 Vali Loss: 0.1706271 Test Loss: 0.1706271
Validation loss decreased (0.174655 --> 0.170627).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9565470218658447
Epoch: 7, Steps: 46 | Train Loss: 0.2655195 Vali Loss: 0.1751739 Test Loss: 0.1751739
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.959315299987793
Epoch: 8, Steps: 46 | Train Loss: 0.2620909 Vali Loss: 0.1732872 Test Loss: 0.1732872
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0760164260864258
Epoch: 9, Steps: 46 | Train Loss: 0.2481190 Vali Loss: 0.1711507 Test Loss: 0.1711507
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.024076243862509727, mae:0.11874145269393921, smape:11.846527457237244, dtw:not calculated
horizon:2 mse:0.0892404094338417, mae:0.22864454984664917, smape:22.557048499584198, dtw:not calculated
horizon:3 mse:0.0938136875629425, mae:0.22466933727264404, smape:22.092173993587494, dtw:not calculated
horizon:4 mse:0.14210470020771027, mae:0.28892675042152405, smape:28.242912888526917, dtw:not calculated
horizon:5 mse:0.15561634302139282, mae:0.30014678835868835, smape:29.270091652870178, dtw:not calculated
horizon:6 mse:0.1659925878047943, mae:0.3185376524925232, smape:31.072205305099487, dtw:not calculated
horizon:7 mse:0.17552851140499115, mae:0.32619982957839966, smape:31.76623284816742, dtw:not calculated
horizon:8 mse:0.2403540164232254, mae:0.3864101469516754, smape:37.28890120983124, dtw:not calculated
horizon:9 mse:0.22684581577777863, mae:0.37404799461364746, smape:36.16883456707001, dtw:not calculated
horizon:10 mse:0.2352762371301651, mae:0.3800290822982788, smape:36.697497963905334, dtw:not calculated
horizon:11 mse:0.26340368390083313, mae:0.40383434295654297, smape:38.8405978679657, dtw:not calculated
horizon:12 mse:0.2352728545665741, mae:0.384952574968338, smape:37.18859851360321, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.11180733144283295, mae:0.2466110736131668, smape:24.180161952972412, dtw:not calculated
average metrics: horizon upto:12 mse:0.17062707245349884, mae:0.31126168370246887, smape:30.25263547897339, dtw:not calculated
===============================================================================
average of horizons: mse:0.17062707245349884, mae:0.31126168370246887, smape:30.25263547897339, dtw:not calculated
mean smape over horizons:  30.2526352306207
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.522761344909668
Epoch: 1, Steps: 46 | Train Loss: 0.4363262 Vali Loss: 0.1825725 Test Loss: 0.1825725
Validation loss decreased (inf --> 0.182573).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.987398624420166
Epoch: 2, Steps: 46 | Train Loss: 0.2783835 Vali Loss: 0.1593866 Test Loss: 0.1593866
Validation loss decreased (0.182573 --> 0.159387).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9648089408874512
Epoch: 3, Steps: 46 | Train Loss: 0.2797467 Vali Loss: 0.1614464 Test Loss: 0.1614464
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0446524620056152
Epoch: 4, Steps: 46 | Train Loss: 0.2622433 Vali Loss: 0.1616368 Test Loss: 0.1616368
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0465328693389893
Epoch: 5, Steps: 46 | Train Loss: 0.2570023 Vali Loss: 0.1622555 Test Loss: 0.1622555
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02126658707857132, mae:0.11073365062475204, smape:11.050929874181747, dtw:not calculated
horizon:2 mse:0.06952348351478577, mae:0.19756156206130981, smape:19.533325731754303, dtw:not calculated
horizon:3 mse:0.08304239809513092, mae:0.2109789103269577, smape:20.78310400247574, dtw:not calculated
horizon:4 mse:0.14238335192203522, mae:0.28742095828056335, smape:28.08634042739868, dtw:not calculated
horizon:5 mse:0.13376349210739136, mae:0.2743341326713562, smape:26.827803254127502, dtw:not calculated
horizon:6 mse:0.15472683310508728, mae:0.3056405782699585, smape:29.84028160572052, dtw:not calculated
horizon:7 mse:0.16003744304180145, mae:0.3084970712661743, smape:30.106273293495178, dtw:not calculated
horizon:8 mse:0.22865363955497742, mae:0.38323405385017395, smape:37.09661364555359, dtw:not calculated
horizon:9 mse:0.21416933834552765, mae:0.353842169046402, smape:34.201809763908386, dtw:not calculated
horizon:10 mse:0.23010320961475372, mae:0.37945765256881714, smape:36.705175042152405, dtw:not calculated
horizon:11 mse:0.2407045215368271, mae:0.38854292035102844, smape:37.522971630096436, dtw:not calculated
horizon:12 mse:0.23426520824432373, mae:0.3798794448375702, smape:36.66418790817261, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10078436136245728, mae:0.23111163079738617, smape:22.68696427345276, dtw:not calculated
average metrics: horizon upto:12 mse:0.15938663482666016, mae:0.29834359884262085, smape:29.034900665283203, dtw:not calculated
===============================================================================
average of horizons: mse:0.15938663482666016, mae:0.29834359884262085, smape:29.034900665283203, dtw:not calculated
mean smape over horizons:  29.03490134825309
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4762029647827148
Epoch: 1, Steps: 46 | Train Loss: 0.3921149 Vali Loss: 0.1846207 Test Loss: 0.1846207
Validation loss decreased (inf --> 0.184621).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.944603443145752
Epoch: 2, Steps: 46 | Train Loss: 0.2960367 Vali Loss: 0.1629011 Test Loss: 0.1629011
Validation loss decreased (0.184621 --> 0.162901).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0372314453125
Epoch: 3, Steps: 46 | Train Loss: 0.2812792 Vali Loss: 0.1572998 Test Loss: 0.1572998
Validation loss decreased (0.162901 --> 0.157300).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9977920055389404
Epoch: 4, Steps: 46 | Train Loss: 0.2595043 Vali Loss: 0.1533334 Test Loss: 0.1533334
Validation loss decreased (0.157300 --> 0.153333).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9979476928710938
Epoch: 5, Steps: 46 | Train Loss: 0.2476767 Vali Loss: 0.1523420 Test Loss: 0.1523420
Validation loss decreased (0.153333 --> 0.152342).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9411706924438477
Epoch: 6, Steps: 46 | Train Loss: 0.2636530 Vali Loss: 0.1529403 Test Loss: 0.1529403
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9777538776397705
Epoch: 7, Steps: 46 | Train Loss: 0.2585617 Vali Loss: 0.1530657 Test Loss: 0.1530657
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0145769119262695
Epoch: 8, Steps: 46 | Train Loss: 0.2461844 Vali Loss: 0.1531566 Test Loss: 0.1531566
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.017425445839762688, mae:0.10378368198871613, smape:10.368916392326355, dtw:not calculated
horizon:2 mse:0.04243682697415352, mae:0.15489032864570618, smape:15.398052334785461, dtw:not calculated
horizon:3 mse:0.07483801245689392, mae:0.202727809548378, smape:20.01684606075287, dtw:not calculated
horizon:4 mse:0.11348772794008255, mae:0.2572961449623108, smape:25.27257204055786, dtw:not calculated
horizon:5 mse:0.12504726648330688, mae:0.2678302228450775, smape:26.24344527721405, dtw:not calculated
horizon:6 mse:0.18136721849441528, mae:0.3348405063152313, smape:32.57624804973602, dtw:not calculated
horizon:7 mse:0.1723984330892563, mae:0.3235456347465515, smape:31.52216672897339, dtw:not calculated
horizon:8 mse:0.2053118348121643, mae:0.3648909330368042, smape:35.471510887145996, dtw:not calculated
horizon:9 mse:0.20550698041915894, mae:0.357982337474823, smape:34.73421335220337, dtw:not calculated
horizon:10 mse:0.21358932554721832, mae:0.3752261698246002, smape:36.455237865448, dtw:not calculated
horizon:11 mse:0.2495041787624359, mae:0.3913280665874481, smape:37.71944046020508, dtw:not calculated
horizon:12 mse:0.2271907478570938, mae:0.3813849687576294, smape:36.936524510383606, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09243375062942505, mae:0.22022812068462372, smape:21.646013855934143, dtw:not calculated
average metrics: horizon upto:12 mse:0.15234199166297913, mae:0.2929772436618805, smape:28.559595346450806, dtw:not calculated
===============================================================================
average of horizons: mse:0.15234199166297913, mae:0.2929772436618805, smape:28.559595346450806, dtw:not calculated
mean smape over horizons:  28.559597829977672
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.446629524230957
Epoch: 1, Steps: 46 | Train Loss: 0.3978080 Vali Loss: 0.1673240 Test Loss: 0.1673240
Validation loss decreased (inf --> 0.167324).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0030434131622314
Epoch: 2, Steps: 46 | Train Loss: 0.2890016 Vali Loss: 0.1577879 Test Loss: 0.1577879
Validation loss decreased (0.167324 --> 0.157788).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0655763149261475
Epoch: 3, Steps: 46 | Train Loss: 0.2731066 Vali Loss: 0.1495813 Test Loss: 0.1495813
Validation loss decreased (0.157788 --> 0.149581).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.045732021331787
Epoch: 4, Steps: 46 | Train Loss: 0.2539185 Vali Loss: 0.1441317 Test Loss: 0.1441317
Validation loss decreased (0.149581 --> 0.144132).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0056571960449219
Epoch: 5, Steps: 46 | Train Loss: 0.2409635 Vali Loss: 0.1429138 Test Loss: 0.1429138
Validation loss decreased (0.144132 --> 0.142914).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9863123893737793
Epoch: 6, Steps: 46 | Train Loss: 0.2535854 Vali Loss: 0.1433441 Test Loss: 0.1433441
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0146512985229492
Epoch: 7, Steps: 46 | Train Loss: 0.2542005 Vali Loss: 0.1433765 Test Loss: 0.1433765
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0410187244415283
Epoch: 8, Steps: 46 | Train Loss: 0.2395702 Vali Loss: 0.1436767 Test Loss: 0.1436767
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.01880800351500511, mae:0.10612557083368301, smape:10.59882789850235, dtw:not calculated
horizon:2 mse:0.0419699102640152, mae:0.15563832223415375, smape:15.474925935268402, dtw:not calculated
horizon:3 mse:0.06754735857248306, mae:0.1876872181892395, smape:18.54812651872635, dtw:not calculated
horizon:4 mse:0.10289283096790314, mae:0.24448944628238678, smape:24.05865043401718, dtw:not calculated
horizon:5 mse:0.11368401348590851, mae:0.2546079158782959, smape:24.98294711112976, dtw:not calculated
horizon:6 mse:0.1627715677022934, mae:0.3133355379104614, smape:30.555176734924316, dtw:not calculated
horizon:7 mse:0.16345933079719543, mae:0.30629345774650574, smape:29.809078574180603, dtw:not calculated
horizon:8 mse:0.1844710409641266, mae:0.33980247378349304, smape:33.07867348194122, dtw:not calculated
horizon:9 mse:0.20814502239227295, mae:0.36008888483047485, smape:34.92918610572815, dtw:not calculated
horizon:10 mse:0.1913137286901474, mae:0.3502451777458191, smape:34.105098247528076, dtw:not calculated
horizon:11 mse:0.2333589345216751, mae:0.3829052746295929, smape:37.02522814273834, dtw:not calculated
horizon:12 mse:0.22654317319393158, mae:0.38074082136154175, smape:36.87210977077484, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08461228013038635, mae:0.21031402051448822, smape:20.703110098838806, dtw:not calculated
average metrics: horizon upto:12 mse:0.14291372895240784, mae:0.2818300127983093, smape:27.5031715631485, dtw:not calculated
===============================================================================
average of horizons: mse:0.14291372895240784, mae:0.2818300127983093, smape:27.5031715631485, dtw:not calculated
mean smape over horizons:  27.50316907962163
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4916129112243652
Epoch: 1, Steps: 46 | Train Loss: 0.4038085 Vali Loss: 0.1802465 Test Loss: 0.1802465
Validation loss decreased (inf --> 0.180247).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9952082633972168
Epoch: 2, Steps: 46 | Train Loss: 0.2924816 Vali Loss: 0.1694523 Test Loss: 0.1694523
Validation loss decreased (0.180247 --> 0.169452).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1487677097320557
Epoch: 3, Steps: 46 | Train Loss: 0.2776614 Vali Loss: 0.1592352 Test Loss: 0.1592352
Validation loss decreased (0.169452 --> 0.159235).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0199627876281738
Epoch: 4, Steps: 46 | Train Loss: 0.2567972 Vali Loss: 0.1579304 Test Loss: 0.1579304
Validation loss decreased (0.159235 --> 0.157930).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.063398838043213
Epoch: 5, Steps: 46 | Train Loss: 0.2452426 Vali Loss: 0.1557932 Test Loss: 0.1557932
Validation loss decreased (0.157930 --> 0.155793).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9673962593078613
Epoch: 6, Steps: 46 | Train Loss: 0.2549342 Vali Loss: 0.1565744 Test Loss: 0.1565744
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0536038875579834
Epoch: 7, Steps: 46 | Train Loss: 0.2576394 Vali Loss: 0.1562165 Test Loss: 0.1562165
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0938985347747803
Epoch: 8, Steps: 46 | Train Loss: 0.2418630 Vali Loss: 0.1563203 Test Loss: 0.1563203
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.01827104203402996, mae:0.1029997393488884, smape:10.285311192274094, dtw:not calculated
horizon:2 mse:0.04862791672348976, mae:0.16385112702846527, smape:16.258159279823303, dtw:not calculated
horizon:3 mse:0.07633040845394135, mae:0.20380663871765137, smape:20.114269852638245, dtw:not calculated
horizon:4 mse:0.10889130085706711, mae:0.24048484861850739, smape:23.58488142490387, dtw:not calculated
horizon:5 mse:0.1225181594491005, mae:0.26897573471069336, smape:26.384025812149048, dtw:not calculated
horizon:6 mse:0.17018143832683563, mae:0.3178461492061615, smape:30.94049096107483, dtw:not calculated
horizon:7 mse:0.1847602128982544, mae:0.32777267694473267, smape:31.815755367279053, dtw:not calculated
horizon:8 mse:0.20012056827545166, mae:0.34469205141067505, smape:33.40011537075043, dtw:not calculated
horizon:9 mse:0.23657436668872833, mae:0.38387802243232727, smape:37.065669894218445, dtw:not calculated
horizon:10 mse:0.20578019320964813, mae:0.3628697693347931, smape:35.24860739707947, dtw:not calculated
horizon:11 mse:0.2579825818538666, mae:0.4021463096141815, smape:38.72021734714508, dtw:not calculated
horizon:12 mse:0.2394806295633316, mae:0.38576436042785645, smape:37.2441291809082, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09080337733030319, mae:0.21632736921310425, smape:21.261189877986908, dtw:not calculated
average metrics: horizon upto:12 mse:0.15579324960708618, mae:0.29209062457084656, smape:28.421804308891296, dtw:not calculated
===============================================================================
average of horizons: mse:0.15579324960708618, mae:0.29209062457084656, smape:28.421804308891296, dtw:not calculated
mean smape over horizons:  28.421802756687004
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3524038791656494
Epoch: 1, Steps: 46 | Train Loss: 0.3947565 Vali Loss: 0.1812511 Test Loss: 0.1812511
Validation loss decreased (inf --> 0.181251).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.126349687576294
Epoch: 2, Steps: 46 | Train Loss: 0.2940059 Vali Loss: 0.1670539 Test Loss: 0.1670539
Validation loss decreased (0.181251 --> 0.167054).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1011252403259277
Epoch: 3, Steps: 46 | Train Loss: 0.2781353 Vali Loss: 0.1619336 Test Loss: 0.1619336
Validation loss decreased (0.167054 --> 0.161934).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0748858451843262
Epoch: 4, Steps: 46 | Train Loss: 0.2599970 Vali Loss: 0.1630084 Test Loss: 0.1630084
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0386478900909424
Epoch: 5, Steps: 46 | Train Loss: 0.2477496 Vali Loss: 0.1602708 Test Loss: 0.1602708
Validation loss decreased (0.161934 --> 0.160271).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0167961120605469
Epoch: 6, Steps: 46 | Train Loss: 0.2606023 Vali Loss: 0.1614034 Test Loss: 0.1614034
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0023763179779053
Epoch: 7, Steps: 46 | Train Loss: 0.2592019 Vali Loss: 0.1607114 Test Loss: 0.1607114
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9468157291412354
Epoch: 8, Steps: 46 | Train Loss: 0.2450824 Vali Loss: 0.1608175 Test Loss: 0.1608175
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.019243555143475533, mae:0.10634587705135345, smape:10.618846118450165, dtw:not calculated
horizon:2 mse:0.04916028678417206, mae:0.16871167719364166, smape:16.750511527061462, dtw:not calculated
horizon:3 mse:0.07603499293327332, mae:0.19953331351280212, smape:19.67531144618988, dtw:not calculated
horizon:4 mse:0.1176503449678421, mae:0.2556712031364441, smape:25.050312280654907, dtw:not calculated
horizon:5 mse:0.1260923594236374, mae:0.2648423910140991, smape:25.9218692779541, dtw:not calculated
horizon:6 mse:0.17005668580532074, mae:0.31864434480667114, smape:31.02550208568573, dtw:not calculated
horizon:7 mse:0.17852690815925598, mae:0.32026493549346924, smape:31.098395586013794, dtw:not calculated
horizon:8 mse:0.20162717998027802, mae:0.3502954840660095, smape:33.957988023757935, dtw:not calculated
horizon:9 mse:0.24252313375473022, mae:0.39312127232551575, smape:37.96941339969635, dtw:not calculated
horizon:10 mse:0.23169098794460297, mae:0.3831343948841095, smape:37.05990016460419, dtw:not calculated
horizon:11 mse:0.26180288195610046, mae:0.40392953157424927, smape:38.84523808956146, dtw:not calculated
horizon:12 mse:0.24883995950222015, mae:0.3985348343849182, smape:38.45607936382294, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09303969889879227, mae:0.21895812451839447, smape:21.507059037685394, dtw:not calculated
average metrics: horizon upto:12 mse:0.1602707803249359, mae:0.2969191074371338, smape:28.869113326072693, dtw:not calculated
===============================================================================
average of horizons: mse:0.1602707803249359, mae:0.2969191074371338, smape:28.869113326072693, dtw:not calculated
mean smape over horizons:  28.86911394695441
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4345924854278564
Epoch: 1, Steps: 46 | Train Loss: 0.4384893 Vali Loss: 0.1611785 Test Loss: 0.1611785
Validation loss decreased (inf --> 0.161178).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0751230716705322
Epoch: 2, Steps: 46 | Train Loss: 0.2906498 Vali Loss: 0.1557849 Test Loss: 0.1557849
Validation loss decreased (0.161178 --> 0.155785).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0021553039550781
Epoch: 3, Steps: 46 | Train Loss: 0.2720437 Vali Loss: 0.1499108 Test Loss: 0.1499108
Validation loss decreased (0.155785 --> 0.149911).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0620594024658203
Epoch: 4, Steps: 46 | Train Loss: 0.2597590 Vali Loss: 0.1477854 Test Loss: 0.1477854
Validation loss decreased (0.149911 --> 0.147785).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0143358707427979
Epoch: 5, Steps: 46 | Train Loss: 0.2639131 Vali Loss: 0.1461866 Test Loss: 0.1461866
Validation loss decreased (0.147785 --> 0.146187).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9753563404083252
Epoch: 6, Steps: 46 | Train Loss: 0.2650863 Vali Loss: 0.1470341 Test Loss: 0.1470341
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.00144362449646
Epoch: 7, Steps: 46 | Train Loss: 0.2665514 Vali Loss: 0.1464473 Test Loss: 0.1464473
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0663795471191406
Epoch: 8, Steps: 46 | Train Loss: 0.2436209 Vali Loss: 0.1468501 Test Loss: 0.1468501
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.024680105969309807, mae:0.1221490204334259, smape:12.188684940338135, dtw:not calculated
horizon:2 mse:0.057613760232925415, mae:0.1801319718360901, smape:17.845891416072845, dtw:not calculated
horizon:3 mse:0.07755334675312042, mae:0.20914313197135925, smape:20.64654529094696, dtw:not calculated
horizon:4 mse:0.11297012120485306, mae:0.2535351514816284, smape:24.897028505802155, dtw:not calculated
horizon:5 mse:0.11591625213623047, mae:0.25190654397010803, smape:24.678555130958557, dtw:not calculated
horizon:6 mse:0.15298870205879211, mae:0.30142468214035034, smape:29.433083534240723, dtw:not calculated
horizon:7 mse:0.16757740080356598, mae:0.3198705017566681, smape:31.175044178962708, dtw:not calculated
horizon:8 mse:0.19086959958076477, mae:0.34756791591644287, smape:33.81427228450775, dtw:not calculated
horizon:9 mse:0.19049470126628876, mae:0.3537614345550537, smape:34.47659611701965, dtw:not calculated
horizon:10 mse:0.1987220048904419, mae:0.35801345109939575, smape:34.84267294406891, dtw:not calculated
horizon:11 mse:0.23945802450180054, mae:0.3898721933364868, smape:37.68695890903473, dtw:not calculated
horizon:12 mse:0.22539547085762024, mae:0.37709563970565796, smape:36.50071620941162, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09028705209493637, mae:0.21971508860588074, smape:21.614965796470642, dtw:not calculated
average metrics: horizon upto:12 mse:0.14618661999702454, mae:0.2887059450149536, smape:28.182169795036316, dtw:not calculated
===============================================================================
average of horizons: mse:0.14618661999702454, mae:0.2887059450149536, smape:28.182169795036316, dtw:not calculated
mean smape over horizons:  28.182170788447063
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=20, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.2882413864135742
Epoch: 1, Steps: 46 | Train Loss: 0.4356475 Vali Loss: 0.1491032 Test Loss: 0.1491032
Validation loss decreased (inf --> 0.149103).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0344517230987549
Epoch: 2, Steps: 46 | Train Loss: 0.2871200 Vali Loss: 0.1549630 Test Loss: 0.1549630
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9892780780792236
Epoch: 3, Steps: 46 | Train Loss: 0.2665797 Vali Loss: 0.1478997 Test Loss: 0.1478997
Validation loss decreased (0.149103 --> 0.147900).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9372713565826416
Epoch: 4, Steps: 46 | Train Loss: 0.2577313 Vali Loss: 0.1464246 Test Loss: 0.1464246
Validation loss decreased (0.147900 --> 0.146425).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0154802799224854
Epoch: 5, Steps: 46 | Train Loss: 0.2596923 Vali Loss: 0.1446717 Test Loss: 0.1446717
Validation loss decreased (0.146425 --> 0.144672).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0948824882507324
Epoch: 6, Steps: 46 | Train Loss: 0.2645488 Vali Loss: 0.1453782 Test Loss: 0.1453782
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0174322128295898
Epoch: 7, Steps: 46 | Train Loss: 0.2640941 Vali Loss: 0.1444441 Test Loss: 0.1444441
Validation loss decreased (0.144672 --> 0.144444).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0427825450897217
Epoch: 8, Steps: 46 | Train Loss: 0.2407544 Vali Loss: 0.1449424 Test Loss: 0.1449424
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.026440143585205
Epoch: 9, Steps: 46 | Train Loss: 0.2569086 Vali Loss: 0.1456119 Test Loss: 0.1456119
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0927715301513672
Epoch: 10, Steps: 46 | Train Loss: 0.2518295 Vali Loss: 0.1451294 Test Loss: 0.1451294
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02120867744088173, mae:0.111638143658638, smape:11.144250631332397, dtw:not calculated
horizon:2 mse:0.05317886173725128, mae:0.1702176034450531, smape:16.87580943107605, dtw:not calculated
horizon:3 mse:0.0822686031460762, mae:0.21297378838062286, smape:20.98945528268814, dtw:not calculated
horizon:4 mse:0.11040029674768448, mae:0.24819687008857727, smape:24.367831647396088, dtw:not calculated
horizon:5 mse:0.12155239284038544, mae:0.2608919143676758, smape:25.54132044315338, dtw:not calculated
horizon:6 mse:0.1558351367712021, mae:0.30027511715888977, smape:29.267263412475586, dtw:not calculated
horizon:7 mse:0.1630192995071411, mae:0.3122345805168152, smape:30.424460768699646, dtw:not calculated
horizon:8 mse:0.1833573281764984, mae:0.34124624729156494, smape:33.22767615318298, dtw:not calculated
horizon:9 mse:0.18994228541851044, mae:0.35243964195251465, smape:34.337565302848816, dtw:not calculated
horizon:10 mse:0.20166273415088654, mae:0.3645181357860565, smape:35.483187437057495, dtw:not calculated
horizon:11 mse:0.22742082178592682, mae:0.3843734562397003, smape:37.25752830505371, dtw:not calculated
horizon:12 mse:0.22348296642303467, mae:0.38057586550712585, smape:36.89233660697937, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09074065834283829, mae:0.217365562915802, smape:21.364322304725647, dtw:not calculated
average metrics: horizon upto:12 mse:0.14444410800933838, mae:0.2866317927837372, smape:27.98405885696411, dtw:not calculated
===============================================================================
average of horizons: mse:0.14444410800933838, mae:0.2866317927837372, smape:27.98405885696411, dtw:not calculated
mean smape over horizons:  27.984057118495304
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5150279998779297
Epoch: 1, Steps: 46 | Train Loss: 0.4130540 Vali Loss: 0.2070320 Test Loss: 0.2070320
Validation loss decreased (inf --> 0.207032).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0067448616027832
Epoch: 2, Steps: 46 | Train Loss: 0.3095060 Vali Loss: 0.1935718 Test Loss: 0.1935718
Validation loss decreased (0.207032 --> 0.193572).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0102999210357666
Epoch: 3, Steps: 46 | Train Loss: 0.3012563 Vali Loss: 0.1774331 Test Loss: 0.1774331
Validation loss decreased (0.193572 --> 0.177433).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9920914173126221
Epoch: 4, Steps: 46 | Train Loss: 0.2744678 Vali Loss: 0.1722524 Test Loss: 0.1722524
Validation loss decreased (0.177433 --> 0.172252).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.075730323791504
Epoch: 5, Steps: 46 | Train Loss: 0.2646670 Vali Loss: 0.1695811 Test Loss: 0.1695811
Validation loss decreased (0.172252 --> 0.169581).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0933053493499756
Epoch: 6, Steps: 46 | Train Loss: 0.2752973 Vali Loss: 0.1681685 Test Loss: 0.1681685
Validation loss decreased (0.169581 --> 0.168168).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.018587350845337
Epoch: 7, Steps: 46 | Train Loss: 0.2643483 Vali Loss: 0.1688854 Test Loss: 0.1688854
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0117595195770264
Epoch: 8, Steps: 46 | Train Loss: 0.2534477 Vali Loss: 0.1693670 Test Loss: 0.1693670
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9867446422576904
Epoch: 9, Steps: 46 | Train Loss: 0.2626304 Vali Loss: 0.1670837 Test Loss: 0.1670837
Validation loss decreased (0.168168 --> 0.167084).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0660057067871094
Epoch: 10, Steps: 46 | Train Loss: 0.2634326 Vali Loss: 0.1673139 Test Loss: 0.1673139
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.029221143573522568, mae:0.1299883872270584, smape:12.956507503986359, dtw:not calculated
horizon:2 mse:0.0769512802362442, mae:0.21800877153873444, smape:21.570469439029694, dtw:not calculated
horizon:3 mse:0.10410376638174057, mae:0.24682089686393738, smape:24.290403723716736, dtw:not calculated
horizon:4 mse:0.1338818371295929, mae:0.28658798336982727, smape:28.09222638607025, dtw:not calculated
horizon:5 mse:0.1623714119195938, mae:0.3065989017486572, smape:29.86464500427246, dtw:not calculated
horizon:6 mse:0.16851967573165894, mae:0.31733623147010803, smape:30.92152178287506, dtw:not calculated
horizon:7 mse:0.1757240742444992, mae:0.32877659797668457, smape:32.038113474845886, dtw:not calculated
horizon:8 mse:0.2067030966281891, mae:0.3583848476409912, smape:34.76819396018982, dtw:not calculated
horizon:9 mse:0.21455085277557373, mae:0.36264482140541077, smape:35.13253331184387, dtw:not calculated
horizon:10 mse:0.23418191075325012, mae:0.37492460012435913, smape:36.17955446243286, dtw:not calculated
horizon:11 mse:0.24984842538833618, mae:0.38146042823791504, smape:36.61830127239227, dtw:not calculated
horizon:12 mse:0.24894678592681885, mae:0.39464858174324036, smape:38.03958296775818, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.11250818520784378, mae:0.25089016556739807, smape:24.61596131324768, dtw:not calculated
average metrics: horizon upto:12 mse:0.16708368062973022, mae:0.30884841084480286, smape:30.039334297180176, dtw:not calculated
===============================================================================
average of horizons: mse:0.16708368062973022, mae:0.30884841084480286, smape:30.039334297180176, dtw:not calculated
mean smape over horizons:  30.039337774117786
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3245737552642822
Epoch: 1, Steps: 46 | Train Loss: 0.4363683 Vali Loss: 0.1953559 Test Loss: 0.1953559
Validation loss decreased (inf --> 0.195356).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.952312707901001
Epoch: 2, Steps: 46 | Train Loss: 0.2972235 Vali Loss: 0.1719525 Test Loss: 0.1719525
Validation loss decreased (0.195356 --> 0.171953).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0682616233825684
Epoch: 3, Steps: 46 | Train Loss: 0.2824032 Vali Loss: 0.1670640 Test Loss: 0.1670640
Validation loss decreased (0.171953 --> 0.167064).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0444700717926025
Epoch: 4, Steps: 46 | Train Loss: 0.2717003 Vali Loss: 0.1629666 Test Loss: 0.1629666
Validation loss decreased (0.167064 --> 0.162967).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0479118824005127
Epoch: 5, Steps: 46 | Train Loss: 0.2751212 Vali Loss: 0.1605980 Test Loss: 0.1605980
Validation loss decreased (0.162967 --> 0.160598).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.011263370513916
Epoch: 6, Steps: 46 | Train Loss: 0.2639209 Vali Loss: 0.1591790 Test Loss: 0.1591790
Validation loss decreased (0.160598 --> 0.159179).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9648382663726807
Epoch: 7, Steps: 46 | Train Loss: 0.2732556 Vali Loss: 0.1636261 Test Loss: 0.1636261
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.018031358718872
Epoch: 8, Steps: 46 | Train Loss: 0.2541026 Vali Loss: 0.1613200 Test Loss: 0.1613200
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9978985786437988
Epoch: 9, Steps: 46 | Train Loss: 0.2530893 Vali Loss: 0.1596266 Test Loss: 0.1596266
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.024058030918240547, mae:0.12057522684335709, smape:12.031640857458115, dtw:not calculated
horizon:2 mse:0.05979924649000168, mae:0.18369901180267334, smape:18.193162977695465, dtw:not calculated
horizon:3 mse:0.07537028193473816, mae:0.20405863225460052, smape:20.15412151813507, dtw:not calculated
horizon:4 mse:0.11126868426799774, mae:0.2538822293281555, smape:24.93263930082321, dtw:not calculated
horizon:5 mse:0.14292995631694794, mae:0.2933558225631714, smape:28.704887628555298, dtw:not calculated
horizon:6 mse:0.15419644117355347, mae:0.3034583628177643, smape:29.648330807685852, dtw:not calculated
horizon:7 mse:0.21769006550312042, mae:0.36681511998176575, smape:35.51859259605408, dtw:not calculated
horizon:8 mse:0.21144326031208038, mae:0.36184555292129517, smape:35.07782220840454, dtw:not calculated
horizon:9 mse:0.20643620193004608, mae:0.359587162733078, smape:34.911561012268066, dtw:not calculated
horizon:10 mse:0.22791853547096252, mae:0.3792470097541809, smape:36.698099970817566, dtw:not calculated
horizon:11 mse:0.24313023686408997, mae:0.398031622171402, smape:38.480016589164734, dtw:not calculated
horizon:12 mse:0.2359074354171753, mae:0.3830505907535553, smape:36.989977955818176, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.0946037769317627, mae:0.22650489211082458, smape:22.277462482452393, dtw:not calculated
average metrics: horizon upto:12 mse:0.15917903184890747, mae:0.30063384771347046, smape:29.278406500816345, dtw:not calculated
===============================================================================
average of horizons: mse:0.15917903184890747, mae:0.30063384771347046, smape:29.278406500816345, dtw:not calculated
mean smape over horizons:  29.27840445190668
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4517571926116943
Epoch: 1, Steps: 46 | Train Loss: 0.4549261 Vali Loss: 0.1922207 Test Loss: 0.1922207
Validation loss decreased (inf --> 0.192221).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9873919486999512
Epoch: 2, Steps: 46 | Train Loss: 0.3016295 Vali Loss: 0.1721054 Test Loss: 0.1721054
Validation loss decreased (0.192221 --> 0.172105).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0350446701049805
Epoch: 3, Steps: 46 | Train Loss: 0.2787237 Vali Loss: 0.1637318 Test Loss: 0.1637318
Validation loss decreased (0.172105 --> 0.163732).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1618304252624512
Epoch: 4, Steps: 46 | Train Loss: 0.2719904 Vali Loss: 0.1621179 Test Loss: 0.1621179
Validation loss decreased (0.163732 --> 0.162118).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.084324836730957
Epoch: 5, Steps: 46 | Train Loss: 0.2643266 Vali Loss: 0.1628640 Test Loss: 0.1628640
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0809662342071533
Epoch: 6, Steps: 46 | Train Loss: 0.2744681 Vali Loss: 0.1609369 Test Loss: 0.1609369
Validation loss decreased (0.162118 --> 0.160937).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.057586669921875
Epoch: 7, Steps: 46 | Train Loss: 0.2601562 Vali Loss: 0.1612895 Test Loss: 0.1612895
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.1413497924804688
Epoch: 8, Steps: 46 | Train Loss: 0.2611711 Vali Loss: 0.1624318 Test Loss: 0.1624318
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0486557483673096
Epoch: 9, Steps: 46 | Train Loss: 0.2650352 Vali Loss: 0.1601223 Test Loss: 0.1601223
Validation loss decreased (0.160937 --> 0.160122).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0587048530578613
Epoch: 10, Steps: 46 | Train Loss: 0.2632892 Vali Loss: 0.1619076 Test Loss: 0.1619076
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.022692153230309486, mae:0.1150803416967392, smape:11.48136705160141, dtw:not calculated
horizon:2 mse:0.05705416202545166, mae:0.18266193568706512, smape:18.11792552471161, dtw:not calculated
horizon:3 mse:0.08747393637895584, mae:0.2173166424036026, smape:21.39020413160324, dtw:not calculated
horizon:4 mse:0.11662623286247253, mae:0.2583674192428589, smape:25.341016054153442, dtw:not calculated
horizon:5 mse:0.14773347973823547, mae:0.2909679412841797, smape:28.403937816619873, dtw:not calculated
horizon:6 mse:0.16998173296451569, mae:0.3241107761859894, smape:31.603416800498962, dtw:not calculated
horizon:7 mse:0.18876291811466217, mae:0.34037449955940247, smape:33.088454604148865, dtw:not calculated
horizon:8 mse:0.1945536732673645, mae:0.3558918833732605, smape:34.64534878730774, dtw:not calculated
horizon:9 mse:0.22247470915317535, mae:0.3765221834182739, smape:36.46746575832367, dtw:not calculated
horizon:10 mse:0.2279156744480133, mae:0.3794632852077484, smape:36.73613965511322, dtw:not calculated
horizon:11 mse:0.24139873683452606, mae:0.39669570326805115, smape:38.37158679962158, dtw:not calculated
horizon:12 mse:0.2447996884584427, mae:0.3897654712200165, smape:37.58253455162048, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10026028752326965, mae:0.23141750693321228, smape:22.72297739982605, dtw:not calculated
average metrics: horizon upto:12 mse:0.16012226045131683, mae:0.3022681772708893, smape:29.435783624649048, dtw:not calculated
===============================================================================
average of horizons: mse:0.16012226045131683, mae:0.3022681772708893, smape:29.435783624649048, dtw:not calculated
mean smape over horizons:  29.435783127943676
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5238330364227295
Epoch: 1, Steps: 46 | Train Loss: 0.4258614 Vali Loss: 0.1780641 Test Loss: 0.1780641
Validation loss decreased (inf --> 0.178064).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0405960083007812
Epoch: 2, Steps: 46 | Train Loss: 0.2822447 Vali Loss: 0.1628666 Test Loss: 0.1628666
Validation loss decreased (0.178064 --> 0.162867).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.057866096496582
Epoch: 3, Steps: 46 | Train Loss: 0.2689054 Vali Loss: 0.1565384 Test Loss: 0.1565384
Validation loss decreased (0.162867 --> 0.156538).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0603258609771729
Epoch: 4, Steps: 46 | Train Loss: 0.2583416 Vali Loss: 0.1550916 Test Loss: 0.1550916
Validation loss decreased (0.156538 --> 0.155092).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.039492130279541
Epoch: 5, Steps: 46 | Train Loss: 0.2527852 Vali Loss: 0.1553180 Test Loss: 0.1553180
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0348730087280273
Epoch: 6, Steps: 46 | Train Loss: 0.2602059 Vali Loss: 0.1537842 Test Loss: 0.1537842
Validation loss decreased (0.155092 --> 0.153784).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9492440223693848
Epoch: 7, Steps: 46 | Train Loss: 0.2507299 Vali Loss: 0.1547870 Test Loss: 0.1547870
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9860830307006836
Epoch: 8, Steps: 46 | Train Loss: 0.2506642 Vali Loss: 0.1547618 Test Loss: 0.1547618
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9978716373443604
Epoch: 9, Steps: 46 | Train Loss: 0.2530443 Vali Loss: 0.1533481 Test Loss: 0.1533481
Validation loss decreased (0.153784 --> 0.153348).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9715249538421631
Epoch: 10, Steps: 46 | Train Loss: 0.2527008 Vali Loss: 0.1542277 Test Loss: 0.1542277
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.019039735198020935, mae:0.10303152352571487, smape:10.285171866416931, dtw:not calculated
horizon:2 mse:0.05082005634903908, mae:0.17039477825164795, smape:16.913840174674988, dtw:not calculated
horizon:3 mse:0.0801578164100647, mae:0.2086791843175888, smape:20.574571192264557, dtw:not calculated
horizon:4 mse:0.10051323473453522, mae:0.23142370581626892, smape:22.729305922985077, dtw:not calculated
horizon:5 mse:0.13247555494308472, mae:0.26934754848480225, smape:26.331552863121033, dtw:not calculated
horizon:6 mse:0.1606958508491516, mae:0.3070661127567291, smape:29.941973090171814, dtw:not calculated
horizon:7 mse:0.1871042400598526, mae:0.33427461981773376, smape:32.4667364358902, dtw:not calculated
horizon:8 mse:0.19162826240062714, mae:0.34661903977394104, smape:33.71759057044983, dtw:not calculated
horizon:9 mse:0.20767773687839508, mae:0.3598288297653198, smape:34.90724265575409, dtw:not calculated
horizon:10 mse:0.22196193039417267, mae:0.377095490694046, smape:36.55176758766174, dtw:not calculated
horizon:11 mse:0.24127613008022308, mae:0.39090627431869507, smape:37.759965658187866, dtw:not calculated
horizon:12 mse:0.2468263804912567, mae:0.3942488431930542, smape:38.03073763847351, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09061704576015472, mae:0.21499048173427582, smape:21.1294025182724, dtw:not calculated
average metrics: horizon upto:12 mse:0.15334808826446533, mae:0.29107633233070374, smape:28.350871801376343, dtw:not calculated
===============================================================================
average of horizons: mse:0.15334808826446533, mae:0.29107633233070374, smape:28.350871801376343, dtw:not calculated
mean smape over horizons:  28.35087130467097
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4432661533355713
Epoch: 1, Steps: 46 | Train Loss: 0.4200190 Vali Loss: 0.1744598 Test Loss: 0.1744598
Validation loss decreased (inf --> 0.174460).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0983736515045166
Epoch: 2, Steps: 46 | Train Loss: 0.2813107 Vali Loss: 0.1646414 Test Loss: 0.1646414
Validation loss decreased (0.174460 --> 0.164641).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0378515720367432
Epoch: 3, Steps: 46 | Train Loss: 0.2719346 Vali Loss: 0.1581545 Test Loss: 0.1581545
Validation loss decreased (0.164641 --> 0.158155).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0973739624023438
Epoch: 4, Steps: 46 | Train Loss: 0.2586019 Vali Loss: 0.1554708 Test Loss: 0.1554708
Validation loss decreased (0.158155 --> 0.155471).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0418968200683594
Epoch: 5, Steps: 46 | Train Loss: 0.2537622 Vali Loss: 0.1552259 Test Loss: 0.1552259
Validation loss decreased (0.155471 --> 0.155226).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.1176066398620605
Epoch: 6, Steps: 46 | Train Loss: 0.2588741 Vali Loss: 0.1545892 Test Loss: 0.1545892
Validation loss decreased (0.155226 --> 0.154589).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0526008605957031
Epoch: 7, Steps: 46 | Train Loss: 0.2533225 Vali Loss: 0.1552060 Test Loss: 0.1552060
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0560681819915771
Epoch: 8, Steps: 46 | Train Loss: 0.2530682 Vali Loss: 0.1549142 Test Loss: 0.1549142
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.045952320098877
Epoch: 9, Steps: 46 | Train Loss: 0.2529435 Vali Loss: 0.1542038 Test Loss: 0.1542038
Validation loss decreased (0.154589 --> 0.154204).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0740866661071777
Epoch: 10, Steps: 46 | Train Loss: 0.2551246 Vali Loss: 0.1544473 Test Loss: 0.1544473
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.019691498950123787, mae:0.10427730530500412, smape:10.408824682235718, dtw:not calculated
horizon:2 mse:0.05171336606144905, mae:0.1720782071352005, smape:17.07574725151062, dtw:not calculated
horizon:3 mse:0.08038247376680374, mae:0.204163059592247, smape:20.10917216539383, dtw:not calculated
horizon:4 mse:0.10103695094585419, mae:0.23131561279296875, smape:22.710953652858734, dtw:not calculated
horizon:5 mse:0.13054382801055908, mae:0.27064234018325806, smape:26.474791765213013, dtw:not calculated
horizon:6 mse:0.16587528586387634, mae:0.31683191657066345, smape:30.892813205718994, dtw:not calculated
horizon:7 mse:0.18436285853385925, mae:0.3327409029006958, smape:32.3313444852829, dtw:not calculated
horizon:8 mse:0.19680075347423553, mae:0.35339343547821045, smape:34.3541294336319, dtw:not calculated
horizon:9 mse:0.21345019340515137, mae:0.3646613359451294, smape:35.3450745344162, dtw:not calculated
horizon:10 mse:0.22342899441719055, mae:0.379049152135849, smape:36.7432177066803, dtw:not calculated
horizon:11 mse:0.24564005434513092, mae:0.3943076431751251, smape:38.0799263715744, dtw:not calculated
horizon:12 mse:0.23751871287822723, mae:0.387337863445282, smape:37.411364912986755, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09154056012630463, mae:0.21655140817165375, smape:21.27871960401535, dtw:not calculated
average metrics: horizon upto:12 mse:0.15420374274253845, mae:0.29256653785705566, smape:28.494781255722046, dtw:not calculated
===============================================================================
average of horizons: mse:0.15420374274253845, mae:0.29256653785705566, smape:28.494781255722046, dtw:not calculated
mean smape over horizons:  28.494780013958614
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.370445728302002
Epoch: 1, Steps: 46 | Train Loss: 0.4170112 Vali Loss: 0.1729980 Test Loss: 0.1729980
Validation loss decreased (inf --> 0.172998).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0621440410614014
Epoch: 2, Steps: 46 | Train Loss: 0.2803517 Vali Loss: 0.1577206 Test Loss: 0.1577206
Validation loss decreased (0.172998 --> 0.157721).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.04854154586792
Epoch: 3, Steps: 46 | Train Loss: 0.2676281 Vali Loss: 0.1503475 Test Loss: 0.1503475
Validation loss decreased (0.157721 --> 0.150348).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1085546016693115
Epoch: 4, Steps: 46 | Train Loss: 0.2551568 Vali Loss: 0.1464025 Test Loss: 0.1464025
Validation loss decreased (0.150348 --> 0.146402).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0025990009307861
Epoch: 5, Steps: 46 | Train Loss: 0.2513934 Vali Loss: 0.1464816 Test Loss: 0.1464816
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9479436874389648
Epoch: 6, Steps: 46 | Train Loss: 0.2573687 Vali Loss: 0.1457200 Test Loss: 0.1457200
Validation loss decreased (0.146402 --> 0.145720).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0167737007141113
Epoch: 7, Steps: 46 | Train Loss: 0.2489465 Vali Loss: 0.1459683 Test Loss: 0.1459683
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0143587589263916
Epoch: 8, Steps: 46 | Train Loss: 0.2496883 Vali Loss: 0.1458700 Test Loss: 0.1458700
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0228641033172607
Epoch: 9, Steps: 46 | Train Loss: 0.2523414 Vali Loss: 0.1453235 Test Loss: 0.1453235
Validation loss decreased (0.145720 --> 0.145324).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0195388793945312
Epoch: 10, Steps: 46 | Train Loss: 0.2521599 Vali Loss: 0.1454734 Test Loss: 0.1454734
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.01792050525546074, mae:0.09888924658298492, smape:9.873472899198532, dtw:not calculated
horizon:2 mse:0.04530628025531769, mae:0.15955336391925812, smape:15.844540297985077, dtw:not calculated
horizon:3 mse:0.08009795099496841, mae:0.20356503129005432, smape:20.053328573703766, dtw:not calculated
horizon:4 mse:0.090699702501297, mae:0.217914879322052, smape:21.431124210357666, dtw:not calculated
horizon:5 mse:0.12187377363443375, mae:0.26294979453086853, smape:25.765857100486755, dtw:not calculated
horizon:6 mse:0.15901298820972443, mae:0.30837371945381165, smape:30.088016390800476, dtw:not calculated
horizon:7 mse:0.1691199392080307, mae:0.3160760998725891, smape:30.773738026618958, dtw:not calculated
horizon:8 mse:0.18464089930057526, mae:0.3369247019290924, smape:32.78651833534241, dtw:not calculated
horizon:9 mse:0.20222848653793335, mae:0.3575552701950073, smape:34.74304676055908, dtw:not calculated
horizon:10 mse:0.2207828164100647, mae:0.3739166557788849, smape:36.24148070812225, dtw:not calculated
horizon:11 mse:0.23115287721157074, mae:0.3836081326007843, smape:37.1389776468277, dtw:not calculated
horizon:12 mse:0.22104594111442566, mae:0.3746415078639984, smape:36.30283772945404, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08581852912902832, mae:0.20854102075099945, smape:20.50938904285431, dtw:not calculated
average metrics: horizon upto:12 mse:0.1453235149383545, mae:0.28283071517944336, smape:27.586913108825684, dtw:not calculated
===============================================================================
average of horizons: mse:0.1453235149383545, mae:0.28283071517944336, smape:27.586913108825684, dtw:not calculated
mean smape over horizons:  27.58691155662139
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3364665508270264
Epoch: 1, Steps: 46 | Train Loss: 0.4512864 Vali Loss: 0.1883705 Test Loss: 0.1883705
Validation loss decreased (inf --> 0.188370).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0065293312072754
Epoch: 2, Steps: 46 | Train Loss: 0.2914730 Vali Loss: 0.1735341 Test Loss: 0.1735341
Validation loss decreased (0.188370 --> 0.173534).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0997967720031738
Epoch: 3, Steps: 46 | Train Loss: 0.2632912 Vali Loss: 0.1762023 Test Loss: 0.1762023
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8883819580078125
Epoch: 4, Steps: 46 | Train Loss: 0.2643528 Vali Loss: 0.1675063 Test Loss: 0.1675063
Validation loss decreased (0.173534 --> 0.167506).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.048668384552002
Epoch: 5, Steps: 46 | Train Loss: 0.2634043 Vali Loss: 0.1660439 Test Loss: 0.1660439
Validation loss decreased (0.167506 --> 0.166044).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.086087703704834
Epoch: 6, Steps: 46 | Train Loss: 0.2575577 Vali Loss: 0.1642534 Test Loss: 0.1642534
Validation loss decreased (0.166044 --> 0.164253).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.027014970779419
Epoch: 7, Steps: 46 | Train Loss: 0.2593795 Vali Loss: 0.1637854 Test Loss: 0.1637854
Validation loss decreased (0.164253 --> 0.163785).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.082939863204956
Epoch: 8, Steps: 46 | Train Loss: 0.2567555 Vali Loss: 0.1643876 Test Loss: 0.1643876
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0108587741851807
Epoch: 9, Steps: 46 | Train Loss: 0.2646059 Vali Loss: 0.1635049 Test Loss: 0.1635049
Validation loss decreased (0.163785 --> 0.163505).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0687732696533203
Epoch: 10, Steps: 46 | Train Loss: 0.2532817 Vali Loss: 0.1635091 Test Loss: 0.1635091
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03316102549433708, mae:0.13686247169971466, smape:13.626489043235779, dtw:not calculated
horizon:2 mse:0.05275231599807739, mae:0.174387127161026, smape:17.298100888729095, dtw:not calculated
horizon:3 mse:0.0951722115278244, mae:0.23065465688705444, smape:22.699972987174988, dtw:not calculated
horizon:4 mse:0.13000835478305817, mae:0.27571067214012146, smape:27.007755637168884, dtw:not calculated
horizon:5 mse:0.1441718339920044, mae:0.296283483505249, smape:28.989630937576294, dtw:not calculated
horizon:6 mse:0.16532574594020844, mae:0.3109881281852722, smape:30.28988540172577, dtw:not calculated
horizon:7 mse:0.1985502541065216, mae:0.3539886474609375, smape:34.39889848232269, dtw:not calculated
horizon:8 mse:0.20791053771972656, mae:0.3601754903793335, smape:34.93902087211609, dtw:not calculated
horizon:9 mse:0.2219012826681137, mae:0.37818869948387146, smape:36.67142987251282, dtw:not calculated
horizon:10 mse:0.2459585964679718, mae:0.3935486376285553, smape:37.998417019844055, dtw:not calculated
horizon:11 mse:0.23535022139549255, mae:0.3825629651546478, smape:36.9511604309082, dtw:not calculated
horizon:12 mse:0.2317964732646942, mae:0.38644587993621826, smape:37.417590618133545, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10343191772699356, mae:0.23748108744621277, smape:23.318637907505035, dtw:not calculated
average metrics: horizon upto:12 mse:0.16350491344928741, mae:0.30664974451065063, smape:29.857364296913147, dtw:not calculated
===============================================================================
average of horizons: mse:0.16350491344928741, mae:0.30664974451065063, smape:29.857364296913147, dtw:not calculated
mean smape over horizons:  29.857362682620685
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.334320306777954
Epoch: 1, Steps: 46 | Train Loss: 0.4369639 Vali Loss: 0.1675738 Test Loss: 0.1675738
Validation loss decreased (inf --> 0.167574).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0198278427124023
Epoch: 2, Steps: 46 | Train Loss: 0.2765757 Vali Loss: 0.1568421 Test Loss: 0.1568421
Validation loss decreased (0.167574 --> 0.156842).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0906555652618408
Epoch: 3, Steps: 46 | Train Loss: 0.2525831 Vali Loss: 0.1631358 Test Loss: 0.1631358
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.961611270904541
Epoch: 4, Steps: 46 | Train Loss: 0.2544767 Vali Loss: 0.1573601 Test Loss: 0.1573601
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.044623851776123
Epoch: 5, Steps: 46 | Train Loss: 0.2526353 Vali Loss: 0.1557373 Test Loss: 0.1557373
Validation loss decreased (0.156842 --> 0.155737).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.031275987625122
Epoch: 6, Steps: 46 | Train Loss: 0.2523660 Vali Loss: 0.1542037 Test Loss: 0.1542037
Validation loss decreased (0.155737 --> 0.154204).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0338325500488281
Epoch: 7, Steps: 46 | Train Loss: 0.2533915 Vali Loss: 0.1539716 Test Loss: 0.1539716
Validation loss decreased (0.154204 --> 0.153972).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0352919101715088
Epoch: 8, Steps: 46 | Train Loss: 0.2493084 Vali Loss: 0.1555221 Test Loss: 0.1555221
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9914712905883789
Epoch: 9, Steps: 46 | Train Loss: 0.2544519 Vali Loss: 0.1537542 Test Loss: 0.1537542
Validation loss decreased (0.153972 --> 0.153754).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9437355995178223
Epoch: 10, Steps: 46 | Train Loss: 0.2444015 Vali Loss: 0.1532241 Test Loss: 0.1532241
Validation loss decreased (0.153754 --> 0.153224).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.023377716541290283, mae:0.11665979027748108, smape:11.639692634344101, dtw:not calculated
horizon:2 mse:0.04311559349298477, mae:0.15899522602558136, smape:15.80633819103241, dtw:not calculated
horizon:3 mse:0.0809827670454979, mae:0.21176700294017792, smape:20.896287262439728, dtw:not calculated
horizon:4 mse:0.11146277189254761, mae:0.25170794129371643, smape:24.709706008434296, dtw:not calculated
horizon:5 mse:0.1327480673789978, mae:0.28232696652412415, smape:27.661842107772827, dtw:not calculated
horizon:6 mse:0.15637458860874176, mae:0.3030352294445038, smape:29.556524753570557, dtw:not calculated
horizon:7 mse:0.182596355676651, mae:0.3378913998603821, smape:32.90155529975891, dtw:not calculated
horizon:8 mse:0.20130379498004913, mae:0.35455799102783203, smape:34.43217575550079, dtw:not calculated
horizon:9 mse:0.2182663232088089, mae:0.3727124333381653, smape:36.14679276943207, dtw:not calculated
horizon:10 mse:0.23943807184696198, mae:0.3889870345592499, smape:37.60382831096649, dtw:not calculated
horizon:11 mse:0.22733011841773987, mae:0.3787141442298889, smape:36.65004372596741, dtw:not calculated
horizon:12 mse:0.22169314324855804, mae:0.3822903335094452, smape:37.092721462249756, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09134358167648315, mae:0.22074870765209198, smape:21.71173244714737, dtw:not calculated
average metrics: horizon upto:12 mse:0.1532241255044937, mae:0.29497048258781433, smape:28.758126497268677, dtw:not calculated
===============================================================================
average of horizons: mse:0.1532241255044937, mae:0.29497048258781433, smape:28.758126497268677, dtw:not calculated
mean smape over horizons:  28.758125690122444
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4811317920684814
Epoch: 1, Steps: 46 | Train Loss: 0.4321910 Vali Loss: 0.1565854 Test Loss: 0.1565854
Validation loss decreased (inf --> 0.156585).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.015230655670166
Epoch: 2, Steps: 46 | Train Loss: 0.2734733 Vali Loss: 0.1493659 Test Loss: 0.1493659
Validation loss decreased (0.156585 --> 0.149366).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0863845348358154
Epoch: 3, Steps: 46 | Train Loss: 0.2501414 Vali Loss: 0.1563055 Test Loss: 0.1563055
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0758512020111084
Epoch: 4, Steps: 46 | Train Loss: 0.2521633 Vali Loss: 0.1525134 Test Loss: 0.1525134
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.919081449508667
Epoch: 5, Steps: 46 | Train Loss: 0.2507125 Vali Loss: 0.1528624 Test Loss: 0.1528624
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.026600725948810577, mae:0.12487271428108215, smape:12.4516099691391, dtw:not calculated
horizon:2 mse:0.04730833321809769, mae:0.16552238166332245, smape:16.432875394821167, dtw:not calculated
horizon:3 mse:0.07537785172462463, mae:0.20428568124771118, smape:20.182950794696808, dtw:not calculated
horizon:4 mse:0.1022629365324974, mae:0.24589261412620544, smape:24.19867515563965, dtw:not calculated
horizon:5 mse:0.13027025759220123, mae:0.2728860378265381, smape:26.71225070953369, dtw:not calculated
horizon:6 mse:0.1530771106481552, mae:0.3019922375679016, smape:29.474151134490967, dtw:not calculated
horizon:7 mse:0.16205567121505737, mae:0.3184770941734314, smape:31.105756759643555, dtw:not calculated
horizon:8 mse:0.20038414001464844, mae:0.35459160804748535, smape:34.443917870521545, dtw:not calculated
horizon:9 mse:0.2172759771347046, mae:0.3781505227088928, smape:36.718934774398804, dtw:not calculated
horizon:10 mse:0.24268609285354614, mae:0.39665624499320984, smape:38.36447894573212, dtw:not calculated
horizon:11 mse:0.21962012350559235, mae:0.37236952781677246, smape:36.069661378860474, dtw:not calculated
horizon:12 mse:0.2154712677001953, mae:0.38470786809921265, smape:37.42915689945221, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08914953470230103, mae:0.21924197673797607, smape:21.57541960477829, dtw:not calculated
average metrics: horizon upto:12 mse:0.1493658870458603, mae:0.29336705803871155, smape:28.632035851478577, dtw:not calculated
===============================================================================
average of horizons: mse:0.1493658870458603, mae:0.29336705803871155, smape:28.632035851478577, dtw:not calculated
mean smape over horizons:  28.632034982244175
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=22, stride=22
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3838365077972412
Epoch: 1, Steps: 46 | Train Loss: 0.4345932 Vali Loss: 0.1483478 Test Loss: 0.1483478
Validation loss decreased (inf --> 0.148348).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0896811485290527
Epoch: 2, Steps: 46 | Train Loss: 0.2731362 Vali Loss: 0.1494541 Test Loss: 0.1494541
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0161433219909668
Epoch: 3, Steps: 46 | Train Loss: 0.2517742 Vali Loss: 0.1556045 Test Loss: 0.1556045
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0315604209899902
Epoch: 4, Steps: 46 | Train Loss: 0.2540951 Vali Loss: 0.1528576 Test Loss: 0.1528576
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05754411220550537, mae:0.19045647978782654, smape:18.906177580356598, dtw:not calculated
horizon:2 mse:0.05386201664805412, mae:0.17868304252624512, smape:17.717671394348145, dtw:not calculated
horizon:3 mse:0.07182461023330688, mae:0.20372018218040466, smape:20.133842527866364, dtw:not calculated
horizon:4 mse:0.1030087023973465, mae:0.24614183604717255, smape:24.20736253261566, dtw:not calculated
horizon:5 mse:0.13400983810424805, mae:0.2823510766029358, smape:27.64972150325775, dtw:not calculated
horizon:6 mse:0.1261579841375351, mae:0.2770331799983978, smape:27.159738540649414, dtw:not calculated
horizon:7 mse:0.16133451461791992, mae:0.32354816794395447, smape:31.627988815307617, dtw:not calculated
horizon:8 mse:0.1919236034154892, mae:0.3558753728866577, smape:34.66884195804596, dtw:not calculated
horizon:9 mse:0.18312372267246246, mae:0.3600071966648102, smape:35.193440318107605, dtw:not calculated
horizon:10 mse:0.23303188383579254, mae:0.4164016544818878, smape:40.52332043647766, dtw:not calculated
horizon:11 mse:0.21454766392707825, mae:0.3817555010318756, smape:37.12944686412811, dtw:not calculated
horizon:12 mse:0.2498043179512024, mae:0.42285558581352234, smape:40.985190868377686, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09106787294149399, mae:0.22973096370697021, smape:22.629086673259735, dtw:not calculated
average metrics: horizon upto:12 mse:0.14834775030612946, mae:0.3032357692718506, smape:29.658564925193787, dtw:not calculated
===============================================================================
average of horizons: mse:0.14834775030612946, mae:0.3032357692718506, smape:29.658564925193787, dtw:not calculated
mean smape over horizons:  29.658561944961548
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5144047737121582
Epoch: 1, Steps: 46 | Train Loss: 0.4179157 Vali Loss: 0.1850586 Test Loss: 0.1850586
Validation loss decreased (inf --> 0.185059).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.092885971069336
Epoch: 2, Steps: 46 | Train Loss: 0.3048047 Vali Loss: 0.1670701 Test Loss: 0.1670701
Validation loss decreased (0.185059 --> 0.167070).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0328352451324463
Epoch: 3, Steps: 46 | Train Loss: 0.2781667 Vali Loss: 0.1805422 Test Loss: 0.1805422
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0684731006622314
Epoch: 4, Steps: 46 | Train Loss: 0.2607791 Vali Loss: 0.1593900 Test Loss: 0.1593900
Validation loss decreased (0.167070 --> 0.159390).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0306446552276611
Epoch: 5, Steps: 46 | Train Loss: 0.2569706 Vali Loss: 0.1609928 Test Loss: 0.1609928
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.031991720199585
Epoch: 6, Steps: 46 | Train Loss: 0.2701317 Vali Loss: 0.1613452 Test Loss: 0.1613452
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0760762691497803
Epoch: 7, Steps: 46 | Train Loss: 0.2500232 Vali Loss: 0.1584818 Test Loss: 0.1584818
Validation loss decreased (0.159390 --> 0.158482).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9810595512390137
Epoch: 8, Steps: 46 | Train Loss: 0.2524655 Vali Loss: 0.1572383 Test Loss: 0.1572383
Validation loss decreased (0.158482 --> 0.157238).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0633342266082764
Epoch: 9, Steps: 46 | Train Loss: 0.2610978 Vali Loss: 0.1573557 Test Loss: 0.1573557
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0637190341949463
Epoch: 10, Steps: 46 | Train Loss: 0.2581073 Vali Loss: 0.1583430 Test Loss: 0.1583430
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.026081640273332596, mae:0.1201603040099144, smape:11.977618932723999, dtw:not calculated
horizon:2 mse:0.06412050127983093, mae:0.19176623225212097, smape:18.996867537498474, dtw:not calculated
horizon:3 mse:0.08596412092447281, mae:0.2198745757341385, smape:21.68022394180298, dtw:not calculated
horizon:4 mse:0.10350562632083893, mae:0.24381643533706665, smape:23.976685106754303, dtw:not calculated
horizon:5 mse:0.14227597415447235, mae:0.28431275486946106, smape:27.76995599269867, dtw:not calculated
horizon:6 mse:0.14685259759426117, mae:0.28953245282173157, smape:28.26029062271118, dtw:not calculated
horizon:7 mse:0.17961162328720093, mae:0.3277004063129425, smape:31.867974996566772, dtw:not calculated
horizon:8 mse:0.21027065813541412, mae:0.35299983620643616, smape:34.17776823043823, dtw:not calculated
horizon:9 mse:0.22963853180408478, mae:0.3733775019645691, smape:36.057743430137634, dtw:not calculated
horizon:10 mse:0.2322215437889099, mae:0.3763731122016907, smape:36.3529235124588, dtw:not calculated
horizon:11 mse:0.22223183512687683, mae:0.37905779480934143, smape:36.76089644432068, dtw:not calculated
horizon:12 mse:0.24408479034900665, mae:0.38176053762435913, smape:36.74623370170593, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09480007737874985, mae:0.2249104529619217, smape:22.11027294397354, dtw:not calculated
average metrics: horizon upto:12 mse:0.15723828971385956, mae:0.29506099224090576, smape:28.718769550323486, dtw:not calculated
===============================================================================
average of horizons: mse:0.15723828971385956, mae:0.29506099224090576, smape:28.718769550323486, dtw:not calculated
mean smape over horizons:  28.71876520415147
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3252387046813965
Epoch: 1, Steps: 46 | Train Loss: 0.4251566 Vali Loss: 0.1982749 Test Loss: 0.1982749
Validation loss decreased (inf --> 0.198275).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0338327884674072
Epoch: 2, Steps: 46 | Train Loss: 0.3092669 Vali Loss: 0.1771825 Test Loss: 0.1771825
Validation loss decreased (0.198275 --> 0.177182).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9960789680480957
Epoch: 3, Steps: 46 | Train Loss: 0.2672046 Vali Loss: 0.1605981 Test Loss: 0.1605981
Validation loss decreased (0.177182 --> 0.160598).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.8950717449188232
Epoch: 4, Steps: 46 | Train Loss: 0.2604583 Vali Loss: 0.1579180 Test Loss: 0.1579180
Validation loss decreased (0.160598 --> 0.157918).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9941482543945312
Epoch: 5, Steps: 46 | Train Loss: 0.2565110 Vali Loss: 0.1536999 Test Loss: 0.1536999
Validation loss decreased (0.157918 --> 0.153700).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0121386051177979
Epoch: 6, Steps: 46 | Train Loss: 0.2588119 Vali Loss: 0.1572033 Test Loss: 0.1572033
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0214002132415771
Epoch: 7, Steps: 46 | Train Loss: 0.2684429 Vali Loss: 0.1535077 Test Loss: 0.1535077
Validation loss decreased (0.153700 --> 0.153508).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9774069786071777
Epoch: 8, Steps: 46 | Train Loss: 0.2504708 Vali Loss: 0.1570023 Test Loss: 0.1570023
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0248522758483887
Epoch: 9, Steps: 46 | Train Loss: 0.2704565 Vali Loss: 0.1569500 Test Loss: 0.1569500
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0733826160430908
Epoch: 10, Steps: 46 | Train Loss: 0.2561219 Vali Loss: 0.1572596 Test Loss: 0.1572596
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.019030964002013206, mae:0.10664118081331253, smape:10.650372505187988, dtw:not calculated
horizon:2 mse:0.05977098271250725, mae:0.18664364516735077, smape:18.50358694791794, dtw:not calculated
horizon:3 mse:0.08428199589252472, mae:0.21712930500507355, smape:21.41939103603363, dtw:not calculated
horizon:4 mse:0.09384118765592575, mae:0.22547860443592072, smape:22.185218334197998, dtw:not calculated
horizon:5 mse:0.11690235137939453, mae:0.2629637122154236, smape:25.8247971534729, dtw:not calculated
horizon:6 mse:0.1837943196296692, mae:0.3427875339984894, smape:33.38944911956787, dtw:not calculated
horizon:7 mse:0.1788501888513565, mae:0.3269503116607666, smape:31.79250657558441, dtw:not calculated
horizon:8 mse:0.18097642064094543, mae:0.33337095379829407, smape:32.46181309223175, dtw:not calculated
horizon:9 mse:0.22375065088272095, mae:0.37553924322128296, smape:36.35740578174591, dtw:not calculated
horizon:10 mse:0.20947828888893127, mae:0.36343181133270264, smape:35.25598645210266, dtw:not calculated
horizon:11 mse:0.2544586658477783, mae:0.39447328448295593, smape:37.96561658382416, dtw:not calculated
horizon:12 mse:0.23695622384548187, mae:0.38571685552597046, smape:37.25972771644592, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09293696284294128, mae:0.22360733151435852, smape:21.995466947555542, dtw:not calculated
average metrics: horizon upto:12 mse:0.15350769460201263, mae:0.2934271991252899, smape:28.58882248401642, dtw:not calculated
===============================================================================
average of horizons: mse:0.15350769460201263, mae:0.2934271991252899, smape:28.58882248401642, dtw:not calculated
mean smape over horizons:  28.58882260819276
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4079229831695557
Epoch: 1, Steps: 46 | Train Loss: 0.4264537 Vali Loss: 0.1999167 Test Loss: 0.1999167
Validation loss decreased (inf --> 0.199917).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0770771503448486
Epoch: 2, Steps: 46 | Train Loss: 0.2980296 Vali Loss: 0.1731387 Test Loss: 0.1731387
Validation loss decreased (0.199917 --> 0.173139).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9411864280700684
Epoch: 3, Steps: 46 | Train Loss: 0.2854551 Vali Loss: 0.1690971 Test Loss: 0.1690971
Validation loss decreased (0.173139 --> 0.169097).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9716928005218506
Epoch: 4, Steps: 46 | Train Loss: 0.2633489 Vali Loss: 0.1650071 Test Loss: 0.1650071
Validation loss decreased (0.169097 --> 0.165007).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9987466335296631
Epoch: 5, Steps: 46 | Train Loss: 0.2691988 Vali Loss: 0.1654934 Test Loss: 0.1654934
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0422370433807373
Epoch: 6, Steps: 46 | Train Loss: 0.2681490 Vali Loss: 0.1612663 Test Loss: 0.1612663
Validation loss decreased (0.165007 --> 0.161266).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0351214408874512
Epoch: 7, Steps: 46 | Train Loss: 0.2701140 Vali Loss: 0.1615121 Test Loss: 0.1615121
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0369296073913574
Epoch: 8, Steps: 46 | Train Loss: 0.2528698 Vali Loss: 0.1627629 Test Loss: 0.1627629
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9756143093109131
Epoch: 9, Steps: 46 | Train Loss: 0.2577786 Vali Loss: 0.1613379 Test Loss: 0.1613379
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.022706294432282448, mae:0.11499124765396118, smape:11.473522335290909, dtw:not calculated
horizon:2 mse:0.05981605499982834, mae:0.18499796092510223, smape:18.338817358016968, dtw:not calculated
horizon:3 mse:0.08432220667600632, mae:0.21481473743915558, smape:21.178729832172394, dtw:not calculated
horizon:4 mse:0.11567729711532593, mae:0.25620850920677185, smape:25.14151632785797, dtw:not calculated
horizon:5 mse:0.14763802289962769, mae:0.29817768931388855, smape:29.13825809955597, dtw:not calculated
horizon:6 mse:0.14994797110557556, mae:0.29881399869918823, smape:29.185613989830017, dtw:not calculated
horizon:7 mse:0.18168479204177856, mae:0.33206918835639954, smape:32.30845630168915, dtw:not calculated
horizon:8 mse:0.21572402119636536, mae:0.3655480742454529, smape:35.42170822620392, dtw:not calculated
horizon:9 mse:0.2167658805847168, mae:0.3618828356266022, smape:35.016876459121704, dtw:not calculated
horizon:10 mse:0.2366800457239151, mae:0.38409513235092163, smape:37.09460198879242, dtw:not calculated
horizon:11 mse:0.2573646903038025, mae:0.3962657153606415, smape:38.136082887649536, dtw:not calculated
horizon:12 mse:0.24686776101589203, mae:0.3957197666168213, smape:38.17644119262695, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09668464213609695, mae:0.2280006855726242, smape:22.409409284591675, dtw:not calculated
average metrics: horizon upto:12 mse:0.1612662374973297, mae:0.3002987504005432, smape:29.21755313873291, dtw:not calculated
===============================================================================
average of horizons: mse:0.1612662374973297, mae:0.3002987504005432, smape:29.21755313873291, dtw:not calculated
mean smape over horizons:  29.217552083233993
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3822596073150635
Epoch: 1, Steps: 46 | Train Loss: 0.4275345 Vali Loss: 0.1788384 Test Loss: 0.1788384
Validation loss decreased (inf --> 0.178838).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0409247875213623
Epoch: 2, Steps: 46 | Train Loss: 0.2902107 Vali Loss: 0.1742807 Test Loss: 0.1742807
Validation loss decreased (0.178838 --> 0.174281).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9471838474273682
Epoch: 3, Steps: 46 | Train Loss: 0.2771056 Vali Loss: 0.1708589 Test Loss: 0.1708589
Validation loss decreased (0.174281 --> 0.170859).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9800052642822266
Epoch: 4, Steps: 46 | Train Loss: 0.2552948 Vali Loss: 0.1682179 Test Loss: 0.1682179
Validation loss decreased (0.170859 --> 0.168218).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0813159942626953
Epoch: 5, Steps: 46 | Train Loss: 0.2634071 Vali Loss: 0.1678993 Test Loss: 0.1678993
Validation loss decreased (0.168218 --> 0.167899).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0470693111419678
Epoch: 6, Steps: 46 | Train Loss: 0.2627202 Vali Loss: 0.1647617 Test Loss: 0.1647617
Validation loss decreased (0.167899 --> 0.164762).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0236074924468994
Epoch: 7, Steps: 46 | Train Loss: 0.2642431 Vali Loss: 0.1647198 Test Loss: 0.1647198
Validation loss decreased (0.164762 --> 0.164720).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0023362636566162
Epoch: 8, Steps: 46 | Train Loss: 0.2468460 Vali Loss: 0.1656053 Test Loss: 0.1656053
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0611662864685059
Epoch: 9, Steps: 46 | Train Loss: 0.2535863 Vali Loss: 0.1647217 Test Loss: 0.1647217
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9370641708374023
Epoch: 10, Steps: 46 | Train Loss: 0.2640517 Vali Loss: 0.1667835 Test Loss: 0.1667835
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.018210552632808685, mae:0.09713593870401382, smape:9.693959355354309, dtw:not calculated
horizon:2 mse:0.05783938989043236, mae:0.18428584933280945, smape:18.272827565670013, dtw:not calculated
horizon:3 mse:0.08895330131053925, mae:0.2249857485294342, smape:22.174938023090363, dtw:not calculated
horizon:4 mse:0.12122759222984314, mae:0.2629840672016144, smape:25.789615511894226, dtw:not calculated
horizon:5 mse:0.1412784606218338, mae:0.29175591468811035, smape:28.546923398971558, dtw:not calculated
horizon:6 mse:0.16081929206848145, mae:0.31022703647613525, smape:30.263754725456238, dtw:not calculated
horizon:7 mse:0.17948319017887115, mae:0.3263076841831207, smape:31.744033098220825, dtw:not calculated
horizon:8 mse:0.22809569537639618, mae:0.3756009638309479, smape:36.30988895893097, dtw:not calculated
horizon:9 mse:0.2261025607585907, mae:0.3684859275817871, smape:35.605016350746155, dtw:not calculated
horizon:10 mse:0.2489091157913208, mae:0.39665645360946655, smape:38.27683925628662, dtw:not calculated
horizon:11 mse:0.2608998119831085, mae:0.4045102596282959, smape:38.95942270755768, dtw:not calculated
horizon:12 mse:0.24481834471225739, mae:0.3981596529483795, smape:38.476571440696716, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09805476665496826, mae:0.22856241464614868, smape:22.457002103328705, dtw:not calculated
average metrics: horizon upto:12 mse:0.16471977531909943, mae:0.3034246265888214, smape:29.50948476791382, dtw:not calculated
===============================================================================
average of horizons: mse:0.16471977531909943, mae:0.3034246265888214, smape:29.50948476791382, dtw:not calculated
mean smape over horizons:  29.50948253273964
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3928132057189941
Epoch: 1, Steps: 46 | Train Loss: 0.4180302 Vali Loss: 0.1627203 Test Loss: 0.1627203
Validation loss decreased (inf --> 0.162720).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0687851905822754
Epoch: 2, Steps: 46 | Train Loss: 0.2838154 Vali Loss: 0.1729031 Test Loss: 0.1729031
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.952805757522583
Epoch: 3, Steps: 46 | Train Loss: 0.2689177 Vali Loss: 0.1717225 Test Loss: 0.1717225
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1400837898254395
Epoch: 4, Steps: 46 | Train Loss: 0.2498219 Vali Loss: 0.1723618 Test Loss: 0.1723618
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0369594506919384, mae:0.14742937684059143, smape:14.672978222370148, dtw:not calculated
horizon:2 mse:0.09437362849712372, mae:0.24952135980129242, smape:24.646970629692078, dtw:not calculated
horizon:3 mse:0.10251583158969879, mae:0.2455248087644577, smape:24.159231781959534, dtw:not calculated
horizon:4 mse:0.12291508167982101, mae:0.26850900053977966, smape:26.339244842529297, dtw:not calculated
horizon:5 mse:0.13565288484096527, mae:0.27878835797309875, smape:27.26690173149109, dtw:not calculated
horizon:6 mse:0.1586458832025528, mae:0.3123926818370819, smape:30.51041066646576, dtw:not calculated
horizon:7 mse:0.16412699222564697, mae:0.31570756435394287, smape:30.79732060432434, dtw:not calculated
horizon:8 mse:0.2116585671901703, mae:0.37122347950935364, smape:36.043137311935425, dtw:not calculated
horizon:9 mse:0.20049120485782623, mae:0.35285136103630066, smape:34.26136076450348, dtw:not calculated
horizon:10 mse:0.24036885797977448, mae:0.4062686860561371, smape:39.388906955718994, dtw:not calculated
horizon:11 mse:0.23959685862064362, mae:0.38794994354248047, smape:37.443387508392334, dtw:not calculated
horizon:12 mse:0.2453378289937973, mae:0.39776477217674255, smape:38.44200372695923, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10851046442985535, mae:0.2503609359264374, smape:24.599291384220123, dtw:not calculated
average metrics: horizon upto:12 mse:0.16272026300430298, mae:0.31116095185279846, smape:30.33098876476288, dtw:not calculated
===============================================================================
average of horizons: mse:0.16272026300430298, mae:0.31116095185279846, smape:30.33098876476288, dtw:not calculated
mean smape over horizons:  30.330987895528477
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4313068389892578
Epoch: 1, Steps: 46 | Train Loss: 0.4516814 Vali Loss: 0.1803772 Test Loss: 0.1803772
Validation loss decreased (inf --> 0.180377).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9149501323699951
Epoch: 2, Steps: 46 | Train Loss: 0.3016641 Vali Loss: 0.1571869 Test Loss: 0.1571869
Validation loss decreased (0.180377 --> 0.157187).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0484941005706787
Epoch: 3, Steps: 46 | Train Loss: 0.2894473 Vali Loss: 0.1664464 Test Loss: 0.1664464
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0418376922607422
Epoch: 4, Steps: 46 | Train Loss: 0.2717009 Vali Loss: 0.1547082 Test Loss: 0.1547082
Validation loss decreased (0.157187 --> 0.154708).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0345423221588135
Epoch: 5, Steps: 46 | Train Loss: 0.2694581 Vali Loss: 0.1543322 Test Loss: 0.1543322
Validation loss decreased (0.154708 --> 0.154332).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0109498500823975
Epoch: 6, Steps: 46 | Train Loss: 0.2638402 Vali Loss: 0.1540009 Test Loss: 0.1540009
Validation loss decreased (0.154332 --> 0.154001).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.023543119430542
Epoch: 7, Steps: 46 | Train Loss: 0.2626016 Vali Loss: 0.1518463 Test Loss: 0.1518463
Validation loss decreased (0.154001 --> 0.151846).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0864548683166504
Epoch: 8, Steps: 46 | Train Loss: 0.2570619 Vali Loss: 0.1537382 Test Loss: 0.1537382
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.054483413696289
Epoch: 9, Steps: 46 | Train Loss: 0.2685179 Vali Loss: 0.1517254 Test Loss: 0.1517254
Validation loss decreased (0.151846 --> 0.151725).  Saving model ...
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 0.9688875675201416
Epoch: 10, Steps: 46 | Train Loss: 0.2686441 Vali Loss: 0.1521017 Test Loss: 0.1521017
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02809739299118519, mae:0.1237393468618393, smape:12.32687160372734, dtw:not calculated
horizon:2 mse:0.06298742443323135, mae:0.18681839108467102, smape:18.48871409893036, dtw:not calculated
horizon:3 mse:0.07924003899097443, mae:0.21008941531181335, smape:20.740200579166412, dtw:not calculated
horizon:4 mse:0.11206458508968353, mae:0.25621846318244934, smape:25.169920921325684, dtw:not calculated
horizon:5 mse:0.13310685753822327, mae:0.2805899381637573, smape:27.492767572402954, dtw:not calculated
horizon:6 mse:0.16199073195457458, mae:0.3134300708770752, smape:30.589962005615234, dtw:not calculated
horizon:7 mse:0.17528477311134338, mae:0.3323725461959839, smape:32.40400552749634, dtw:not calculated
horizon:8 mse:0.19633513689041138, mae:0.34804803133010864, smape:33.81507396697998, dtw:not calculated
horizon:9 mse:0.203258216381073, mae:0.3622628450393677, smape:35.227170586586, dtw:not calculated
horizon:10 mse:0.21163269877433777, mae:0.37235432863235474, smape:36.18552386760712, dtw:not calculated
horizon:11 mse:0.22945599257946014, mae:0.38007068634033203, smape:36.75784468650818, dtw:not calculated
horizon:12 mse:0.227250337600708, mae:0.37871527671813965, smape:36.63486838340759, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.0962478443980217, mae:0.22848094999790192, smape:22.468073666095734, dtw:not calculated
average metrics: horizon upto:12 mse:0.15172535181045532, mae:0.2953924238681793, smape:28.819409012794495, dtw:not calculated
===============================================================================
average of horizons: mse:0.15172535181045532, mae:0.2953924238681793, smape:28.819409012794495, dtw:not calculated
mean smape over horizons:  28.8194103166461
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.466705322265625
Epoch: 1, Steps: 46 | Train Loss: 0.4479601 Vali Loss: 0.1641620 Test Loss: 0.1641620
Validation loss decreased (inf --> 0.164162).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0890178680419922
Epoch: 2, Steps: 46 | Train Loss: 0.2936213 Vali Loss: 0.1548005 Test Loss: 0.1548005
Validation loss decreased (0.164162 --> 0.154801).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.003108263015747
Epoch: 3, Steps: 46 | Train Loss: 0.2815518 Vali Loss: 0.1564194 Test Loss: 0.1564194
EarlyStopping counter: 1 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0889956951141357
Epoch: 4, Steps: 46 | Train Loss: 0.2669953 Vali Loss: 0.1504630 Test Loss: 0.1504630
Validation loss decreased (0.154801 --> 0.150463).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.000044345855713
Epoch: 5, Steps: 46 | Train Loss: 0.2614376 Vali Loss: 0.1507825 Test Loss: 0.1507825
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0198800563812256
Epoch: 6, Steps: 46 | Train Loss: 0.2587745 Vali Loss: 0.1501357 Test Loss: 0.1501357
Validation loss decreased (0.150463 --> 0.150136).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0054957866668701
Epoch: 7, Steps: 46 | Train Loss: 0.2576814 Vali Loss: 0.1486504 Test Loss: 0.1486504
Validation loss decreased (0.150136 --> 0.148650).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0144920349121094
Epoch: 8, Steps: 46 | Train Loss: 0.2519475 Vali Loss: 0.1492225 Test Loss: 0.1492225
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0595757961273193
Epoch: 9, Steps: 46 | Train Loss: 0.2606053 Vali Loss: 0.1489762 Test Loss: 0.1489762
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0483827590942383
Epoch: 10, Steps: 46 | Train Loss: 0.2634303 Vali Loss: 0.1492331 Test Loss: 0.1492331
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0268030297011137, mae:0.1240096241235733, smape:12.360832840204239, dtw:not calculated
horizon:2 mse:0.05609296262264252, mae:0.1817711591720581, smape:18.02970916032791, dtw:not calculated
horizon:3 mse:0.07531348615884781, mae:0.20532700419425964, smape:20.287102460861206, dtw:not calculated
horizon:4 mse:0.11252129822969437, mae:0.2585766911506653, smape:25.413790345191956, dtw:not calculated
horizon:5 mse:0.12856492400169373, mae:0.27178242802619934, smape:26.621785759925842, dtw:not calculated
horizon:6 mse:0.15318861603736877, mae:0.3048650622367859, smape:29.794761538505554, dtw:not calculated
horizon:7 mse:0.1852235496044159, mae:0.33992287516593933, smape:33.088430762290955, dtw:not calculated
horizon:8 mse:0.19579501450061798, mae:0.347687691450119, smape:33.789971470832825, dtw:not calculated
horizon:9 mse:0.196584552526474, mae:0.36041003465652466, smape:35.10652184486389, dtw:not calculated
horizon:10 mse:0.20926935970783234, mae:0.3735146224498749, smape:36.336350440979004, dtw:not calculated
horizon:11 mse:0.22845794260501862, mae:0.3802258372306824, smape:36.807551980018616, dtw:not calculated
horizon:12 mse:0.21599021553993225, mae:0.3705785274505615, smape:35.942354798316956, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09208071976900101, mae:0.22438865900039673, smape:22.084665298461914, dtw:not calculated
average metrics: horizon upto:12 mse:0.1486504077911377, mae:0.2932226061820984, smape:28.631597757339478, dtw:not calculated
===============================================================================
average of horizons: mse:0.1486504077911377, mae:0.2932226061820984, smape:28.631597757339478, dtw:not calculated
mean smape over horizons:  28.631596950193245
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.469559669494629
Epoch: 1, Steps: 46 | Train Loss: 0.4462516 Vali Loss: 0.1622179 Test Loss: 0.1622179
Validation loss decreased (inf --> 0.162218).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.1451416015625
Epoch: 2, Steps: 46 | Train Loss: 0.2885794 Vali Loss: 0.1582691 Test Loss: 0.1582691
Validation loss decreased (0.162218 --> 0.158269).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0110137462615967
Epoch: 3, Steps: 46 | Train Loss: 0.2770990 Vali Loss: 0.1504568 Test Loss: 0.1504568
Validation loss decreased (0.158269 --> 0.150457).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.139972448348999
Epoch: 4, Steps: 46 | Train Loss: 0.2633900 Vali Loss: 0.1495644 Test Loss: 0.1495644
Validation loss decreased (0.150457 --> 0.149564).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0072078704833984
Epoch: 5, Steps: 46 | Train Loss: 0.2547508 Vali Loss: 0.1499651 Test Loss: 0.1499651
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0225603580474854
Epoch: 6, Steps: 46 | Train Loss: 0.2556940 Vali Loss: 0.1501146 Test Loss: 0.1501146
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0768892765045166
Epoch: 7, Steps: 46 | Train Loss: 0.2537427 Vali Loss: 0.1481345 Test Loss: 0.1481345
Validation loss decreased (0.149564 --> 0.148134).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0723614692687988
Epoch: 8, Steps: 46 | Train Loss: 0.2483595 Vali Loss: 0.1481074 Test Loss: 0.1481074
Validation loss decreased (0.148134 --> 0.148107).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9556665420532227
Epoch: 9, Steps: 46 | Train Loss: 0.2573676 Vali Loss: 0.1488149 Test Loss: 0.1488149
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0303258895874023
Epoch: 10, Steps: 46 | Train Loss: 0.2605402 Vali Loss: 0.1490499 Test Loss: 0.1490499
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.027192726731300354, mae:0.12147673219442368, smape:12.102514505386353, dtw:not calculated
horizon:2 mse:0.05635708570480347, mae:0.17406004667282104, smape:17.240862548351288, dtw:not calculated
horizon:3 mse:0.07398327440023422, mae:0.2006513923406601, smape:19.81808841228485, dtw:not calculated
horizon:4 mse:0.11158193647861481, mae:0.25637567043304443, smape:25.192207098007202, dtw:not calculated
horizon:5 mse:0.12800325453281403, mae:0.26996052265167236, smape:26.434868574142456, dtw:not calculated
horizon:6 mse:0.15065978467464447, mae:0.3049125075340271, smape:29.819998145103455, dtw:not calculated
horizon:7 mse:0.19086115062236786, mae:0.3437085449695587, smape:33.41167867183685, dtw:not calculated
horizon:8 mse:0.20357424020767212, mae:0.3565695583820343, smape:34.623974561691284, dtw:not calculated
horizon:9 mse:0.18939892947673798, mae:0.352871835231781, smape:34.4036340713501, dtw:not calculated
horizon:10 mse:0.20200109481811523, mae:0.3648383319377899, smape:35.51273047924042, dtw:not calculated
horizon:11 mse:0.23261025547981262, mae:0.38742542266845703, smape:37.51232028007507, dtw:not calculated
horizon:12 mse:0.21106509864330292, mae:0.37097886204719543, smape:36.04523241519928, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09129633754491806, mae:0.22123947739601135, smape:21.7680886387825, dtw:not calculated
average metrics: horizon upto:12 mse:0.1481073945760727, mae:0.29198578000068665, smape:28.50984036922455, dtw:not calculated
===============================================================================
average of horizons: mse:0.1481073945760727, mae:0.29198578000068665, smape:28.50984036922455, dtw:not calculated
mean smape over horizons:  28.509842480222385
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4330477714538574
Epoch: 1, Steps: 46 | Train Loss: 0.4366236 Vali Loss: 0.1519690 Test Loss: 0.1519690
Validation loss decreased (inf --> 0.151969).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0769195556640625
Epoch: 2, Steps: 46 | Train Loss: 0.2812701 Vali Loss: 0.1548104 Test Loss: 0.1548104
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1538937091827393
Epoch: 3, Steps: 46 | Train Loss: 0.2708467 Vali Loss: 0.1448922 Test Loss: 0.1448922
Validation loss decreased (0.151969 --> 0.144892).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.974879264831543
Epoch: 4, Steps: 46 | Train Loss: 0.2596234 Vali Loss: 0.1461564 Test Loss: 0.1461564
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9795839786529541
Epoch: 5, Steps: 46 | Train Loss: 0.2497988 Vali Loss: 0.1468000 Test Loss: 0.1468000
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9804174900054932
Epoch: 6, Steps: 46 | Train Loss: 0.2512961 Vali Loss: 0.1472216 Test Loss: 0.1472216
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.023632662370800972, mae:0.11173604428768158, smape:11.141854524612427, dtw:not calculated
horizon:2 mse:0.05181974917650223, mae:0.16580632328987122, smape:16.431894898414612, dtw:not calculated
horizon:3 mse:0.0713496133685112, mae:0.1951877474784851, smape:19.259370863437653, dtw:not calculated
horizon:4 mse:0.10221246629953384, mae:0.23941275477409363, smape:23.53019267320633, dtw:not calculated
horizon:5 mse:0.12123371660709381, mae:0.2572869658470154, smape:25.193139910697937, dtw:not calculated
horizon:6 mse:0.14845265448093414, mae:0.29844188690185547, smape:29.155895113945007, dtw:not calculated
horizon:7 mse:0.187429741024971, mae:0.3409773111343384, smape:33.16349685192108, dtw:not calculated
horizon:8 mse:0.2053077220916748, mae:0.3616836965084076, smape:35.13012230396271, dtw:not calculated
horizon:9 mse:0.17591255903244019, mae:0.3433067500591278, smape:33.54710638523102, dtw:not calculated
horizon:10 mse:0.20062440633773804, mae:0.3660772442817688, smape:35.645344853401184, dtw:not calculated
horizon:11 mse:0.23877117037773132, mae:0.39461633563041687, smape:38.186004757881165, dtw:not calculated
horizon:12 mse:0.21195948123931885, mae:0.371268630027771, smape:36.06045842170715, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08645014464855194, mae:0.2113119512796402, smape:20.785391330718994, dtw:not calculated
average metrics: horizon upto:12 mse:0.14489217102527618, mae:0.28715014457702637, smape:28.037074208259583, dtw:not calculated
===============================================================================
average of horizons: mse:0.14489217102527618, mae:0.28715014457702637, smape:28.037074208259583, dtw:not calculated
mean smape over horizons:  28.037073463201523
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=22
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4843692779541016
Epoch: 1, Steps: 46 | Train Loss: 0.4352103 Vali Loss: 0.1433209 Test Loss: 0.1433209
Validation loss decreased (inf --> 0.143321).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9774022102355957
Epoch: 2, Steps: 46 | Train Loss: 0.2801212 Vali Loss: 0.1494239 Test Loss: 0.1494239
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9617834091186523
Epoch: 3, Steps: 46 | Train Loss: 0.2678192 Vali Loss: 0.1423019 Test Loss: 0.1423019
Validation loss decreased (0.143321 --> 0.142302).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9782822132110596
Epoch: 4, Steps: 46 | Train Loss: 0.2584787 Vali Loss: 0.1434697 Test Loss: 0.1434697
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0611934661865234
Epoch: 5, Steps: 46 | Train Loss: 0.2493993 Vali Loss: 0.1439361 Test Loss: 0.1439361
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.010887861251831
Epoch: 6, Steps: 46 | Train Loss: 0.2500440 Vali Loss: 0.1446480 Test Loss: 0.1446480
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.019208190962672234, mae:0.10355282574892044, smape:10.336960107088089, dtw:not calculated
horizon:2 mse:0.04971009120345116, mae:0.16415195167064667, smape:16.277240216732025, dtw:not calculated
horizon:3 mse:0.07226354628801346, mae:0.1948750615119934, smape:19.219961762428284, dtw:not calculated
horizon:4 mse:0.0867260992527008, mae:0.2156534492969513, smape:21.232275664806366, dtw:not calculated
horizon:5 mse:0.1150890365242958, mae:0.24967998266220093, smape:24.46710765361786, dtw:not calculated
horizon:6 mse:0.14080439507961273, mae:0.2822790741920471, smape:27.564215660095215, dtw:not calculated
horizon:7 mse:0.17900972068309784, mae:0.32934460043907166, smape:32.05057382583618, dtw:not calculated
horizon:8 mse:0.20824281871318817, mae:0.36040133237838745, smape:34.96409058570862, dtw:not calculated
horizon:9 mse:0.18639911711215973, mae:0.3456701338291168, smape:33.685025572776794, dtw:not calculated
horizon:10 mse:0.20005503296852112, mae:0.3648603558540344, smape:35.52698791027069, dtw:not calculated
horizon:11 mse:0.23134572803974152, mae:0.38497716188430786, smape:37.26385235786438, dtw:not calculated
horizon:12 mse:0.21876902878284454, mae:0.37362104654312134, smape:36.22391223907471, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08063356578350067, mae:0.20169872045516968, smape:19.84962671995163, dtw:not calculated
average metrics: horizon upto:12 mse:0.14230190217494965, mae:0.2807556092739105, smape:27.401018142700195, dtw:not calculated
===============================================================================
average of horizons: mse:0.14230190217494965, mae:0.2807556092739105, smape:27.401018142700195, dtw:not calculated
mean smape over horizons:  27.401016963024933
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=24, stride=24
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4702413082122803
Epoch: 1, Steps: 46 | Train Loss: 0.4349329 Vali Loss: 0.1405418 Test Loss: 0.1405418
Validation loss decreased (inf --> 0.140542).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0554099082946777
Epoch: 2, Steps: 46 | Train Loss: 0.2815435 Vali Loss: 0.1446746 Test Loss: 0.1446746
EarlyStopping counter: 1 out of 3
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9869391918182373
Epoch: 3, Steps: 46 | Train Loss: 0.2683651 Vali Loss: 0.1460340 Test Loss: 0.1460340
EarlyStopping counter: 2 out of 3
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.1323981285095215
Epoch: 4, Steps: 46 | Train Loss: 0.2575694 Vali Loss: 0.1436302 Test Loss: 0.1436302
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.030937504023313522, mae:0.13627110421657562, smape:13.579414784908295, dtw:not calculated
horizon:2 mse:0.06567825376987457, mae:0.19797076284885406, smape:19.60366666316986, dtw:not calculated
horizon:3 mse:0.08536047488451004, mae:0.22520889341831207, smape:22.21086323261261, dtw:not calculated
horizon:4 mse:0.09091626852750778, mae:0.22514916956424713, smape:22.158969938755035, dtw:not calculated
horizon:5 mse:0.11421840637922287, mae:0.2594448924064636, smape:25.478971004486084, dtw:not calculated
horizon:6 mse:0.14636416733264923, mae:0.29806068539619446, smape:29.16225790977478, dtw:not calculated
horizon:7 mse:0.16030330955982208, mae:0.3145342469215393, smape:30.704620480537415, dtw:not calculated
horizon:8 mse:0.19755502045154572, mae:0.35801005363464355, smape:34.82240438461304, dtw:not calculated
horizon:9 mse:0.1835090070962906, mae:0.3550291061401367, smape:34.674280881881714, dtw:not calculated
horizon:10 mse:0.1800241768360138, mae:0.35674169659614563, smape:34.89549160003662, dtw:not calculated
horizon:11 mse:0.23274162411689758, mae:0.39573872089385986, smape:38.33828270435333, dtw:not calculated
horizon:12 mse:0.1988929659128189, mae:0.3627157509326935, smape:35.31955182552338, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08891251683235168, mae:0.22368425130844116, smape:22.03235626220703, dtw:not calculated
average metrics: horizon upto:12 mse:0.14054177701473236, mae:0.2904062867164612, smape:28.41239869594574, dtw:not calculated
===============================================================================
average of horizons: mse:0.14054177701473236, mae:0.2904062867164612, smape:28.41239869594574, dtw:not calculated
mean smape over horizons:  28.41239795088768
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5205702781677246
Epoch: 1, Steps: 46 | Train Loss: 0.4380788 Vali Loss: 0.1963024 Test Loss: 0.1963024
Validation loss decreased (inf --> 0.196302).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0401926040649414
Epoch: 2, Steps: 46 | Train Loss: 0.3048046 Vali Loss: 0.1684410 Test Loss: 0.1684410
Validation loss decreased (0.196302 --> 0.168441).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0322246551513672
Epoch: 3, Steps: 46 | Train Loss: 0.2852455 Vali Loss: 0.1630091 Test Loss: 0.1630091
Validation loss decreased (0.168441 --> 0.163009).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0521266460418701
Epoch: 4, Steps: 46 | Train Loss: 0.2607724 Vali Loss: 0.1608717 Test Loss: 0.1608717
Validation loss decreased (0.163009 --> 0.160872).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0966887474060059
Epoch: 5, Steps: 46 | Train Loss: 0.2588114 Vali Loss: 0.1589112 Test Loss: 0.1589112
Validation loss decreased (0.160872 --> 0.158911).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0560145378112793
Epoch: 6, Steps: 46 | Train Loss: 0.2668321 Vali Loss: 0.1624589 Test Loss: 0.1624589
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.017087459564209
Epoch: 7, Steps: 46 | Train Loss: 0.2804963 Vali Loss: 0.1609358 Test Loss: 0.1609358
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0421483516693115
Epoch: 8, Steps: 46 | Train Loss: 0.2513956 Vali Loss: 0.1621961 Test Loss: 0.1621961
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.020228084176778793, mae:0.10915572941303253, smape:10.897574573755264, dtw:not calculated
horizon:2 mse:0.05805740877985954, mae:0.18081995844841003, smape:17.920297384262085, dtw:not calculated
horizon:3 mse:0.08909235894680023, mae:0.22267025709152222, smape:21.93681448698044, dtw:not calculated
horizon:4 mse:0.11210733652114868, mae:0.253892719745636, smape:24.911989271640778, dtw:not calculated
horizon:5 mse:0.13021093606948853, mae:0.2750854194164276, smape:26.92227065563202, dtw:not calculated
horizon:6 mse:0.17108851671218872, mae:0.32063090801239014, smape:31.213733553886414, dtw:not calculated
horizon:7 mse:0.19665300846099854, mae:0.35621243715286255, smape:34.64435636997223, dtw:not calculated
horizon:8 mse:0.20876605808734894, mae:0.35753917694091797, smape:34.64962542057037, dtw:not calculated
horizon:9 mse:0.23064614832401276, mae:0.3804682493209839, smape:36.791402101516724, dtw:not calculated
horizon:10 mse:0.2144351750612259, mae:0.3603244721889496, smape:34.90046262741089, dtw:not calculated
horizon:11 mse:0.23590761423110962, mae:0.37892892956733704, smape:36.55245304107666, dtw:not calculated
horizon:12 mse:0.23974169790744781, mae:0.38349083065986633, smape:36.959850788116455, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09679743647575378, mae:0.22704248130321503, smape:22.300447523593903, dtw:not calculated
average metrics: horizon upto:12 mse:0.15891119837760925, mae:0.29826825857162476, smape:29.025065898895264, dtw:not calculated
===============================================================================
average of horizons: mse:0.15891119837760925, mae:0.29826825857162476, smape:29.025065898895264, dtw:not calculated
mean smape over horizons:  29.02506918956836
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4345910549163818
Epoch: 1, Steps: 46 | Train Loss: 0.4122003 Vali Loss: 0.2221444 Test Loss: 0.2221444
Validation loss decreased (inf --> 0.222144).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9419889450073242
Epoch: 2, Steps: 46 | Train Loss: 0.3142517 Vali Loss: 0.1784678 Test Loss: 0.1784678
Validation loss decreased (0.222144 --> 0.178468).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0256781578063965
Epoch: 3, Steps: 46 | Train Loss: 0.2868359 Vali Loss: 0.1744874 Test Loss: 0.1744874
Validation loss decreased (0.178468 --> 0.174487).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0993239879608154
Epoch: 4, Steps: 46 | Train Loss: 0.2647097 Vali Loss: 0.1663964 Test Loss: 0.1663964
Validation loss decreased (0.174487 --> 0.166396).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.1022303104400635
Epoch: 5, Steps: 46 | Train Loss: 0.2737796 Vali Loss: 0.1658392 Test Loss: 0.1658392
Validation loss decreased (0.166396 --> 0.165839).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.03656005859375
Epoch: 6, Steps: 46 | Train Loss: 0.2730270 Vali Loss: 0.1823271 Test Loss: 0.1823271
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0204222202301025
Epoch: 7, Steps: 46 | Train Loss: 0.2841743 Vali Loss: 0.1707789 Test Loss: 0.1707789
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.043494701385498
Epoch: 8, Steps: 46 | Train Loss: 0.2608121 Vali Loss: 0.1653178 Test Loss: 0.1653178
Validation loss decreased (0.165839 --> 0.165318).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9848003387451172
Epoch: 9, Steps: 46 | Train Loss: 0.2685787 Vali Loss: 0.1700322 Test Loss: 0.1700322
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0131611824035645
Epoch: 10, Steps: 46 | Train Loss: 0.2703376 Vali Loss: 0.1646714 Test Loss: 0.1646714
Validation loss decreased (0.165318 --> 0.164671).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.024662083014845848, mae:0.11957713216543198, smape:11.928970366716385, dtw:not calculated
horizon:2 mse:0.04654143378138542, mae:0.16759547591209412, smape:16.66119694709778, dtw:not calculated
horizon:3 mse:0.08869317919015884, mae:0.21713943779468536, smape:21.384266018867493, dtw:not calculated
horizon:4 mse:0.12207003682851791, mae:0.26193204522132874, smape:25.658491253852844, dtw:not calculated
horizon:5 mse:0.13017451763153076, mae:0.2738837003707886, smape:26.820644736289978, dtw:not calculated
horizon:6 mse:0.18662598729133606, mae:0.3360981047153473, smape:32.66755938529968, dtw:not calculated
horizon:7 mse:0.21360352635383606, mae:0.36481744050979614, smape:35.35040616989136, dtw:not calculated
horizon:8 mse:0.21414771676063538, mae:0.3677683472633362, smape:35.663941502571106, dtw:not calculated
horizon:9 mse:0.22393468022346497, mae:0.3756294548511505, smape:36.3717645406723, dtw:not calculated
horizon:10 mse:0.23695260286331177, mae:0.38887572288513184, smape:37.60465681552887, dtw:not calculated
horizon:11 mse:0.23262611031532288, mae:0.3769361972808838, smape:36.367589235305786, dtw:not calculated
horizon:12 mse:0.2560245096683502, mae:0.38790130615234375, smape:37.21354603767395, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09979453682899475, mae:0.22937098145484924, smape:22.520188987255096, dtw:not calculated
average metrics: horizon upto:12 mse:0.1646713763475418, mae:0.303179532289505, smape:29.474419355392456, dtw:not calculated
===============================================================================
average of horizons: mse:0.1646713763475418, mae:0.303179532289505, smape:29.474419355392456, dtw:not calculated
mean smape over horizons:  29.47441941748063
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.510673999786377
Epoch: 1, Steps: 46 | Train Loss: 0.3906444 Vali Loss: 0.2091470 Test Loss: 0.2091470
Validation loss decreased (inf --> 0.209147).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.161456823348999
Epoch: 2, Steps: 46 | Train Loss: 0.2984809 Vali Loss: 0.1716166 Test Loss: 0.1716166
Validation loss decreased (0.209147 --> 0.171617).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.026059865951538
Epoch: 3, Steps: 46 | Train Loss: 0.2688804 Vali Loss: 0.1687757 Test Loss: 0.1687757
Validation loss decreased (0.171617 --> 0.168776).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9953298568725586
Epoch: 4, Steps: 46 | Train Loss: 0.2514779 Vali Loss: 0.1624925 Test Loss: 0.1624925
Validation loss decreased (0.168776 --> 0.162492).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0601527690887451
Epoch: 5, Steps: 46 | Train Loss: 0.2578121 Vali Loss: 0.1614667 Test Loss: 0.1614667
Validation loss decreased (0.162492 --> 0.161467).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.043616533279419
Epoch: 6, Steps: 46 | Train Loss: 0.2577770 Vali Loss: 0.1708097 Test Loss: 0.1708097
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.1464922428131104
Epoch: 7, Steps: 46 | Train Loss: 0.2724327 Vali Loss: 0.1640678 Test Loss: 0.1640678
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0275702476501465
Epoch: 8, Steps: 46 | Train Loss: 0.2515719 Vali Loss: 0.1610830 Test Loss: 0.1610830
Validation loss decreased (0.161467 --> 0.161083).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0153028964996338
Epoch: 9, Steps: 46 | Train Loss: 0.2521399 Vali Loss: 0.1632392 Test Loss: 0.1632392
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0677204132080078
Epoch: 10, Steps: 46 | Train Loss: 0.2588397 Vali Loss: 0.1600349 Test Loss: 0.1600349
Validation loss decreased (0.161083 --> 0.160035).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.018744006752967834, mae:0.10187826305627823, smape:10.17197072505951, dtw:not calculated
horizon:2 mse:0.04501368850469589, mae:0.15874139964580536, smape:15.768657624721527, dtw:not calculated
horizon:3 mse:0.07852701097726822, mae:0.20086120069026947, smape:19.798840582370758, dtw:not calculated
horizon:4 mse:0.10827253013849258, mae:0.24388520419597626, smape:23.94178807735443, dtw:not calculated
horizon:5 mse:0.12632767856121063, mae:0.2648483216762543, smape:25.928622484207153, dtw:not calculated
horizon:6 mse:0.16770879924297333, mae:0.3126252591609955, smape:30.436420440673828, dtw:not calculated
horizon:7 mse:0.20178155601024628, mae:0.34639638662338257, smape:33.57621431350708, dtw:not calculated
horizon:8 mse:0.2114168107509613, mae:0.36516907811164856, smape:35.42499840259552, dtw:not calculated
horizon:9 mse:0.2361500859260559, mae:0.3794246315956116, smape:36.63567006587982, dtw:not calculated
horizon:10 mse:0.24199135601520538, mae:0.38843464851379395, smape:37.504348158836365, dtw:not calculated
horizon:11 mse:0.23771177232265472, mae:0.3802449107170105, smape:36.6756409406662, dtw:not calculated
horizon:12 mse:0.2467731386423111, mae:0.3875523805618286, smape:37.32163906097412, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09076561778783798, mae:0.21380659937858582, smape:21.0077166557312, dtw:not calculated
average metrics: horizon upto:12 mse:0.1600348800420761, mae:0.2941717803478241, smape:28.598731756210327, dtw:not calculated
===============================================================================
average of horizons: mse:0.1600348800420761, mae:0.2941717803478241, smape:28.598731756210327, dtw:not calculated
mean smape over horizons:  28.598734239737194
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3943982124328613
Epoch: 1, Steps: 46 | Train Loss: 0.4584858 Vali Loss: 0.2098388 Test Loss: 0.2098388
Validation loss decreased (inf --> 0.209839).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0044546127319336
Epoch: 2, Steps: 46 | Train Loss: 0.3101006 Vali Loss: 0.1840516 Test Loss: 0.1840516
Validation loss decreased (0.209839 --> 0.184052).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.082958459854126
Epoch: 3, Steps: 46 | Train Loss: 0.3002288 Vali Loss: 0.1765261 Test Loss: 0.1765261
Validation loss decreased (0.184052 --> 0.176526).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0056068897247314
Epoch: 4, Steps: 46 | Train Loss: 0.2827410 Vali Loss: 0.1710774 Test Loss: 0.1710774
Validation loss decreased (0.176526 --> 0.171077).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0667977333068848
Epoch: 5, Steps: 46 | Train Loss: 0.2785667 Vali Loss: 0.1696098 Test Loss: 0.1696098
Validation loss decreased (0.171077 --> 0.169610).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.042344331741333
Epoch: 6, Steps: 46 | Train Loss: 0.2816150 Vali Loss: 0.1676571 Test Loss: 0.1676571
Validation loss decreased (0.169610 --> 0.167657).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9653832912445068
Epoch: 7, Steps: 46 | Train Loss: 0.2871335 Vali Loss: 0.1666096 Test Loss: 0.1666096
Validation loss decreased (0.167657 --> 0.166610).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.061544418334961
Epoch: 8, Steps: 46 | Train Loss: 0.2608360 Vali Loss: 0.1665406 Test Loss: 0.1665406
Validation loss decreased (0.166610 --> 0.166541).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 0.9900784492492676
Epoch: 9, Steps: 46 | Train Loss: 0.2729448 Vali Loss: 0.1688913 Test Loss: 0.1688913
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0159168243408203
Epoch: 10, Steps: 46 | Train Loss: 0.2716573 Vali Loss: 0.1659173 Test Loss: 0.1659173
Validation loss decreased (0.166541 --> 0.165917).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.025138186290860176, mae:0.11751362681388855, smape:11.718456447124481, dtw:not calculated
horizon:2 mse:0.06886200606822968, mae:0.19939984381198883, smape:19.731396436691284, dtw:not calculated
horizon:3 mse:0.0933898538351059, mae:0.23031872510910034, smape:22.680118680000305, dtw:not calculated
horizon:4 mse:0.13706614077091217, mae:0.2876037657260895, smape:28.16370129585266, dtw:not calculated
horizon:5 mse:0.13873860239982605, mae:0.2824335992336273, smape:27.61465311050415, dtw:not calculated
horizon:6 mse:0.1765454262495041, mae:0.3325749635696411, smape:32.39905536174774, dtw:not calculated
horizon:7 mse:0.19785843789577484, mae:0.34971410036087036, smape:33.9651495218277, dtw:not calculated
horizon:8 mse:0.2066199779510498, mae:0.3574184775352478, smape:34.66711640357971, dtw:not calculated
horizon:9 mse:0.24170710146427155, mae:0.39720579981803894, smape:38.42085003852844, dtw:not calculated
horizon:10 mse:0.2361111044883728, mae:0.38613346219062805, smape:37.342336773872375, dtw:not calculated
horizon:11 mse:0.23614898324012756, mae:0.38926854729652405, smape:37.665629386901855, dtw:not calculated
horizon:12 mse:0.23282155394554138, mae:0.39221519231796265, smape:38.01796734333038, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10662337392568588, mae:0.24164076149463654, smape:23.71789813041687, dtw:not calculated
average metrics: horizon upto:12 mse:0.1659172773361206, mae:0.31014999747276306, smape:30.198872089385986, dtw:not calculated
===============================================================================
average of horizons: mse:0.1659172773361206, mae:0.31014999747276306, smape:30.198872089385986, dtw:not calculated
mean smape over horizons:  30.19886923333009
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4431369304656982
Epoch: 1, Steps: 46 | Train Loss: 0.4507994 Vali Loss: 0.1967973 Test Loss: 0.1967973
Validation loss decreased (inf --> 0.196797).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0081815719604492
Epoch: 2, Steps: 46 | Train Loss: 0.3025213 Vali Loss: 0.1742096 Test Loss: 0.1742096
Validation loss decreased (0.196797 --> 0.174210).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9528720378875732
Epoch: 3, Steps: 46 | Train Loss: 0.2915168 Vali Loss: 0.1683255 Test Loss: 0.1683255
Validation loss decreased (0.174210 --> 0.168326).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0444014072418213
Epoch: 4, Steps: 46 | Train Loss: 0.2737998 Vali Loss: 0.1633984 Test Loss: 0.1633984
Validation loss decreased (0.168326 --> 0.163398).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0375409126281738
Epoch: 5, Steps: 46 | Train Loss: 0.2711386 Vali Loss: 0.1626644 Test Loss: 0.1626644
Validation loss decreased (0.163398 --> 0.162664).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9855396747589111
Epoch: 6, Steps: 46 | Train Loss: 0.2740346 Vali Loss: 0.1613289 Test Loss: 0.1613289
Validation loss decreased (0.162664 --> 0.161329).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.057765245437622
Epoch: 7, Steps: 46 | Train Loss: 0.2819806 Vali Loss: 0.1605087 Test Loss: 0.1605087
Validation loss decreased (0.161329 --> 0.160509).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.035384178161621
Epoch: 8, Steps: 46 | Train Loss: 0.2536179 Vali Loss: 0.1603734 Test Loss: 0.1603734
Validation loss decreased (0.160509 --> 0.160373).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.062699794769287
Epoch: 9, Steps: 46 | Train Loss: 0.2656582 Vali Loss: 0.1628025 Test Loss: 0.1628025
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0619306564331055
Epoch: 10, Steps: 46 | Train Loss: 0.2654032 Vali Loss: 0.1598313 Test Loss: 0.1598313
Validation loss decreased (0.160373 --> 0.159831).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02165394462645054, mae:0.11295342445373535, smape:11.274442821741104, dtw:not calculated
horizon:2 mse:0.06680825352668762, mae:0.1946392059326172, smape:19.262439012527466, dtw:not calculated
horizon:3 mse:0.08681314438581467, mae:0.22098302841186523, smape:21.78388684988022, dtw:not calculated
horizon:4 mse:0.1271127164363861, mae:0.274465948343277, smape:26.909935474395752, dtw:not calculated
horizon:5 mse:0.13284920156002045, mae:0.2758133113384247, smape:26.99560523033142, dtw:not calculated
horizon:6 mse:0.16665829718112946, mae:0.3214944899082184, smape:31.36531412601471, dtw:not calculated
horizon:7 mse:0.18854106962680817, mae:0.3417947590351105, smape:33.25512111186981, dtw:not calculated
horizon:8 mse:0.2022838443517685, mae:0.35456880927085876, smape:34.428322315216064, dtw:not calculated
horizon:9 mse:0.23209267854690552, mae:0.3875287175178528, smape:37.53175437450409, dtw:not calculated
horizon:10 mse:0.2387087494134903, mae:0.3892672657966614, smape:37.64021694660187, dtw:not calculated
horizon:11 mse:0.23322899639606476, mae:0.3883880078792572, smape:37.598493695259094, dtw:not calculated
horizon:12 mse:0.22122450172901154, mae:0.38093382120132446, smape:36.96569800376892, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10031592100858688, mae:0.23339156806468964, smape:22.931934893131256, dtw:not calculated
average metrics: horizon upto:12 mse:0.15983127057552338, mae:0.3035692572593689, smape:29.584264755249023, dtw:not calculated
===============================================================================
average of horizons: mse:0.15983127057552338, mae:0.3035692572593689, smape:29.584264755249023, dtw:not calculated
mean smape over horizons:  29.58426916350921
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4585959911346436
Epoch: 1, Steps: 46 | Train Loss: 0.4416014 Vali Loss: 0.1816375 Test Loss: 0.1816375
Validation loss decreased (inf --> 0.181638).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0332896709442139
Epoch: 2, Steps: 46 | Train Loss: 0.2949061 Vali Loss: 0.1645934 Test Loss: 0.1645934
Validation loss decreased (0.181638 --> 0.164593).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0840353965759277
Epoch: 3, Steps: 46 | Train Loss: 0.2818194 Vali Loss: 0.1606599 Test Loss: 0.1606599
Validation loss decreased (0.164593 --> 0.160660).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9180581569671631
Epoch: 4, Steps: 46 | Train Loss: 0.2657126 Vali Loss: 0.1570873 Test Loss: 0.1570873
Validation loss decreased (0.160660 --> 0.157087).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.1303818225860596
Epoch: 5, Steps: 46 | Train Loss: 0.2639304 Vali Loss: 0.1571471 Test Loss: 0.1571471
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.084015130996704
Epoch: 6, Steps: 46 | Train Loss: 0.2674900 Vali Loss: 0.1561433 Test Loss: 0.1561433
Validation loss decreased (0.157087 --> 0.156143).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0122311115264893
Epoch: 7, Steps: 46 | Train Loss: 0.2765349 Vali Loss: 0.1553060 Test Loss: 0.1553060
Validation loss decreased (0.156143 --> 0.155306).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9405615329742432
Epoch: 8, Steps: 46 | Train Loss: 0.2485136 Vali Loss: 0.1553254 Test Loss: 0.1553254
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0368647575378418
Epoch: 9, Steps: 46 | Train Loss: 0.2594503 Vali Loss: 0.1567649 Test Loss: 0.1567649
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0637612342834473
Epoch: 10, Steps: 46 | Train Loss: 0.2605931 Vali Loss: 0.1547662 Test Loss: 0.1547662
Validation loss decreased (0.155306 --> 0.154766).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.018907302990555763, mae:0.10636433213949203, smape:10.62295287847519, dtw:not calculated
horizon:2 mse:0.0577487051486969, mae:0.17895390093326569, smape:17.73126572370529, dtw:not calculated
horizon:3 mse:0.07908136397600174, mae:0.20621439814567566, smape:20.34442126750946, dtw:not calculated
horizon:4 mse:0.11832118034362793, mae:0.26143375039100647, smape:25.654035806655884, dtw:not calculated
horizon:5 mse:0.13388699293136597, mae:0.2775886356830597, smape:27.172240614891052, dtw:not calculated
horizon:6 mse:0.16018280386924744, mae:0.3124026358127594, smape:30.488476157188416, dtw:not calculated
horizon:7 mse:0.18532082438468933, mae:0.33876916766166687, smape:32.97785222530365, dtw:not calculated
horizon:8 mse:0.20359787344932556, mae:0.35707029700279236, smape:34.67167019844055, dtw:not calculated
horizon:9 mse:0.21899358928203583, mae:0.37570634484291077, smape:36.454060673713684, dtw:not calculated
horizon:10 mse:0.2364134043455124, mae:0.3875991404056549, smape:37.49760985374451, dtw:not calculated
horizon:11 mse:0.22952395677566528, mae:0.38097673654556274, smape:36.87625825405121, dtw:not calculated
horizon:12 mse:0.2152170091867447, mae:0.3781592547893524, smape:36.74167990684509, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.0946880578994751, mae:0.22382628917694092, smape:22.00223207473755, dtw:not calculated
average metrics: horizon upto:12 mse:0.1547662615776062, mae:0.2967698574066162, smape:28.93604338169098, dtw:not calculated
===============================================================================
average of horizons: mse:0.1547662615776062, mae:0.2967698574066162, smape:28.93604338169098, dtw:not calculated
mean smape over horizons:  28.936043630043667
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.373532772064209
Epoch: 1, Steps: 46 | Train Loss: 0.4384813 Vali Loss: 0.1713122 Test Loss: 0.1713122
Validation loss decreased (inf --> 0.171312).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.098738431930542
Epoch: 2, Steps: 46 | Train Loss: 0.2911861 Vali Loss: 0.1615388 Test Loss: 0.1615388
Validation loss decreased (0.171312 --> 0.161539).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.081345558166504
Epoch: 3, Steps: 46 | Train Loss: 0.2786519 Vali Loss: 0.1587390 Test Loss: 0.1587390
Validation loss decreased (0.161539 --> 0.158739).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0594244003295898
Epoch: 4, Steps: 46 | Train Loss: 0.2606540 Vali Loss: 0.1557656 Test Loss: 0.1557656
Validation loss decreased (0.158739 --> 0.155766).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0754268169403076
Epoch: 5, Steps: 46 | Train Loss: 0.2594293 Vali Loss: 0.1559236 Test Loss: 0.1559236
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0699894428253174
Epoch: 6, Steps: 46 | Train Loss: 0.2627036 Vali Loss: 0.1554751 Test Loss: 0.1554751
Validation loss decreased (0.155766 --> 0.155475).  Saving model ...
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 0.9975714683532715
Epoch: 7, Steps: 46 | Train Loss: 0.2731542 Vali Loss: 0.1544363 Test Loss: 0.1544363
Validation loss decreased (0.155475 --> 0.154436).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0566353797912598
Epoch: 8, Steps: 46 | Train Loss: 0.2455564 Vali Loss: 0.1543939 Test Loss: 0.1543939
Validation loss decreased (0.154436 --> 0.154394).  Saving model ...
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0813040733337402
Epoch: 9, Steps: 46 | Train Loss: 0.2556397 Vali Loss: 0.1552396 Test Loss: 0.1552396
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0310850143432617
Epoch: 10, Steps: 46 | Train Loss: 0.2583231 Vali Loss: 0.1542068 Test Loss: 0.1542068
Validation loss decreased (0.154394 --> 0.154207).  Saving model ...
Updating learning rate to 1.953125e-07
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.01985849067568779, mae:0.10984907299280167, smape:10.969816148281097, dtw:not calculated
horizon:2 mse:0.05208149179816246, mae:0.17048460245132446, smape:16.905835270881653, dtw:not calculated
horizon:3 mse:0.07571347057819366, mae:0.20243550837039948, smape:19.98348832130432, dtw:not calculated
horizon:4 mse:0.1162741631269455, mae:0.2575209438800812, smape:25.27364492416382, dtw:not calculated
horizon:5 mse:0.1375461369752884, mae:0.2781391441822052, smape:27.188187837600708, dtw:not calculated
horizon:6 mse:0.1555653214454651, mae:0.3043665289878845, smape:29.69880998134613, dtw:not calculated
horizon:7 mse:0.18456076085567474, mae:0.3346060812473297, smape:32.549527287483215, dtw:not calculated
horizon:8 mse:0.20696187019348145, mae:0.36008161306381226, smape:34.94534194469452, dtw:not calculated
horizon:9 mse:0.21325060725212097, mae:0.3710463047027588, smape:36.031123995780945, dtw:not calculated
horizon:10 mse:0.23447497189044952, mae:0.3876025974750519, smape:37.518101930618286, dtw:not calculated
horizon:11 mse:0.2371506541967392, mae:0.39178407192230225, smape:37.91811168193817, dtw:not calculated
horizon:12 mse:0.21704337000846863, mae:0.380270391702652, smape:36.94007396697998, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09283985197544098, mae:0.22046595811843872, smape:21.669964492321014, dtw:not calculated
average metrics: horizon upto:12 mse:0.154206782579422, mae:0.29568225145339966, smape:28.82683575153351, dtw:not calculated
===============================================================================
average of horizons: mse:0.154206782579422, mae:0.29568225145339966, smape:28.82683575153351, dtw:not calculated
mean smape over horizons:  28.826838607589405
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4954993724822998
Epoch: 1, Steps: 46 | Train Loss: 0.4414110 Vali Loss: 0.1646892 Test Loss: 0.1646892
Validation loss decreased (inf --> 0.164689).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9769949913024902
Epoch: 2, Steps: 46 | Train Loss: 0.2879272 Vali Loss: 0.1596449 Test Loss: 0.1596449
Validation loss decreased (0.164689 --> 0.159645).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.083240032196045
Epoch: 3, Steps: 46 | Train Loss: 0.2759668 Vali Loss: 0.1565972 Test Loss: 0.1565972
Validation loss decreased (0.159645 --> 0.156597).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9758903980255127
Epoch: 4, Steps: 46 | Train Loss: 0.2584949 Vali Loss: 0.1549244 Test Loss: 0.1549244
Validation loss decreased (0.156597 --> 0.154924).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0423610210418701
Epoch: 5, Steps: 46 | Train Loss: 0.2553840 Vali Loss: 0.1540842 Test Loss: 0.1540842
Validation loss decreased (0.154924 --> 0.154084).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9831442832946777
Epoch: 6, Steps: 46 | Train Loss: 0.2582241 Vali Loss: 0.1542684 Test Loss: 0.1542684
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0033671855926514
Epoch: 7, Steps: 46 | Train Loss: 0.2698036 Vali Loss: 0.1529512 Test Loss: 0.1529512
Validation loss decreased (0.154084 --> 0.152951).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 0.9660756587982178
Epoch: 8, Steps: 46 | Train Loss: 0.2454223 Vali Loss: 0.1530087 Test Loss: 0.1530087
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0407164096832275
Epoch: 9, Steps: 46 | Train Loss: 0.2523850 Vali Loss: 0.1531356 Test Loss: 0.1531356
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.0454463958740234
Epoch: 10, Steps: 46 | Train Loss: 0.2551410 Vali Loss: 0.1530777 Test Loss: 0.1530777
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02105989120900631, mae:0.1126476302742958, smape:11.246173828840256, dtw:not calculated
horizon:2 mse:0.04429535195231438, mae:0.15382564067840576, smape:15.269960463047028, dtw:not calculated
horizon:3 mse:0.07376840710639954, mae:0.19699442386627197, smape:19.438311457633972, dtw:not calculated
horizon:4 mse:0.11415167152881622, mae:0.2539994418621063, smape:24.926617741584778, dtw:not calculated
horizon:5 mse:0.14031220972537994, mae:0.2784039080142975, smape:27.184367179870605, dtw:not calculated
horizon:6 mse:0.15248307585716248, mae:0.30080291628837585, smape:29.35727834701538, dtw:not calculated
horizon:7 mse:0.18490323424339294, mae:0.3358387053012848, smape:32.663336396217346, dtw:not calculated
horizon:8 mse:0.20390793681144714, mae:0.3594535291194916, smape:34.905582666397095, dtw:not calculated
horizon:9 mse:0.20942780375480652, mae:0.3684515357017517, smape:35.79987287521362, dtw:not calculated
horizon:10 mse:0.23023872077465057, mae:0.3869335949420929, smape:37.50501871109009, dtw:not calculated
horizon:11 mse:0.2411680370569229, mae:0.39631032943725586, smape:38.33937644958496, dtw:not calculated
horizon:12 mse:0.21969768404960632, mae:0.3797670304775238, smape:36.85872554779053, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09101177752017975, mae:0.21611233055591583, smape:21.237118542194366, dtw:not calculated
average metrics: horizon upto:12 mse:0.1529511660337448, mae:0.2936190664768219, smape:28.624552488327026, dtw:not calculated
===============================================================================
average of horizons: mse:0.1529511660337448, mae:0.2936190664768219, smape:28.624552488327026, dtw:not calculated
mean smape over horizons:  28.62455180535714
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4162724018096924
Epoch: 1, Steps: 46 | Train Loss: 0.4438764 Vali Loss: 0.1644281 Test Loss: 0.1644281
Validation loss decreased (inf --> 0.164428).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9802002906799316
Epoch: 2, Steps: 46 | Train Loss: 0.2877560 Vali Loss: 0.1600443 Test Loss: 0.1600443
Validation loss decreased (0.164428 --> 0.160044).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.1515803337097168
Epoch: 3, Steps: 46 | Train Loss: 0.2760842 Vali Loss: 0.1570373 Test Loss: 0.1570373
Validation loss decreased (0.160044 --> 0.157037).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 0.9375226497650146
Epoch: 4, Steps: 46 | Train Loss: 0.2592989 Vali Loss: 0.1547385 Test Loss: 0.1547385
Validation loss decreased (0.157037 --> 0.154738).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9676072597503662
Epoch: 5, Steps: 46 | Train Loss: 0.2542922 Vali Loss: 0.1532794 Test Loss: 0.1532794
Validation loss decreased (0.154738 --> 0.153279).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0520272254943848
Epoch: 6, Steps: 46 | Train Loss: 0.2584280 Vali Loss: 0.1541809 Test Loss: 0.1541809
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.0256154537200928
Epoch: 7, Steps: 46 | Train Loss: 0.2684817 Vali Loss: 0.1526417 Test Loss: 0.1526417
Validation loss decreased (0.153279 --> 0.152642).  Saving model ...
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0021982192993164
Epoch: 8, Steps: 46 | Train Loss: 0.2454524 Vali Loss: 0.1527122 Test Loss: 0.1527122
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-07
Epoch: 9 cost time: 1.0038106441497803
Epoch: 9, Steps: 46 | Train Loss: 0.2536153 Vali Loss: 0.1527945 Test Loss: 0.1527945
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-07
Epoch: 10 cost time: 1.1125590801239014
Epoch: 10, Steps: 46 | Train Loss: 0.2545062 Vali Loss: 0.1528655 Test Loss: 0.1528655
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.020285962149500847, mae:0.10971402376890182, smape:10.954340547323227, dtw:not calculated
horizon:2 mse:0.044665295630693436, mae:0.15022829174995422, smape:14.902614057064056, dtw:not calculated
horizon:3 mse:0.0742131918668747, mae:0.1990579068660736, smape:19.644831120967865, dtw:not calculated
horizon:4 mse:0.11189527064561844, mae:0.24704888463020325, smape:24.230961501598358, dtw:not calculated
horizon:5 mse:0.14329633116722107, mae:0.286007821559906, smape:27.95029878616333, dtw:not calculated
horizon:6 mse:0.15597932040691376, mae:0.3050018846988678, smape:29.75260019302368, dtw:not calculated
horizon:7 mse:0.18681736290454865, mae:0.33683568239212036, smape:32.748374342918396, dtw:not calculated
horizon:8 mse:0.19344623386859894, mae:0.34920960664749146, smape:33.95957350730896, dtw:not calculated
horizon:9 mse:0.20913520455360413, mae:0.3684243857860565, smape:35.79799234867096, dtw:not calculated
horizon:10 mse:0.2295641154050827, mae:0.38739892840385437, smape:37.55820095539093, dtw:not calculated
horizon:11 mse:0.23903584480285645, mae:0.38813093304634094, smape:37.51523792743683, dtw:not calculated
horizon:12 mse:0.22336578369140625, mae:0.38252493739128113, smape:37.10387051105499, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09172255545854568, mae:0.21617648005485535, smape:21.239276230335236, dtw:not calculated
average metrics: horizon upto:12 mse:0.1526416540145874, mae:0.2924652695655823, smape:28.50990891456604, dtw:not calculated
===============================================================================
average of horizons: mse:0.1526416540145874, mae:0.2924652695655823, smape:28.50990891456604, dtw:not calculated
mean smape over horizons:  28.509907983243465
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=22
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3879494667053223
Epoch: 1, Steps: 46 | Train Loss: 0.4447066 Vali Loss: 0.1636984 Test Loss: 0.1636984
Validation loss decreased (inf --> 0.163698).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 0.9397861957550049
Epoch: 2, Steps: 46 | Train Loss: 0.2870965 Vali Loss: 0.1576631 Test Loss: 0.1576631
Validation loss decreased (0.163698 --> 0.157663).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 0.9808096885681152
Epoch: 3, Steps: 46 | Train Loss: 0.2751676 Vali Loss: 0.1565317 Test Loss: 0.1565317
Validation loss decreased (0.157663 --> 0.156532).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.062898874282837
Epoch: 4, Steps: 46 | Train Loss: 0.2591167 Vali Loss: 0.1527149 Test Loss: 0.1527149
Validation loss decreased (0.156532 --> 0.152715).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 0.9455103874206543
Epoch: 5, Steps: 46 | Train Loss: 0.2538209 Vali Loss: 0.1512853 Test Loss: 0.1512853
Validation loss decreased (0.152715 --> 0.151285).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 1.0951366424560547
Epoch: 6, Steps: 46 | Train Loss: 0.2571581 Vali Loss: 0.1532426 Test Loss: 0.1532426
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.1253511905670166
Epoch: 7, Steps: 46 | Train Loss: 0.2655423 Vali Loss: 0.1513887 Test Loss: 0.1513887
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.0448830127716064
Epoch: 8, Steps: 46 | Train Loss: 0.2434478 Vali Loss: 0.1516653 Test Loss: 0.1516653
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.019551562145352364, mae:0.10737926512956619, smape:10.72201281785965, dtw:not calculated
horizon:2 mse:0.044358473271131516, mae:0.15512241423130035, smape:15.403090417385101, dtw:not calculated
horizon:3 mse:0.07487837225198746, mae:0.19750967621803284, smape:19.486114382743835, dtw:not calculated
horizon:4 mse:0.10718085616827011, mae:0.24276846647262573, smape:23.82795661687851, dtw:not calculated
horizon:5 mse:0.14677681028842926, mae:0.28971418738365173, smape:28.281450271606445, dtw:not calculated
horizon:6 mse:0.15767283737659454, mae:0.3034263253211975, smape:29.574453830718994, dtw:not calculated
horizon:7 mse:0.1795741468667984, mae:0.3285619020462036, smape:31.96834921836853, dtw:not calculated
horizon:8 mse:0.1864280104637146, mae:0.34172704815864563, smape:33.26275050640106, dtw:not calculated
horizon:9 mse:0.2092178463935852, mae:0.36513563990592957, smape:35.45091152191162, dtw:not calculated
horizon:10 mse:0.22458530962467194, mae:0.3814166486263275, smape:36.9842529296875, dtw:not calculated
horizon:11 mse:0.24385280907154083, mae:0.39539504051208496, smape:38.21012079715729, dtw:not calculated
horizon:12 mse:0.22134627401828766, mae:0.380291223526001, smape:36.87650263309479, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09173648059368134, mae:0.2159867286682129, smape:21.21584564447403, dtw:not calculated
average metrics: horizon upto:12 mse:0.15128527581691742, mae:0.29070398211479187, smape:28.337332606315613, dtw:not calculated
===============================================================================
average of horizons: mse:0.15128527581691742, mae:0.29070398211479187, smape:28.337332606315613, dtw:not calculated
mean smape over horizons:  28.337330495317776
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.0001, patch_len=28, stride=24
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.478135585784912
Epoch: 1, Steps: 46 | Train Loss: 0.4431135 Vali Loss: 0.1682276 Test Loss: 0.1682276
Validation loss decreased (inf --> 0.168228).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 1.0401930809020996
Epoch: 2, Steps: 46 | Train Loss: 0.2889029 Vali Loss: 0.1590518 Test Loss: 0.1590518
Validation loss decreased (0.168228 --> 0.159052).  Saving model ...
Updating learning rate to 5e-05
Epoch: 3 cost time: 1.0389339923858643
Epoch: 3, Steps: 46 | Train Loss: 0.2741868 Vali Loss: 0.1579972 Test Loss: 0.1579972
Validation loss decreased (0.159052 --> 0.157997).  Saving model ...
Updating learning rate to 2.5e-05
Epoch: 4 cost time: 1.0198371410369873
Epoch: 4, Steps: 46 | Train Loss: 0.2584576 Vali Loss: 0.1517542 Test Loss: 0.1517542
Validation loss decreased (0.157997 --> 0.151754).  Saving model ...
Updating learning rate to 1.25e-05
Epoch: 5 cost time: 1.0193822383880615
Epoch: 5, Steps: 46 | Train Loss: 0.2531792 Vali Loss: 0.1511604 Test Loss: 0.1511604
Validation loss decreased (0.151754 --> 0.151160).  Saving model ...
Updating learning rate to 6.25e-06
Epoch: 6 cost time: 0.9552502632141113
Epoch: 6, Steps: 46 | Train Loss: 0.2576137 Vali Loss: 0.1536038 Test Loss: 0.1536038
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-06
Epoch: 7 cost time: 1.112830400466919
Epoch: 7, Steps: 46 | Train Loss: 0.2656096 Vali Loss: 0.1515246 Test Loss: 0.1515246
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-06
Epoch: 8 cost time: 1.011779546737671
Epoch: 8, Steps: 46 | Train Loss: 0.2423159 Vali Loss: 0.1519653 Test Loss: 0.1519653
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.020298218354582787, mae:0.10948562622070312, smape:10.930651426315308, dtw:not calculated
horizon:2 mse:0.049889303743839264, mae:0.16686594486236572, smape:16.556011140346527, dtw:not calculated
horizon:3 mse:0.07384611666202545, mae:0.19515760242938995, smape:19.252926111221313, dtw:not calculated
horizon:4 mse:0.1060313954949379, mae:0.2413637340068817, smape:23.687979578971863, dtw:not calculated
horizon:5 mse:0.13632628321647644, mae:0.27370813488960266, smape:26.73596739768982, dtw:not calculated
horizon:6 mse:0.15934154391288757, mae:0.30419111251831055, smape:29.628220200538635, dtw:not calculated
horizon:7 mse:0.17603537440299988, mae:0.3260004222393036, smape:31.734630465507507, dtw:not calculated
horizon:8 mse:0.1872902810573578, mae:0.3405293822288513, smape:33.12852382659912, dtw:not calculated
horizon:9 mse:0.2152697592973709, mae:0.3646607995033264, smape:35.32649576663971, dtw:not calculated
horizon:10 mse:0.22753416001796722, mae:0.3863835632801056, smape:37.46194541454315, dtw:not calculated
horizon:11 mse:0.2404358834028244, mae:0.39196157455444336, smape:37.888261675834656, dtw:not calculated
horizon:12 mse:0.22162722051143646, mae:0.3784653842449188, smape:36.675140261650085, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09095548093318939, mae:0.21512870490550995, smape:21.131958067417145, dtw:not calculated
average metrics: horizon upto:12 mse:0.15116044878959656, mae:0.28989776968955994, smape:28.250563144683838, dtw:not calculated
===============================================================================
average of horizons: mse:0.15116044878959656, mae:0.28989776968955994, smape:28.250563144683838, dtw:not calculated
mean smape over horizons:  28.250562772154808
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=8, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4320061206817627
Epoch: 1, Steps: 46 | Train Loss: 0.6963015 Vali Loss: 0.5166810 Test Loss: 0.5166810
Validation loss decreased (inf --> 0.516681).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0529675483703613
Epoch: 2, Steps: 46 | Train Loss: 0.4701062 Vali Loss: 0.3257493 Test Loss: 0.3257493
Validation loss decreased (0.516681 --> 0.325749).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9826326370239258
Epoch: 3, Steps: 46 | Train Loss: 0.4030311 Vali Loss: 0.2685019 Test Loss: 0.2685019
Validation loss decreased (0.325749 --> 0.268502).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9881613254547119
Epoch: 4, Steps: 46 | Train Loss: 0.3645793 Vali Loss: 0.2473013 Test Loss: 0.2473013
Validation loss decreased (0.268502 --> 0.247301).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0416266918182373
Epoch: 5, Steps: 46 | Train Loss: 0.3471809 Vali Loss: 0.2377304 Test Loss: 0.2377304
Validation loss decreased (0.247301 --> 0.237730).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0399160385131836
Epoch: 6, Steps: 46 | Train Loss: 0.3589448 Vali Loss: 0.2337033 Test Loss: 0.2337033
Validation loss decreased (0.237730 --> 0.233703).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9694240093231201
Epoch: 7, Steps: 46 | Train Loss: 0.3511658 Vali Loss: 0.2314329 Test Loss: 0.2314329
Validation loss decreased (0.233703 --> 0.231433).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9962608814239502
Epoch: 8, Steps: 46 | Train Loss: 0.3403878 Vali Loss: 0.2303632 Test Loss: 0.2303632
Validation loss decreased (0.231433 --> 0.230363).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.136995792388916
Epoch: 9, Steps: 46 | Train Loss: 0.3481951 Vali Loss: 0.2296340 Test Loss: 0.2296340
Validation loss decreased (0.230363 --> 0.229634).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9828407764434814
Epoch: 10, Steps: 46 | Train Loss: 0.3474015 Vali Loss: 0.2289578 Test Loss: 0.2289578
Validation loss decreased (0.229634 --> 0.228958).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.07856980711221695, mae:0.20372281968593597, smape:20.07898986339569, dtw:not calculated
horizon:2 mse:0.14810548722743988, mae:0.3302818834781647, smape:32.45341181755066, dtw:not calculated
horizon:3 mse:0.2537676692008972, mae:0.4154457151889801, smape:40.141111612319946, dtw:not calculated
horizon:4 mse:0.114059679210186, mae:0.27767276763916016, smape:27.360975742340088, dtw:not calculated
horizon:5 mse:0.2677725553512573, mae:0.4196832776069641, smape:40.41141867637634, dtw:not calculated
horizon:6 mse:0.24006447196006775, mae:0.41047248244285583, smape:39.78242874145508, dtw:not calculated
horizon:7 mse:0.2374245822429657, mae:0.3878287076950073, smape:37.44562268257141, dtw:not calculated
horizon:8 mse:0.30768924951553345, mae:0.46037983894348145, smape:44.20475363731384, dtw:not calculated
horizon:9 mse:0.30978628993034363, mae:0.4591538906097412, smape:44.02654469013214, dtw:not calculated
horizon:10 mse:0.2720893621444702, mae:0.4346277415752411, smape:41.94990694522858, dtw:not calculated
horizon:11 mse:0.25033122301101685, mae:0.3971519470214844, smape:38.28979730606079, dtw:not calculated
horizon:12 mse:0.2678329646587372, mae:0.42671361565589905, smape:41.20246469974518, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.18372328579425812, mae:0.3428798019886017, smape:33.37139189243317, dtw:not calculated
average metrics: horizon upto:12 mse:0.22895778715610504, mae:0.3852612376213074, smape:37.278953194618225, dtw:not calculated
===============================================================================
average of horizons: mse:0.22895778715610504, mae:0.3852612376213074, smape:37.278953194618225, dtw:not calculated
mean smape over horizons:  37.27895220120748
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=8, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.407785177230835
Epoch: 1, Steps: 46 | Train Loss: 0.8878629 Vali Loss: 0.8436475 Test Loss: 0.8436475
Validation loss decreased (inf --> 0.843647).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.036820888519287
Epoch: 2, Steps: 46 | Train Loss: 0.6527211 Vali Loss: 0.5830303 Test Loss: 0.5830303
Validation loss decreased (0.843647 --> 0.583030).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9275321960449219
Epoch: 3, Steps: 46 | Train Loss: 0.5260850 Vali Loss: 0.4900809 Test Loss: 0.4900809
Validation loss decreased (0.583030 --> 0.490081).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.056912899017334
Epoch: 4, Steps: 46 | Train Loss: 0.4950231 Vali Loss: 0.4472587 Test Loss: 0.4472587
Validation loss decreased (0.490081 --> 0.447259).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9393634796142578
Epoch: 5, Steps: 46 | Train Loss: 0.4624074 Vali Loss: 0.4295169 Test Loss: 0.4295169
Validation loss decreased (0.447259 --> 0.429517).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9803242683410645
Epoch: 6, Steps: 46 | Train Loss: 0.4651617 Vali Loss: 0.4203367 Test Loss: 0.4203367
Validation loss decreased (0.429517 --> 0.420337).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.036179780960083
Epoch: 7, Steps: 46 | Train Loss: 0.4507542 Vali Loss: 0.4150038 Test Loss: 0.4150038
Validation loss decreased (0.420337 --> 0.415004).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.016005277633667
Epoch: 8, Steps: 46 | Train Loss: 0.4447491 Vali Loss: 0.4138233 Test Loss: 0.4138233
Validation loss decreased (0.415004 --> 0.413823).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0350167751312256
Epoch: 9, Steps: 46 | Train Loss: 0.4574437 Vali Loss: 0.4119896 Test Loss: 0.4119896
Validation loss decreased (0.413823 --> 0.411990).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.011002540588379
Epoch: 10, Steps: 46 | Train Loss: 0.4444378 Vali Loss: 0.4132179 Test Loss: 0.4132179
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.09397374838590622, mae:0.23570162057876587, smape:23.23748767375946, dtw:not calculated
horizon:2 mse:0.31576231122016907, mae:0.47941699624061584, smape:46.12709283828735, dtw:not calculated
horizon:3 mse:0.19678053259849548, mae:0.3561136722564697, smape:34.63261425495148, dtw:not calculated
horizon:4 mse:0.3007357716560364, mae:0.4573806822299957, smape:43.98616850376129, dtw:not calculated
horizon:5 mse:0.28493160009384155, mae:0.43646717071533203, smape:41.97316765785217, dtw:not calculated
horizon:6 mse:0.4298695921897888, mae:0.5397283434867859, smape:50.98584294319153, dtw:not calculated
horizon:7 mse:0.6312400698661804, mae:0.6765366196632385, smape:62.803441286087036, dtw:not calculated
horizon:8 mse:0.41699618101119995, mae:0.5424126982688904, smape:51.43672227859497, dtw:not calculated
horizon:9 mse:0.7142384052276611, mae:0.7153384685516357, smape:65.71058630943298, dtw:not calculated
horizon:10 mse:0.5732802748680115, mae:0.6112427115440369, smape:56.62837028503418, dtw:not calculated
horizon:11 mse:0.532538890838623, mae:0.614606499671936, smape:57.58639574050903, dtw:not calculated
horizon:12 mse:0.4535270929336548, mae:0.5622272491455078, smape:53.10709476470947, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.27034229040145874, mae:0.4174681305885315, smape:40.15706479549408, dtw:not calculated
average metrics: horizon upto:12 mse:0.4119895398616791, mae:0.5189310908317566, smape:49.01791512966156, dtw:not calculated
===============================================================================
average of horizons: mse:0.4119895398616791, mae:0.5189310908317566, smape:49.01791512966156, dtw:not calculated
mean smape over horizons:  49.017915378014244
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=8, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4878168106079102
Epoch: 1, Steps: 46 | Train Loss: 0.8932347 Vali Loss: 0.6587034 Test Loss: 0.6587034
Validation loss decreased (inf --> 0.658703).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0898733139038086
Epoch: 2, Steps: 46 | Train Loss: 0.5497249 Vali Loss: 0.4727657 Test Loss: 0.4727657
Validation loss decreased (0.658703 --> 0.472766).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0139613151550293
Epoch: 3, Steps: 46 | Train Loss: 0.4665769 Vali Loss: 0.4026970 Test Loss: 0.4026970
Validation loss decreased (0.472766 --> 0.402697).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9746794700622559
Epoch: 4, Steps: 46 | Train Loss: 0.4264703 Vali Loss: 0.3714025 Test Loss: 0.3714025
Validation loss decreased (0.402697 --> 0.371403).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9656970500946045
Epoch: 5, Steps: 46 | Train Loss: 0.4212372 Vali Loss: 0.3588528 Test Loss: 0.3588528
Validation loss decreased (0.371403 --> 0.358853).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9733121395111084
Epoch: 6, Steps: 46 | Train Loss: 0.4167053 Vali Loss: 0.3531159 Test Loss: 0.3531159
Validation loss decreased (0.358853 --> 0.353116).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0440406799316406
Epoch: 7, Steps: 46 | Train Loss: 0.4023570 Vali Loss: 0.3496296 Test Loss: 0.3496296
Validation loss decreased (0.353116 --> 0.349630).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9949712753295898
Epoch: 8, Steps: 46 | Train Loss: 0.4034642 Vali Loss: 0.3475011 Test Loss: 0.3475011
Validation loss decreased (0.349630 --> 0.347501).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.913848876953125
Epoch: 9, Steps: 46 | Train Loss: 0.3878549 Vali Loss: 0.3467059 Test Loss: 0.3467059
Validation loss decreased (0.347501 --> 0.346706).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9836521148681641
Epoch: 10, Steps: 46 | Train Loss: 0.3969565 Vali Loss: 0.3456985 Test Loss: 0.3456985
Validation loss decreased (0.346706 --> 0.345699).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05927210673689842, mae:0.19503143429756165, smape:19.36168074607849, dtw:not calculated
horizon:2 mse:0.23142597079277039, mae:0.40761616826057434, smape:39.60421085357666, dtw:not calculated
horizon:3 mse:0.1770285964012146, mae:0.3371690511703491, smape:32.8565776348114, dtw:not calculated
horizon:4 mse:0.32881277799606323, mae:0.48146843910217285, smape:46.19928002357483, dtw:not calculated
horizon:5 mse:0.282234787940979, mae:0.4402405321598053, smape:42.411619424819946, dtw:not calculated
horizon:6 mse:0.4595397412776947, mae:0.5532218813896179, smape:51.974791288375854, dtw:not calculated
horizon:7 mse:0.3424583971500397, mae:0.4876561760902405, smape:46.66953980922699, dtw:not calculated
horizon:8 mse:0.3587402105331421, mae:0.4860372841358185, smape:46.267104148864746, dtw:not calculated
horizon:9 mse:0.5581710338592529, mae:0.6240905523300171, smape:58.26079845428467, dtw:not calculated
horizon:10 mse:0.26949259638786316, mae:0.4236980974674225, smape:40.84552228450775, dtw:not calculated
horizon:11 mse:0.6027732491493225, mae:0.6284692883491516, smape:57.984042167663574, dtw:not calculated
horizon:12 mse:0.4784325063228607, mae:0.581134021282196, smape:54.77849841117859, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2563856542110443, mae:0.4024578928947449, smape:38.734689354896545, dtw:not calculated
average metrics: horizon upto:12 mse:0.3456984758377075, mae:0.4704861044883728, smape:44.76780593395233, dtw:not calculated
===============================================================================
average of horizons: mse:0.3456984758377075, mae:0.4704861044883728, smape:44.76780593395233, dtw:not calculated
mean smape over horizons:  44.767805437246956
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=10, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4218642711639404
Epoch: 1, Steps: 46 | Train Loss: 0.8885696 Vali Loss: 0.6705446 Test Loss: 0.6705446
Validation loss decreased (inf --> 0.670545).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0582153797149658
Epoch: 2, Steps: 46 | Train Loss: 0.5380307 Vali Loss: 0.4547662 Test Loss: 0.4547662
Validation loss decreased (0.670545 --> 0.454766).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.019906997680664
Epoch: 3, Steps: 46 | Train Loss: 0.4320226 Vali Loss: 0.3812482 Test Loss: 0.3812482
Validation loss decreased (0.454766 --> 0.381248).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0731658935546875
Epoch: 4, Steps: 46 | Train Loss: 0.4044801 Vali Loss: 0.3513968 Test Loss: 0.3513968
Validation loss decreased (0.381248 --> 0.351397).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.154508352279663
Epoch: 5, Steps: 46 | Train Loss: 0.4059282 Vali Loss: 0.3380356 Test Loss: 0.3380356
Validation loss decreased (0.351397 --> 0.338036).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.077636480331421
Epoch: 6, Steps: 46 | Train Loss: 0.4003986 Vali Loss: 0.3309835 Test Loss: 0.3309835
Validation loss decreased (0.338036 --> 0.330983).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0845496654510498
Epoch: 7, Steps: 46 | Train Loss: 0.3915232 Vali Loss: 0.3269065 Test Loss: 0.3269065
Validation loss decreased (0.330983 --> 0.326907).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0103418827056885
Epoch: 8, Steps: 46 | Train Loss: 0.3821343 Vali Loss: 0.3271026 Test Loss: 0.3271026
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0107817649841309
Epoch: 9, Steps: 46 | Train Loss: 0.3995939 Vali Loss: 0.3255459 Test Loss: 0.3255459
Validation loss decreased (0.326907 --> 0.325546).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1069233417510986
Epoch: 10, Steps: 46 | Train Loss: 0.4062004 Vali Loss: 0.3241406 Test Loss: 0.3241406
Validation loss decreased (0.325546 --> 0.324141).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06616096943616867, mae:0.19747503101825714, smape:19.55408751964569, dtw:not calculated
horizon:2 mse:0.26120689511299133, mae:0.41617652773857117, smape:40.169668197631836, dtw:not calculated
horizon:3 mse:0.19936014711856842, mae:0.3685736060142517, smape:35.864946246147156, dtw:not calculated
horizon:4 mse:0.239694744348526, mae:0.4002944529056549, smape:38.71036469936371, dtw:not calculated
horizon:5 mse:0.24164246022701263, mae:0.4133126437664032, smape:40.05209505558014, dtw:not calculated
horizon:6 mse:0.3347722887992859, mae:0.4808201491832733, smape:45.997729897499084, dtw:not calculated
horizon:7 mse:0.5031004548072815, mae:0.5706119537353516, smape:53.24752330780029, dtw:not calculated
horizon:8 mse:0.5749884843826294, mae:0.6479467749595642, smape:60.5341374874115, dtw:not calculated
horizon:9 mse:0.242462620139122, mae:0.39876556396484375, smape:38.5641485452652, dtw:not calculated
horizon:10 mse:0.4889107942581177, mae:0.5939382314682007, smape:56.035518646240234, dtw:not calculated
horizon:11 mse:0.34539124369621277, mae:0.4902085065841675, smape:46.88744246959686, dtw:not calculated
horizon:12 mse:0.3919965326786041, mae:0.50628662109375, smape:47.92525768280029, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.223806232213974, mae:0.37944209575653076, smape:36.724814772605896, dtw:not calculated
average metrics: horizon upto:12 mse:0.32414060831069946, mae:0.4570342004299164, smape:43.62857937812805, dtw:not calculated
===============================================================================
average of horizons: mse:0.32414060831069946, mae:0.4570342004299164, smape:43.62857937812805, dtw:not calculated
mean smape over horizons:  43.6285766462485
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=10, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4928805828094482
Epoch: 1, Steps: 46 | Train Loss: 0.8362502 Vali Loss: 0.6568533 Test Loss: 0.6568533
Validation loss decreased (inf --> 0.656853).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9790468215942383
Epoch: 2, Steps: 46 | Train Loss: 0.5393422 Vali Loss: 0.4654644 Test Loss: 0.4654644
Validation loss decreased (0.656853 --> 0.465464).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0402023792266846
Epoch: 3, Steps: 46 | Train Loss: 0.4581213 Vali Loss: 0.3917371 Test Loss: 0.3917371
Validation loss decreased (0.465464 --> 0.391737).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9872901439666748
Epoch: 4, Steps: 46 | Train Loss: 0.4199790 Vali Loss: 0.3593681 Test Loss: 0.3593681
Validation loss decreased (0.391737 --> 0.359368).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0674633979797363
Epoch: 5, Steps: 46 | Train Loss: 0.4005277 Vali Loss: 0.3472668 Test Loss: 0.3472668
Validation loss decreased (0.359368 --> 0.347267).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0254464149475098
Epoch: 6, Steps: 46 | Train Loss: 0.4039784 Vali Loss: 0.3400567 Test Loss: 0.3400567
Validation loss decreased (0.347267 --> 0.340057).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0694401264190674
Epoch: 7, Steps: 46 | Train Loss: 0.3915468 Vali Loss: 0.3351256 Test Loss: 0.3351256
Validation loss decreased (0.340057 --> 0.335126).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.1375188827514648
Epoch: 8, Steps: 46 | Train Loss: 0.3837987 Vali Loss: 0.3341831 Test Loss: 0.3341831
Validation loss decreased (0.335126 --> 0.334183).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9865584373474121
Epoch: 9, Steps: 46 | Train Loss: 0.3961578 Vali Loss: 0.3330943 Test Loss: 0.3330943
Validation loss decreased (0.334183 --> 0.333094).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1384611129760742
Epoch: 10, Steps: 46 | Train Loss: 0.3938109 Vali Loss: 0.3340674 Test Loss: 0.3340674
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06218399852514267, mae:0.1841764599084854, smape:18.227124214172363, dtw:not calculated
horizon:2 mse:0.147379532456398, mae:0.32926928997039795, smape:32.349756360054016, dtw:not calculated
horizon:3 mse:0.24183474481105804, mae:0.41405194997787476, smape:40.17929136753082, dtw:not calculated
horizon:4 mse:0.3099769353866577, mae:0.4612465798854828, smape:44.28251087665558, dtw:not calculated
horizon:5 mse:0.476489394903183, mae:0.5900000929832458, smape:55.755072832107544, dtw:not calculated
horizon:6 mse:0.261583149433136, mae:0.43503186106681824, smape:42.08052158355713, dtw:not calculated
horizon:7 mse:0.3572126030921936, mae:0.5064590573310852, smape:48.421403765678406, dtw:not calculated
horizon:8 mse:0.38043078780174255, mae:0.515825629234314, smape:49.0716278553009, dtw:not calculated
horizon:9 mse:0.4226212501525879, mae:0.5639663338661194, smape:53.69288921356201, dtw:not calculated
horizon:10 mse:0.4763852655887604, mae:0.5500523447990417, smape:51.36076211929321, dtw:not calculated
horizon:11 mse:0.3677294850349426, mae:0.513468861579895, smape:49.05371069908142, dtw:not calculated
horizon:12 mse:0.49330440163612366, mae:0.5959497690200806, smape:56.18111491203308, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2499079406261444, mae:0.4022960066795349, smape:38.81237804889679, dtw:not calculated
average metrics: horizon upto:12 mse:0.3330942988395691, mae:0.47162479162216187, smape:45.05465030670166, dtw:not calculated
===============================================================================
average of horizons: mse:0.3330942988395691, mae:0.47162479162216187, smape:45.05465030670166, dtw:not calculated
mean smape over horizons:  45.05464881658554
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=10, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5232727527618408
Epoch: 1, Steps: 46 | Train Loss: 0.8155788 Vali Loss: 0.7890572 Test Loss: 0.7890572
Validation loss decreased (inf --> 0.789057).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.010741949081421
Epoch: 2, Steps: 46 | Train Loss: 0.5686652 Vali Loss: 0.5342196 Test Loss: 0.5342196
Validation loss decreased (0.789057 --> 0.534220).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0785887241363525
Epoch: 3, Steps: 46 | Train Loss: 0.4626680 Vali Loss: 0.4376564 Test Loss: 0.4376564
Validation loss decreased (0.534220 --> 0.437656).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0155248641967773
Epoch: 4, Steps: 46 | Train Loss: 0.4314660 Vali Loss: 0.3999833 Test Loss: 0.3999833
Validation loss decreased (0.437656 --> 0.399983).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0676162242889404
Epoch: 5, Steps: 46 | Train Loss: 0.4198452 Vali Loss: 0.3821493 Test Loss: 0.3821493
Validation loss decreased (0.399983 --> 0.382149).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1348388195037842
Epoch: 6, Steps: 46 | Train Loss: 0.4183419 Vali Loss: 0.3728867 Test Loss: 0.3728867
Validation loss decreased (0.382149 --> 0.372887).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.071794033050537
Epoch: 7, Steps: 46 | Train Loss: 0.4214242 Vali Loss: 0.3678922 Test Loss: 0.3678922
Validation loss decreased (0.372887 --> 0.367892).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.061267614364624
Epoch: 8, Steps: 46 | Train Loss: 0.3978632 Vali Loss: 0.3671556 Test Loss: 0.3671556
Validation loss decreased (0.367892 --> 0.367156).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0748424530029297
Epoch: 9, Steps: 46 | Train Loss: 0.4118936 Vali Loss: 0.3662210 Test Loss: 0.3662210
Validation loss decreased (0.367156 --> 0.366221).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0842969417572021
Epoch: 10, Steps: 46 | Train Loss: 0.4153118 Vali Loss: 0.3660664 Test Loss: 0.3660664
Validation loss decreased (0.366221 --> 0.366066).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06571678817272186, mae:0.19487498700618744, smape:19.28546577692032, dtw:not calculated
horizon:2 mse:0.23516413569450378, mae:0.4074888527393341, smape:39.555686712265015, dtw:not calculated
horizon:3 mse:0.48596322536468506, mae:0.5990856885910034, smape:56.65203928947449, dtw:not calculated
horizon:4 mse:0.27859818935394287, mae:0.44624412059783936, smape:43.06843876838684, dtw:not calculated
horizon:5 mse:0.20434218645095825, mae:0.3833234906196594, smape:37.35644221305847, dtw:not calculated
horizon:6 mse:0.2855246961116791, mae:0.43892306089401245, smape:42.2178715467453, dtw:not calculated
horizon:7 mse:0.4056949019432068, mae:0.5266743898391724, smape:49.9275803565979, dtw:not calculated
horizon:8 mse:0.614768385887146, mae:0.6599515080451965, smape:61.23747229576111, dtw:not calculated
horizon:9 mse:0.3821960687637329, mae:0.5044631958007812, smape:47.89105951786041, dtw:not calculated
horizon:10 mse:0.4393312335014343, mae:0.5504162907600403, smape:51.97437405586243, dtw:not calculated
horizon:11 mse:0.4829813838005066, mae:0.5607692003250122, smape:52.417707443237305, dtw:not calculated
horizon:12 mse:0.5125161409378052, mae:0.5743175745010376, smape:53.50877642631531, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2592182159423828, mae:0.4116567075252533, smape:39.68932330608368, dtw:not calculated
average metrics: horizon upto:12 mse:0.36606642603874207, mae:0.4872110188007355, smape:46.25774025917053, dtw:not calculated
===============================================================================
average of horizons: mse:0.36606642603874207, mae:0.4872110188007355, smape:46.25774025917053, dtw:not calculated
mean smape over horizons:  46.25774286687374
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=10, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4073617458343506
Epoch: 1, Steps: 46 | Train Loss: 1.0330414 Vali Loss: 0.7410818 Test Loss: 0.7410818
Validation loss decreased (inf --> 0.741082).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.07053804397583
Epoch: 2, Steps: 46 | Train Loss: 0.6498530 Vali Loss: 0.5299559 Test Loss: 0.5299559
Validation loss decreased (0.741082 --> 0.529956).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0288641452789307
Epoch: 3, Steps: 46 | Train Loss: 0.4994534 Vali Loss: 0.4570812 Test Loss: 0.4570812
Validation loss decreased (0.529956 --> 0.457081).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.01414155960083
Epoch: 4, Steps: 46 | Train Loss: 0.4475593 Vali Loss: 0.4272718 Test Loss: 0.4272718
Validation loss decreased (0.457081 --> 0.427272).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0081124305725098
Epoch: 5, Steps: 46 | Train Loss: 0.4392725 Vali Loss: 0.4123105 Test Loss: 0.4123105
Validation loss decreased (0.427272 --> 0.412311).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9342021942138672
Epoch: 6, Steps: 46 | Train Loss: 0.4463321 Vali Loss: 0.4052382 Test Loss: 0.4052382
Validation loss decreased (0.412311 --> 0.405238).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.055326223373413
Epoch: 7, Steps: 46 | Train Loss: 0.4326129 Vali Loss: 0.4015323 Test Loss: 0.4015323
Validation loss decreased (0.405238 --> 0.401532).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0971918106079102
Epoch: 8, Steps: 46 | Train Loss: 0.4236474 Vali Loss: 0.3996522 Test Loss: 0.3996522
Validation loss decreased (0.401532 --> 0.399652).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.1462147235870361
Epoch: 9, Steps: 46 | Train Loss: 0.4291748 Vali Loss: 0.3984668 Test Loss: 0.3984668
Validation loss decreased (0.399652 --> 0.398467).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0338916778564453
Epoch: 10, Steps: 46 | Train Loss: 0.4230980 Vali Loss: 0.3973904 Test Loss: 0.3973904
Validation loss decreased (0.398467 --> 0.397390).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.059269979596138, mae:0.19377794861793518, smape:19.237120449543, dtw:not calculated
horizon:2 mse:0.3984510004520416, mae:0.5444785952568054, smape:52.0196259021759, dtw:not calculated
horizon:3 mse:0.234049990773201, mae:0.4036610424518585, smape:39.17742669582367, dtw:not calculated
horizon:4 mse:0.17916417121887207, mae:0.34264758229255676, smape:33.393749594688416, dtw:not calculated
horizon:5 mse:0.22542458772659302, mae:0.3946361839771271, smape:38.32363784313202, dtw:not calculated
horizon:6 mse:0.45821672677993774, mae:0.5511725544929504, smape:51.79874897003174, dtw:not calculated
horizon:7 mse:0.44990068674087524, mae:0.5670857429504395, smape:53.62461805343628, dtw:not calculated
horizon:8 mse:0.5449511408805847, mae:0.6406645774841309, smape:60.22838354110718, dtw:not calculated
horizon:9 mse:0.6513733267784119, mae:0.680741012096405, smape:62.98762559890747, dtw:not calculated
horizon:10 mse:0.31193509697914124, mae:0.4564034640789032, smape:43.73989403247833, dtw:not calculated
horizon:11 mse:0.5116443634033203, mae:0.6103386282920837, smape:57.39899277687073, dtw:not calculated
horizon:12 mse:0.744304358959198, mae:0.7111606597900391, smape:64.7834837436676, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.25909608602523804, mae:0.40506231784820557, smape:38.991716504096985, dtw:not calculated
average metrics: horizon upto:12 mse:0.3973904550075531, mae:0.5080640316009521, smape:48.05943965911865, dtw:not calculated
===============================================================================
average of horizons: mse:0.3973904550075531, mae:0.5080640316009521, smape:48.05943965911865, dtw:not calculated
mean smape over horizons:  48.05944226682186
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=12, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4290025234222412
Epoch: 1, Steps: 46 | Train Loss: 0.9756759 Vali Loss: 0.6577799 Test Loss: 0.6577799
Validation loss decreased (inf --> 0.657780).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0192651748657227
Epoch: 2, Steps: 46 | Train Loss: 0.5701878 Vali Loss: 0.4186228 Test Loss: 0.4186228
Validation loss decreased (0.657780 --> 0.418623).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9306683540344238
Epoch: 3, Steps: 46 | Train Loss: 0.4381297 Vali Loss: 0.3477011 Test Loss: 0.3477011
Validation loss decreased (0.418623 --> 0.347701).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0060758590698242
Epoch: 4, Steps: 46 | Train Loss: 0.4037010 Vali Loss: 0.3144737 Test Loss: 0.3144737
Validation loss decreased (0.347701 --> 0.314474).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.00437331199646
Epoch: 5, Steps: 46 | Train Loss: 0.3939677 Vali Loss: 0.3014121 Test Loss: 0.3014121
Validation loss decreased (0.314474 --> 0.301412).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0490357875823975
Epoch: 6, Steps: 46 | Train Loss: 0.3959071 Vali Loss: 0.2949517 Test Loss: 0.2949517
Validation loss decreased (0.301412 --> 0.294952).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.916409969329834
Epoch: 7, Steps: 46 | Train Loss: 0.3851621 Vali Loss: 0.2894002 Test Loss: 0.2894002
Validation loss decreased (0.294952 --> 0.289400).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0946216583251953
Epoch: 8, Steps: 46 | Train Loss: 0.3761382 Vali Loss: 0.2880362 Test Loss: 0.2880362
Validation loss decreased (0.289400 --> 0.288036).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.078486680984497
Epoch: 9, Steps: 46 | Train Loss: 0.3891492 Vali Loss: 0.2879328 Test Loss: 0.2879328
Validation loss decreased (0.288036 --> 0.287933).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1465675830841064
Epoch: 10, Steps: 46 | Train Loss: 0.3875671 Vali Loss: 0.2877274 Test Loss: 0.2877274
Validation loss decreased (0.287933 --> 0.287727).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05665936321020126, mae:0.17655305564403534, smape:17.477886378765106, dtw:not calculated
horizon:2 mse:0.2618076205253601, mae:0.4254043400287628, smape:41.08729958534241, dtw:not calculated
horizon:3 mse:0.13929753005504608, mae:0.30504125356674194, smape:29.92750108242035, dtw:not calculated
horizon:4 mse:0.251356840133667, mae:0.4186713397502899, smape:40.50055742263794, dtw:not calculated
horizon:5 mse:0.5606597065925598, mae:0.65737384557724, smape:61.75925135612488, dtw:not calculated
horizon:6 mse:0.24082905054092407, mae:0.3932987153530121, smape:38.00131380558014, dtw:not calculated
horizon:7 mse:0.36873090267181396, mae:0.5057193636894226, smape:48.24082851409912, dtw:not calculated
horizon:8 mse:0.38339534401893616, mae:0.5104538798332214, smape:48.44537973403931, dtw:not calculated
horizon:9 mse:0.2881337106227875, mae:0.44367292523384094, smape:42.69716143608093, dtw:not calculated
horizon:10 mse:0.38404616713523865, mae:0.5168620347976685, smape:49.18918311595917, dtw:not calculated
horizon:11 mse:0.2951509356498718, mae:0.4396912157535553, smape:42.18906760215759, dtw:not calculated
horizon:12 mse:0.222661092877388, mae:0.38964617252349854, smape:37.8363311290741, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2517683506011963, mae:0.3960570991039276, smape:38.12563419342041, dtw:not calculated
average metrics: horizon upto:12 mse:0.28772732615470886, mae:0.4318656921386719, smape:41.44597947597504, dtw:not calculated
===============================================================================
average of horizons: mse:0.28772732615470886, mae:0.4318656921386719, smape:41.44597947597504, dtw:not calculated
mean smape over horizons:  41.44598009685675
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=12, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4167869091033936
Epoch: 1, Steps: 46 | Train Loss: 0.9306826 Vali Loss: 0.7803493 Test Loss: 0.7803493
Validation loss decreased (inf --> 0.780349).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0603225231170654
Epoch: 2, Steps: 46 | Train Loss: 0.5901727 Vali Loss: 0.5241062 Test Loss: 0.5241062
Validation loss decreased (0.780349 --> 0.524106).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0348479747772217
Epoch: 3, Steps: 46 | Train Loss: 0.4638815 Vali Loss: 0.4273015 Test Loss: 0.4273015
Validation loss decreased (0.524106 --> 0.427301).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0799369812011719
Epoch: 4, Steps: 46 | Train Loss: 0.4121232 Vali Loss: 0.3917248 Test Loss: 0.3917248
Validation loss decreased (0.427301 --> 0.391725).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0634493827819824
Epoch: 5, Steps: 46 | Train Loss: 0.4090930 Vali Loss: 0.3691200 Test Loss: 0.3691200
Validation loss decreased (0.391725 --> 0.369120).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9831721782684326
Epoch: 6, Steps: 46 | Train Loss: 0.4107800 Vali Loss: 0.3616682 Test Loss: 0.3616682
Validation loss decreased (0.369120 --> 0.361668).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0292391777038574
Epoch: 7, Steps: 46 | Train Loss: 0.3968104 Vali Loss: 0.3582447 Test Loss: 0.3582447
Validation loss decreased (0.361668 --> 0.358245).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0831341743469238
Epoch: 8, Steps: 46 | Train Loss: 0.3927857 Vali Loss: 0.3558306 Test Loss: 0.3558306
Validation loss decreased (0.358245 --> 0.355831).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0622732639312744
Epoch: 9, Steps: 46 | Train Loss: 0.4020237 Vali Loss: 0.3543182 Test Loss: 0.3543182
Validation loss decreased (0.355831 --> 0.354318).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1012415885925293
Epoch: 10, Steps: 46 | Train Loss: 0.3985578 Vali Loss: 0.3523906 Test Loss: 0.3523906
Validation loss decreased (0.354318 --> 0.352391).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04269300028681755, mae:0.14912378787994385, smape:14.80717808008194, dtw:not calculated
horizon:2 mse:0.25237029790878296, mae:0.42503055930137634, smape:41.19367599487305, dtw:not calculated
horizon:3 mse:0.28132978081703186, mae:0.445870578289032, smape:43.01624894142151, dtw:not calculated
horizon:4 mse:0.26349344849586487, mae:0.4233644902706146, smape:40.85192084312439, dtw:not calculated
horizon:5 mse:0.33892086148262024, mae:0.48009222745895386, smape:45.89252769947052, dtw:not calculated
horizon:6 mse:0.3713946044445038, mae:0.5195006728172302, smape:49.68816637992859, dtw:not calculated
horizon:7 mse:0.3516085743904114, mae:0.4990510940551758, smape:47.72465527057648, dtw:not calculated
horizon:8 mse:0.5899229645729065, mae:0.6476294994354248, smape:60.25319695472717, dtw:not calculated
horizon:9 mse:0.3479582965373993, mae:0.48654094338417053, smape:46.4790016412735, dtw:not calculated
horizon:10 mse:0.4516472816467285, mae:0.5539860129356384, smape:52.18396782875061, dtw:not calculated
horizon:11 mse:0.3722307085990906, mae:0.5063464045524597, smape:48.243433237075806, dtw:not calculated
horizon:12 mse:0.5651171207427979, mae:0.6151481866836548, smape:57.13069438934326, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.25836697220802307, mae:0.40716370940208435, smape:39.241620898246765, dtw:not calculated
average metrics: horizon upto:12 mse:0.3523905873298645, mae:0.47930702567100525, smape:45.622050762176514, dtw:not calculated
===============================================================================
average of horizons: mse:0.3523905873298645, mae:0.47930702567100525, smape:45.622050762176514, dtw:not calculated
mean smape over horizons:  45.6220556050539
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=12, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4835522174835205
Epoch: 1, Steps: 46 | Train Loss: 0.7534599 Vali Loss: 0.6430876 Test Loss: 0.6430876
Validation loss decreased (inf --> 0.643088).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0229201316833496
Epoch: 2, Steps: 46 | Train Loss: 0.5141781 Vali Loss: 0.4383717 Test Loss: 0.4383717
Validation loss decreased (0.643088 --> 0.438372).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.099050760269165
Epoch: 3, Steps: 46 | Train Loss: 0.4199529 Vali Loss: 0.3695474 Test Loss: 0.3695474
Validation loss decreased (0.438372 --> 0.369547).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.064620018005371
Epoch: 4, Steps: 46 | Train Loss: 0.3779854 Vali Loss: 0.3421369 Test Loss: 0.3421369
Validation loss decreased (0.369547 --> 0.342137).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0983736515045166
Epoch: 5, Steps: 46 | Train Loss: 0.3738111 Vali Loss: 0.3299038 Test Loss: 0.3299038
Validation loss decreased (0.342137 --> 0.329904).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0574853420257568
Epoch: 6, Steps: 46 | Train Loss: 0.3783545 Vali Loss: 0.3235511 Test Loss: 0.3235511
Validation loss decreased (0.329904 --> 0.323551).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.062647819519043
Epoch: 7, Steps: 46 | Train Loss: 0.3817717 Vali Loss: 0.3185953 Test Loss: 0.3185953
Validation loss decreased (0.323551 --> 0.318595).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.054877519607544
Epoch: 8, Steps: 46 | Train Loss: 0.3638219 Vali Loss: 0.3166758 Test Loss: 0.3166758
Validation loss decreased (0.318595 --> 0.316676).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0678260326385498
Epoch: 9, Steps: 46 | Train Loss: 0.3888415 Vali Loss: 0.3179989 Test Loss: 0.3179989
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0917599201202393
Epoch: 10, Steps: 46 | Train Loss: 0.3664776 Vali Loss: 0.3165508 Test Loss: 0.3165508
Validation loss decreased (0.316676 --> 0.316551).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.051216501742601395, mae:0.16466520726680756, smape:16.305585205554962, dtw:not calculated
horizon:2 mse:0.1497259885072708, mae:0.2982947528362274, smape:29.113829135894775, dtw:not calculated
horizon:3 mse:0.2030632495880127, mae:0.3938957750797272, smape:38.47220242023468, dtw:not calculated
horizon:4 mse:0.2398880571126938, mae:0.4014594852924347, smape:38.826775550842285, dtw:not calculated
horizon:5 mse:0.27679288387298584, mae:0.42677780985832214, smape:41.073691844940186, dtw:not calculated
horizon:6 mse:0.3389540910720825, mae:0.47499528527259827, smape:45.323070883750916, dtw:not calculated
horizon:7 mse:0.5998990535736084, mae:0.6589800715446472, smape:61.3752543926239, dtw:not calculated
horizon:8 mse:0.27555713057518005, mae:0.4290825128555298, smape:41.31675064563751, dtw:not calculated
horizon:9 mse:0.5097824931144714, mae:0.5809189677238464, smape:54.200851917266846, dtw:not calculated
horizon:10 mse:0.3195066452026367, mae:0.45333945751190186, smape:43.34036409854889, dtw:not calculated
horizon:11 mse:0.3936968743801117, mae:0.4896172881126404, smape:46.17273211479187, dtw:not calculated
horizon:12 mse:0.44052594900131226, mae:0.5607748627662659, smape:53.12846302986145, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2099401205778122, mae:0.36001473665237427, smape:34.85252559185028, dtw:not calculated
average metrics: horizon upto:12 mse:0.31655073165893555, mae:0.4444001019001007, smape:42.38746166229248, dtw:not calculated
===============================================================================
average of horizons: mse:0.31655073165893555, mae:0.4444001019001007, smape:42.38746166229248, dtw:not calculated
mean smape over horizons:  42.38746426999569
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=12, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3277432918548584
Epoch: 1, Steps: 46 | Train Loss: 0.6109213 Vali Loss: 0.6559156 Test Loss: 0.6559156
Validation loss decreased (inf --> 0.655916).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0453612804412842
Epoch: 2, Steps: 46 | Train Loss: 0.5129207 Vali Loss: 0.4655177 Test Loss: 0.4655177
Validation loss decreased (0.655916 --> 0.465518).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0432672500610352
Epoch: 3, Steps: 46 | Train Loss: 0.4463513 Vali Loss: 0.3921597 Test Loss: 0.3921597
Validation loss decreased (0.465518 --> 0.392160).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.080348253250122
Epoch: 4, Steps: 46 | Train Loss: 0.4123141 Vali Loss: 0.3614779 Test Loss: 0.3614779
Validation loss decreased (0.392160 --> 0.361478).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0403666496276855
Epoch: 5, Steps: 46 | Train Loss: 0.4149803 Vali Loss: 0.3469504 Test Loss: 0.3469504
Validation loss decreased (0.361478 --> 0.346950).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0173895359039307
Epoch: 6, Steps: 46 | Train Loss: 0.4081738 Vali Loss: 0.3422425 Test Loss: 0.3422425
Validation loss decreased (0.346950 --> 0.342243).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0210292339324951
Epoch: 7, Steps: 46 | Train Loss: 0.4022479 Vali Loss: 0.3382687 Test Loss: 0.3382687
Validation loss decreased (0.342243 --> 0.338269).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.1283881664276123
Epoch: 8, Steps: 46 | Train Loss: 0.3886280 Vali Loss: 0.3362352 Test Loss: 0.3362352
Validation loss decreased (0.338269 --> 0.336235).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0113437175750732
Epoch: 9, Steps: 46 | Train Loss: 0.3901948 Vali Loss: 0.3345245 Test Loss: 0.3345245
Validation loss decreased (0.336235 --> 0.334525).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0294804573059082
Epoch: 10, Steps: 46 | Train Loss: 0.4048087 Vali Loss: 0.3349855 Test Loss: 0.3349855
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.053830090910196304, mae:0.18286333978176117, smape:18.154633045196533, dtw:not calculated
horizon:2 mse:0.25522229075431824, mae:0.4446588456630707, smape:43.24810206890106, dtw:not calculated
horizon:3 mse:0.15214553475379944, mae:0.31639882922172546, smape:30.969712138175964, dtw:not calculated
horizon:4 mse:0.20658966898918152, mae:0.38589921593666077, smape:37.62955665588379, dtw:not calculated
horizon:5 mse:0.2883623242378235, mae:0.4426124393939972, smape:42.566853761672974, dtw:not calculated
horizon:6 mse:0.33533212542533875, mae:0.47139236330986023, smape:45.00107765197754, dtw:not calculated
horizon:7 mse:0.4184711277484894, mae:0.5402415990829468, smape:51.196300983428955, dtw:not calculated
horizon:8 mse:0.4069063663482666, mae:0.5156833529472351, smape:48.79695773124695, dtw:not calculated
horizon:9 mse:0.5165674090385437, mae:0.6055355072021484, smape:56.82262182235718, dtw:not calculated
horizon:10 mse:0.6580970883369446, mae:0.6821086406707764, smape:62.9883348941803, dtw:not calculated
horizon:11 mse:0.3329983353614807, mae:0.4675391912460327, smape:44.65134143829346, dtw:not calculated
horizon:12 mse:0.38977208733558655, mae:0.5248658657073975, smape:49.957191944122314, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2152470052242279, mae:0.3739708364009857, smape:36.26165688037872, dtw:not calculated
average metrics: horizon upto:12 mse:0.3345245122909546, mae:0.4649832546710968, smape:44.33189034461975, dtw:not calculated
===============================================================================
average of horizons: mse:0.3345245122909546, mae:0.4649832546710968, smape:44.33189034461975, dtw:not calculated
mean smape over horizons:  44.33189034461975
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=12, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4915409088134766
Epoch: 1, Steps: 46 | Train Loss: 0.5974493 Vali Loss: 0.6202782 Test Loss: 0.6202782
Validation loss decreased (inf --> 0.620278).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0539088249206543
Epoch: 2, Steps: 46 | Train Loss: 0.4725003 Vali Loss: 0.4232683 Test Loss: 0.4232683
Validation loss decreased (0.620278 --> 0.423268).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.053466558456421
Epoch: 3, Steps: 46 | Train Loss: 0.4100591 Vali Loss: 0.3558152 Test Loss: 0.3558152
Validation loss decreased (0.423268 --> 0.355815).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1061406135559082
Epoch: 4, Steps: 46 | Train Loss: 0.3793938 Vali Loss: 0.3297217 Test Loss: 0.3297217
Validation loss decreased (0.355815 --> 0.329722).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0956506729125977
Epoch: 5, Steps: 46 | Train Loss: 0.3837486 Vali Loss: 0.3175757 Test Loss: 0.3175757
Validation loss decreased (0.329722 --> 0.317576).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9665865898132324
Epoch: 6, Steps: 46 | Train Loss: 0.3768845 Vali Loss: 0.3133690 Test Loss: 0.3133690
Validation loss decreased (0.317576 --> 0.313369).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9526464939117432
Epoch: 7, Steps: 46 | Train Loss: 0.3725722 Vali Loss: 0.3100958 Test Loss: 0.3100958
Validation loss decreased (0.313369 --> 0.310096).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0023019313812256
Epoch: 8, Steps: 46 | Train Loss: 0.3566105 Vali Loss: 0.3099632 Test Loss: 0.3099632
Validation loss decreased (0.310096 --> 0.309963).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0388994216918945
Epoch: 9, Steps: 46 | Train Loss: 0.3590655 Vali Loss: 0.3063734 Test Loss: 0.3063734
Validation loss decreased (0.309963 --> 0.306373).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.058959722518921
Epoch: 10, Steps: 46 | Train Loss: 0.3768952 Vali Loss: 0.3068361 Test Loss: 0.3068361
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03004370629787445, mae:0.1414976418018341, smape:14.114576578140259, dtw:not calculated
horizon:2 mse:0.16585686802864075, mae:0.34813544154167175, smape:34.14742350578308, dtw:not calculated
horizon:3 mse:0.1258297860622406, mae:0.277191162109375, smape:27.197864651679993, dtw:not calculated
horizon:4 mse:0.14542676508426666, mae:0.30906257033348083, smape:30.308592319488525, dtw:not calculated
horizon:5 mse:0.269167423248291, mae:0.4266811013221741, smape:41.09108746051788, dtw:not calculated
horizon:6 mse:0.2575589418411255, mae:0.39676281809806824, smape:38.12721073627472, dtw:not calculated
horizon:7 mse:0.3982362151145935, mae:0.5284516215324402, smape:50.1834511756897, dtw:not calculated
horizon:8 mse:0.38029471039772034, mae:0.49935778975486755, smape:47.38700091838837, dtw:not calculated
horizon:9 mse:0.5660684704780579, mae:0.6480154395103455, smape:60.62844395637512, dtw:not calculated
horizon:10 mse:0.6450015306472778, mae:0.6850352883338928, smape:63.48334550857544, dtw:not calculated
horizon:11 mse:0.2908438742160797, mae:0.4449591040611267, smape:42.804574966430664, dtw:not calculated
horizon:12 mse:0.40215185284614563, mae:0.5404149293899536, smape:51.41448378562927, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.16564726829528809, mae:0.3165551424026489, smape:30.831125378608704, dtw:not calculated
average metrics: horizon upto:12 mse:0.30637335777282715, mae:0.4371304214000702, smape:41.740670800209045, dtw:not calculated
===============================================================================
average of horizons: mse:0.30637335777282715, mae:0.4371304214000702, smape:41.740670800209045, dtw:not calculated
mean smape over horizons:  41.74067129691442
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=14, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.331023931503296
Epoch: 1, Steps: 46 | Train Loss: 0.8207802 Vali Loss: 0.6686900 Test Loss: 0.6686900
Validation loss decreased (inf --> 0.668690).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9908154010772705
Epoch: 2, Steps: 46 | Train Loss: 0.5323665 Vali Loss: 0.4318182 Test Loss: 0.4318182
Validation loss decreased (0.668690 --> 0.431818).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.041260004043579
Epoch: 3, Steps: 46 | Train Loss: 0.4569150 Vali Loss: 0.3544443 Test Loss: 0.3544443
Validation loss decreased (0.431818 --> 0.354444).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0560493469238281
Epoch: 4, Steps: 46 | Train Loss: 0.4139167 Vali Loss: 0.3256311 Test Loss: 0.3256311
Validation loss decreased (0.354444 --> 0.325631).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9522488117218018
Epoch: 5, Steps: 46 | Train Loss: 0.3964813 Vali Loss: 0.3146245 Test Loss: 0.3146245
Validation loss decreased (0.325631 --> 0.314625).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9729855060577393
Epoch: 6, Steps: 46 | Train Loss: 0.3962571 Vali Loss: 0.3071726 Test Loss: 0.3071726
Validation loss decreased (0.314625 --> 0.307173).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9841477870941162
Epoch: 7, Steps: 46 | Train Loss: 0.3957170 Vali Loss: 0.3036219 Test Loss: 0.3036219
Validation loss decreased (0.307173 --> 0.303622).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0384624004364014
Epoch: 8, Steps: 46 | Train Loss: 0.3950211 Vali Loss: 0.3010775 Test Loss: 0.3010775
Validation loss decreased (0.303622 --> 0.301077).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9331822395324707
Epoch: 9, Steps: 46 | Train Loss: 0.3873552 Vali Loss: 0.3015014 Test Loss: 0.3015014
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.097154140472412
Epoch: 10, Steps: 46 | Train Loss: 0.3882497 Vali Loss: 0.3002400 Test Loss: 0.3002400
Validation loss decreased (0.301077 --> 0.300240).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.104365274310112, mae:0.2537078559398651, smape:24.985933303833008, dtw:not calculated
horizon:2 mse:0.2334681898355484, mae:0.39162856340408325, smape:37.89258599281311, dtw:not calculated
horizon:3 mse:0.2487107217311859, mae:0.40232715010643005, smape:38.869476318359375, dtw:not calculated
horizon:4 mse:0.2462664544582367, mae:0.39099279046058655, smape:37.68797516822815, dtw:not calculated
horizon:5 mse:0.24861761927604675, mae:0.4125590920448303, smape:39.929378032684326, dtw:not calculated
horizon:6 mse:0.39935144782066345, mae:0.5348003506660461, smape:50.904494524002075, dtw:not calculated
horizon:7 mse:0.3533838391304016, mae:0.49031007289886475, smape:46.82912826538086, dtw:not calculated
horizon:8 mse:0.268636554479599, mae:0.41139259934425354, smape:39.552149176597595, dtw:not calculated
horizon:9 mse:0.30673354864120483, mae:0.43821296095848083, smape:41.876259446144104, dtw:not calculated
horizon:10 mse:0.40376758575439453, mae:0.5219134092330933, smape:49.453940987586975, dtw:not calculated
horizon:11 mse:0.5295108556747437, mae:0.6107908487319946, smape:57.16207027435303, dtw:not calculated
horizon:12 mse:0.26006728410720825, mae:0.4015493392944336, smape:38.60739469528198, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2467966079711914, mae:0.3976692855358124, smape:38.37830722332001, dtw:not calculated
average metrics: horizon upto:12 mse:0.3002399206161499, mae:0.4383487403392792, smape:41.97923243045807, dtw:not calculated
===============================================================================
average of horizons: mse:0.3002399206161499, mae:0.4383487403392792, smape:41.97923243045807, dtw:not calculated
mean smape over horizons:  41.979232182105385
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=14, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4601242542266846
Epoch: 1, Steps: 46 | Train Loss: 0.8095075 Vali Loss: 0.7901962 Test Loss: 0.7901962
Validation loss decreased (inf --> 0.790196).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1220381259918213
Epoch: 2, Steps: 46 | Train Loss: 0.5871578 Vali Loss: 0.5742818 Test Loss: 0.5742818
Validation loss decreased (0.790196 --> 0.574282).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0578703880310059
Epoch: 3, Steps: 46 | Train Loss: 0.4978849 Vali Loss: 0.4880877 Test Loss: 0.4880877
Validation loss decreased (0.574282 --> 0.488088).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.031062364578247
Epoch: 4, Steps: 46 | Train Loss: 0.4818884 Vali Loss: 0.4493646 Test Loss: 0.4493646
Validation loss decreased (0.488088 --> 0.449365).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0420782566070557
Epoch: 5, Steps: 46 | Train Loss: 0.4708881 Vali Loss: 0.4301482 Test Loss: 0.4301482
Validation loss decreased (0.449365 --> 0.430148).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0450716018676758
Epoch: 6, Steps: 46 | Train Loss: 0.4514689 Vali Loss: 0.4212404 Test Loss: 0.4212404
Validation loss decreased (0.430148 --> 0.421240).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.1024816036224365
Epoch: 7, Steps: 46 | Train Loss: 0.4624379 Vali Loss: 0.4176442 Test Loss: 0.4176442
Validation loss decreased (0.421240 --> 0.417644).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.1009149551391602
Epoch: 8, Steps: 46 | Train Loss: 0.4599166 Vali Loss: 0.4158037 Test Loss: 0.4158037
Validation loss decreased (0.417644 --> 0.415804).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0144028663635254
Epoch: 9, Steps: 46 | Train Loss: 0.4506344 Vali Loss: 0.4154390 Test Loss: 0.4154390
Validation loss decreased (0.415804 --> 0.415439).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1045806407928467
Epoch: 10, Steps: 46 | Train Loss: 0.4488232 Vali Loss: 0.4135948 Test Loss: 0.4135948
Validation loss decreased (0.415439 --> 0.413595).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.07635880261659622, mae:0.2107381969690323, smape:20.824792981147766, dtw:not calculated
horizon:2 mse:0.19607116281986237, mae:0.3525327444076538, smape:34.25140380859375, dtw:not calculated
horizon:3 mse:0.30939051508903503, mae:0.472454309463501, smape:45.44544517993927, dtw:not calculated
horizon:4 mse:0.3644387423992157, mae:0.5113607048988342, smape:48.880818486213684, dtw:not calculated
horizon:5 mse:0.33532223105430603, mae:0.4871816039085388, smape:46.70551121234894, dtw:not calculated
horizon:6 mse:0.4832608103752136, mae:0.5737387537956238, smape:53.84414196014404, dtw:not calculated
horizon:7 mse:0.5158756971359253, mae:0.6060609817504883, smape:56.907814741134644, dtw:not calculated
horizon:8 mse:0.7610679268836975, mae:0.7412976026535034, smape:67.87868142127991, dtw:not calculated
horizon:9 mse:0.6944108009338379, mae:0.6986556649208069, smape:64.17067646980286, dtw:not calculated
horizon:10 mse:0.3905014991760254, mae:0.5140543580055237, smape:48.795026540756226, dtw:not calculated
horizon:11 mse:0.4283412992954254, mae:0.5349240303039551, smape:50.47638416290283, dtw:not calculated
horizon:12 mse:0.4080987572669983, mae:0.5052316784858704, smape:47.6277619600296, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.29414036870002747, mae:0.4346677362918854, smape:41.658684611320496, dtw:not calculated
average metrics: horizon upto:12 mse:0.4135948419570923, mae:0.5173525214195251, smape:48.81737232208252, dtw:not calculated
===============================================================================
average of horizons: mse:0.4135948419570923, mae:0.5173525214195251, smape:48.81737232208252, dtw:not calculated
mean smape over horizons:  48.81737157702446
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=14, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.440859079360962
Epoch: 1, Steps: 46 | Train Loss: 0.8518769 Vali Loss: 0.7504308 Test Loss: 0.7504308
Validation loss decreased (inf --> 0.750431).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9899623394012451
Epoch: 2, Steps: 46 | Train Loss: 0.5845760 Vali Loss: 0.5529323 Test Loss: 0.5529323
Validation loss decreased (0.750431 --> 0.552932).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9852466583251953
Epoch: 3, Steps: 46 | Train Loss: 0.5002987 Vali Loss: 0.4748263 Test Loss: 0.4748263
Validation loss decreased (0.552932 --> 0.474826).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.043165922164917
Epoch: 4, Steps: 46 | Train Loss: 0.4715007 Vali Loss: 0.4436588 Test Loss: 0.4436588
Validation loss decreased (0.474826 --> 0.443659).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0527853965759277
Epoch: 5, Steps: 46 | Train Loss: 0.4568213 Vali Loss: 0.4251805 Test Loss: 0.4251805
Validation loss decreased (0.443659 --> 0.425180).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0446703433990479
Epoch: 6, Steps: 46 | Train Loss: 0.4531945 Vali Loss: 0.4187396 Test Loss: 0.4187396
Validation loss decreased (0.425180 --> 0.418740).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0293879508972168
Epoch: 7, Steps: 46 | Train Loss: 0.4472837 Vali Loss: 0.4144068 Test Loss: 0.4144068
Validation loss decreased (0.418740 --> 0.414407).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9901368618011475
Epoch: 8, Steps: 46 | Train Loss: 0.4442248 Vali Loss: 0.4124055 Test Loss: 0.4124055
Validation loss decreased (0.414407 --> 0.412405).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9436485767364502
Epoch: 9, Steps: 46 | Train Loss: 0.4451110 Vali Loss: 0.4114788 Test Loss: 0.4114788
Validation loss decreased (0.412405 --> 0.411479).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0480003356933594
Epoch: 10, Steps: 46 | Train Loss: 0.4426961 Vali Loss: 0.4118218 Test Loss: 0.4118218
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06410964578390121, mae:0.189573273062706, smape:18.766500055789948, dtw:not calculated
horizon:2 mse:0.2675025761127472, mae:0.4316861629486084, smape:41.684457659721375, dtw:not calculated
horizon:3 mse:0.1509692668914795, mae:0.32416772842407227, smape:31.79980218410492, dtw:not calculated
horizon:4 mse:0.42150425910949707, mae:0.5489968061447144, smape:52.127134799957275, dtw:not calculated
horizon:5 mse:0.4601728916168213, mae:0.5562620759010315, smape:52.36822962760925, dtw:not calculated
horizon:6 mse:0.38419806957244873, mae:0.5243432521820068, smape:49.97962713241577, dtw:not calculated
horizon:7 mse:0.5778513550758362, mae:0.6512213349342346, smape:60.8769953250885, dtw:not calculated
horizon:8 mse:0.8486906886100769, mae:0.7529183626174927, smape:67.7405059337616, dtw:not calculated
horizon:9 mse:0.3406237065792084, mae:0.4850362539291382, smape:46.38842046260834, dtw:not calculated
horizon:10 mse:0.41071704030036926, mae:0.5258716940879822, smape:49.793997406959534, dtw:not calculated
horizon:11 mse:0.3572293817996979, mae:0.4912152886390686, smape:46.85077965259552, dtw:not calculated
horizon:12 mse:0.6541767120361328, mae:0.648199200630188, smape:59.28046107292175, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.291409432888031, mae:0.4291715919971466, smape:41.12096130847931, dtw:not calculated
average metrics: horizon upto:12 mse:0.41147881746292114, mae:0.510791003704071, smape:48.138076066970825, dtw:not calculated
===============================================================================
average of horizons: mse:0.41147881746292114, mae:0.510791003704071, smape:48.138076066970825, dtw:not calculated
mean smape over horizons:  48.13807594279448
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=14, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.396352767944336
Epoch: 1, Steps: 46 | Train Loss: 0.7779296 Vali Loss: 0.7381592 Test Loss: 0.7381592
Validation loss decreased (inf --> 0.738159).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9155571460723877
Epoch: 2, Steps: 46 | Train Loss: 0.5375800 Vali Loss: 0.5355835 Test Loss: 0.5355835
Validation loss decreased (0.738159 --> 0.535583).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1048009395599365
Epoch: 3, Steps: 46 | Train Loss: 0.4659508 Vali Loss: 0.4575485 Test Loss: 0.4575485
Validation loss decreased (0.535583 --> 0.457548).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0340564250946045
Epoch: 4, Steps: 46 | Train Loss: 0.4386977 Vali Loss: 0.4260388 Test Loss: 0.4260388
Validation loss decreased (0.457548 --> 0.426039).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.114927053451538
Epoch: 5, Steps: 46 | Train Loss: 0.4278992 Vali Loss: 0.4090935 Test Loss: 0.4090935
Validation loss decreased (0.426039 --> 0.409093).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0955510139465332
Epoch: 6, Steps: 46 | Train Loss: 0.4241836 Vali Loss: 0.4029637 Test Loss: 0.4029637
Validation loss decreased (0.409093 --> 0.402964).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0218250751495361
Epoch: 7, Steps: 46 | Train Loss: 0.4195666 Vali Loss: 0.3988652 Test Loss: 0.3988652
Validation loss decreased (0.402964 --> 0.398865).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9959056377410889
Epoch: 8, Steps: 46 | Train Loss: 0.4141653 Vali Loss: 0.3962047 Test Loss: 0.3962047
Validation loss decreased (0.398865 --> 0.396205).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0847642421722412
Epoch: 9, Steps: 46 | Train Loss: 0.4174953 Vali Loss: 0.3949849 Test Loss: 0.3949849
Validation loss decreased (0.396205 --> 0.394985).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0662360191345215
Epoch: 10, Steps: 46 | Train Loss: 0.4100081 Vali Loss: 0.3952872 Test Loss: 0.3952872
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0437798798084259, mae:0.15432024002075195, smape:15.324240922927856, dtw:not calculated
horizon:2 mse:0.16449753940105438, mae:0.34182336926460266, smape:33.47330093383789, dtw:not calculated
horizon:3 mse:0.1362435221672058, mae:0.2988855540752411, smape:29.34282422065735, dtw:not calculated
horizon:4 mse:0.4290006458759308, mae:0.5498616099357605, smape:52.14294791221619, dtw:not calculated
horizon:5 mse:0.32782432436943054, mae:0.4647071659564972, smape:44.42966878414154, dtw:not calculated
horizon:6 mse:0.3144732117652893, mae:0.4750206172466278, smape:45.684635639190674, dtw:not calculated
horizon:7 mse:0.5373655557632446, mae:0.6222913265228271, smape:58.384788036346436, dtw:not calculated
horizon:8 mse:0.926275372505188, mae:0.8116844296455383, smape:72.89882898330688, dtw:not calculated
horizon:9 mse:0.35014021396636963, mae:0.5008236765861511, smape:47.92034327983856, dtw:not calculated
horizon:10 mse:0.4594491720199585, mae:0.5562271475791931, smape:52.356159687042236, dtw:not calculated
horizon:11 mse:0.3348556160926819, mae:0.4874894320964813, smape:46.70489728450775, dtw:not calculated
horizon:12 mse:0.7159141898155212, mae:0.6692630052566528, smape:60.66211462020874, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.23596985638141632, mae:0.3807697594165802, smape:36.73293590545654, dtw:not calculated
average metrics: horizon upto:12 mse:0.3949849307537079, mae:0.49436643719673157, smape:46.61039710044861, dtw:not calculated
===============================================================================
average of horizons: mse:0.3949849307537079, mae:0.49436643719673157, smape:46.61039710044861, dtw:not calculated
mean smape over horizons:  46.61039585868517
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=14, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4457635879516602
Epoch: 1, Steps: 46 | Train Loss: 0.8355551 Vali Loss: 0.6872848 Test Loss: 0.6872848
Validation loss decreased (inf --> 0.687285).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9909486770629883
Epoch: 2, Steps: 46 | Train Loss: 0.5904852 Vali Loss: 0.5207501 Test Loss: 0.5207501
Validation loss decreased (0.687285 --> 0.520750).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0694026947021484
Epoch: 3, Steps: 46 | Train Loss: 0.5025499 Vali Loss: 0.4593248 Test Loss: 0.4593248
Validation loss decreased (0.520750 --> 0.459325).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1258761882781982
Epoch: 4, Steps: 46 | Train Loss: 0.4628568 Vali Loss: 0.4274136 Test Loss: 0.4274136
Validation loss decreased (0.459325 --> 0.427414).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.053389549255371
Epoch: 5, Steps: 46 | Train Loss: 0.4428423 Vali Loss: 0.4141243 Test Loss: 0.4141243
Validation loss decreased (0.427414 --> 0.414124).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0386724472045898
Epoch: 6, Steps: 46 | Train Loss: 0.4456987 Vali Loss: 0.4093772 Test Loss: 0.4093772
Validation loss decreased (0.414124 --> 0.409377).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.019864559173584
Epoch: 7, Steps: 46 | Train Loss: 0.4475607 Vali Loss: 0.4057740 Test Loss: 0.4057740
Validation loss decreased (0.409377 --> 0.405774).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9587571620941162
Epoch: 8, Steps: 46 | Train Loss: 0.4304385 Vali Loss: 0.4028066 Test Loss: 0.4028066
Validation loss decreased (0.405774 --> 0.402807).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0271580219268799
Epoch: 9, Steps: 46 | Train Loss: 0.4466488 Vali Loss: 0.4012895 Test Loss: 0.4012895
Validation loss decreased (0.402807 --> 0.401289).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0366039276123047
Epoch: 10, Steps: 46 | Train Loss: 0.4395107 Vali Loss: 0.4018245 Test Loss: 0.4018245
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06498564034700394, mae:0.19301991164684296, smape:19.108933210372925, dtw:not calculated
horizon:2 mse:0.2410755753517151, mae:0.42765262722969055, smape:41.589438915252686, dtw:not calculated
horizon:3 mse:0.20343121886253357, mae:0.36459341645240784, smape:35.40848195552826, dtw:not calculated
horizon:4 mse:0.3872705101966858, mae:0.5180863738059998, smape:49.32161271572113, dtw:not calculated
horizon:5 mse:0.4510332942008972, mae:0.5689302086830139, smape:53.85235548019409, dtw:not calculated
horizon:6 mse:0.5792896747589111, mae:0.6283384561538696, smape:58.33756923675537, dtw:not calculated
horizon:7 mse:0.5920189619064331, mae:0.6273490786552429, smape:58.050185441970825, dtw:not calculated
horizon:8 mse:0.41080281138420105, mae:0.5110942125320435, smape:48.180365562438965, dtw:not calculated
horizon:9 mse:0.42301231622695923, mae:0.5370760560035706, smape:50.72851777076721, dtw:not calculated
horizon:10 mse:0.6915234327316284, mae:0.7112661600112915, smape:65.64226150512695, dtw:not calculated
horizon:11 mse:0.4398098587989807, mae:0.5421076416969299, smape:51.09950304031372, dtw:not calculated
horizon:12 mse:0.3312203586101532, mae:0.4561982750892639, smape:43.47846806049347, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3211809992790222, mae:0.4501034915447235, smape:42.93639659881592, dtw:not calculated
average metrics: horizon upto:12 mse:0.4012894928455353, mae:0.5071426630020142, smape:47.89980947971344, dtw:not calculated
===============================================================================
average of horizons: mse:0.4012894928455353, mae:0.5071426630020142, smape:47.89980947971344, dtw:not calculated
mean smape over horizons:  47.899807741244636
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=14, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4444124698638916
Epoch: 1, Steps: 46 | Train Loss: 0.8423559 Vali Loss: 0.7108017 Test Loss: 0.7108017
Validation loss decreased (inf --> 0.710802).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0176470279693604
Epoch: 2, Steps: 46 | Train Loss: 0.5769488 Vali Loss: 0.5212470 Test Loss: 0.5212470
Validation loss decreased (0.710802 --> 0.521247).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0447618961334229
Epoch: 3, Steps: 46 | Train Loss: 0.4890212 Vali Loss: 0.4498690 Test Loss: 0.4498690
Validation loss decreased (0.521247 --> 0.449869).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.950507402420044
Epoch: 4, Steps: 46 | Train Loss: 0.4521106 Vali Loss: 0.4145406 Test Loss: 0.4145406
Validation loss decreased (0.449869 --> 0.414541).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0382733345031738
Epoch: 5, Steps: 46 | Train Loss: 0.4316769 Vali Loss: 0.4011044 Test Loss: 0.4011044
Validation loss decreased (0.414541 --> 0.401104).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0307230949401855
Epoch: 6, Steps: 46 | Train Loss: 0.4332873 Vali Loss: 0.3961233 Test Loss: 0.3961233
Validation loss decreased (0.401104 --> 0.396123).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0718772411346436
Epoch: 7, Steps: 46 | Train Loss: 0.4356639 Vali Loss: 0.3912362 Test Loss: 0.3912362
Validation loss decreased (0.396123 --> 0.391236).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0269966125488281
Epoch: 8, Steps: 46 | Train Loss: 0.4160001 Vali Loss: 0.3894346 Test Loss: 0.3894346
Validation loss decreased (0.391236 --> 0.389435).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.1574633121490479
Epoch: 9, Steps: 46 | Train Loss: 0.4250238 Vali Loss: 0.3883143 Test Loss: 0.3883143
Validation loss decreased (0.389435 --> 0.388314).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1549408435821533
Epoch: 10, Steps: 46 | Train Loss: 0.4214803 Vali Loss: 0.3871371 Test Loss: 0.3871371
Validation loss decreased (0.388314 --> 0.387137).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.039061978459358215, mae:0.15475152432918549, smape:15.399026870727539, dtw:not calculated
horizon:2 mse:0.208186075091362, mae:0.3854931890964508, smape:37.580251693725586, dtw:not calculated
horizon:3 mse:0.18019959330558777, mae:0.34062865376472473, smape:33.19776654243469, dtw:not calculated
horizon:4 mse:0.3763748109340668, mae:0.4971041977405548, smape:47.24757969379425, dtw:not calculated
horizon:5 mse:0.43942272663116455, mae:0.5551169514656067, smape:52.559083700180054, dtw:not calculated
horizon:6 mse:0.4560869634151459, mae:0.5543621778488159, smape:52.21363306045532, dtw:not calculated
horizon:7 mse:0.6491593718528748, mae:0.6803520321846008, smape:62.95351982116699, dtw:not calculated
horizon:8 mse:0.3648552894592285, mae:0.491026908159256, smape:46.6945618391037, dtw:not calculated
horizon:9 mse:0.38550931215286255, mae:0.507425844669342, smape:48.093751072883606, dtw:not calculated
horizon:10 mse:0.7373896837234497, mae:0.7141068577766418, smape:65.25198221206665, dtw:not calculated
horizon:11 mse:0.4336468279361725, mae:0.5392155051231384, smape:50.920069217681885, dtw:not calculated
horizon:12 mse:0.3757532835006714, mae:0.4968522787094116, smape:47.20185697078705, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2832220196723938, mae:0.41457611322402954, smape:39.699557423591614, dtw:not calculated
average metrics: horizon upto:12 mse:0.38713717460632324, mae:0.49303629994392395, smape:46.60942554473877, dtw:not calculated
===============================================================================
average of horizons: mse:0.38713717460632324, mae:0.49303629994392395, smape:46.60942554473877, dtw:not calculated
mean smape over horizons:  46.609423557917275
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=16, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.2981362342834473
Epoch: 1, Steps: 46 | Train Loss: 0.8965273 Vali Loss: 0.6572131 Test Loss: 0.6572131
Validation loss decreased (inf --> 0.657213).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1646525859832764
Epoch: 2, Steps: 46 | Train Loss: 0.5484639 Vali Loss: 0.4232216 Test Loss: 0.4232216
Validation loss decreased (0.657213 --> 0.423222).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1202125549316406
Epoch: 3, Steps: 46 | Train Loss: 0.4488838 Vali Loss: 0.3454878 Test Loss: 0.3454878
Validation loss decreased (0.423222 --> 0.345488).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.044818639755249
Epoch: 4, Steps: 46 | Train Loss: 0.4043957 Vali Loss: 0.3168198 Test Loss: 0.3168198
Validation loss decreased (0.345488 --> 0.316820).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.125960350036621
Epoch: 5, Steps: 46 | Train Loss: 0.3795714 Vali Loss: 0.3016872 Test Loss: 0.3016872
Validation loss decreased (0.316820 --> 0.301687).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0930893421173096
Epoch: 6, Steps: 46 | Train Loss: 0.3768352 Vali Loss: 0.2955414 Test Loss: 0.2955414
Validation loss decreased (0.301687 --> 0.295541).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0480051040649414
Epoch: 7, Steps: 46 | Train Loss: 0.3812753 Vali Loss: 0.2920565 Test Loss: 0.2920565
Validation loss decreased (0.295541 --> 0.292057).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.050323247909546
Epoch: 8, Steps: 46 | Train Loss: 0.3655520 Vali Loss: 0.2903862 Test Loss: 0.2903862
Validation loss decreased (0.292057 --> 0.290386).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0604653358459473
Epoch: 9, Steps: 46 | Train Loss: 0.3767852 Vali Loss: 0.2899691 Test Loss: 0.2899691
Validation loss decreased (0.290386 --> 0.289969).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9869117736816406
Epoch: 10, Steps: 46 | Train Loss: 0.3672960 Vali Loss: 0.2895576 Test Loss: 0.2895576
Validation loss decreased (0.289969 --> 0.289558).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05611397698521614, mae:0.1756943166255951, smape:17.420274019241333, dtw:not calculated
horizon:2 mse:0.16268223524093628, mae:0.34083911776542664, smape:33.393970131874084, dtw:not calculated
horizon:3 mse:0.19140280783176422, mae:0.36849480867385864, smape:35.96287965774536, dtw:not calculated
horizon:4 mse:0.18159900605678558, mae:0.34654688835144043, smape:33.794668316841125, dtw:not calculated
horizon:5 mse:0.28138473629951477, mae:0.45437973737716675, smape:43.89130771160126, dtw:not calculated
horizon:6 mse:0.25878286361694336, mae:0.42273083329200745, smape:40.829265117645264, dtw:not calculated
horizon:7 mse:0.40837791562080383, mae:0.5273352265357971, smape:49.986425042152405, dtw:not calculated
horizon:8 mse:0.28153449296951294, mae:0.4396606981754303, smape:42.363736033439636, dtw:not calculated
horizon:9 mse:0.33252543210983276, mae:0.4699176251888275, smape:44.904688000679016, dtw:not calculated
horizon:10 mse:0.4298964738845825, mae:0.550752580165863, smape:52.203136682510376, dtw:not calculated
horizon:11 mse:0.5139098763465881, mae:0.5961605310440063, smape:55.80563545227051, dtw:not calculated
horizon:12 mse:0.3764815330505371, mae:0.5201249718666077, smape:49.67609345912933, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.18866091966629028, mae:0.35144761204719543, smape:34.21539664268494, dtw:not calculated
average metrics: horizon upto:12 mse:0.2895576059818268, mae:0.4343864619731903, smape:41.686004400253296, dtw:not calculated
===============================================================================
average of horizons: mse:0.2895576059818268, mae:0.4343864619731903, smape:41.686004400253296, dtw:not calculated
mean smape over horizons:  41.686006635427475
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=16, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3476204872131348
Epoch: 1, Steps: 46 | Train Loss: 1.0962358 Vali Loss: 0.7777904 Test Loss: 0.7777904
Validation loss decreased (inf --> 0.777790).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9942193031311035
Epoch: 2, Steps: 46 | Train Loss: 0.6851564 Vali Loss: 0.5144908 Test Loss: 0.5144908
Validation loss decreased (0.777790 --> 0.514491).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9863686561584473
Epoch: 3, Steps: 46 | Train Loss: 0.5078500 Vali Loss: 0.4333141 Test Loss: 0.4333141
Validation loss decreased (0.514491 --> 0.433314).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0338459014892578
Epoch: 4, Steps: 46 | Train Loss: 0.4580926 Vali Loss: 0.3960523 Test Loss: 0.3960523
Validation loss decreased (0.433314 --> 0.396052).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0008649826049805
Epoch: 5, Steps: 46 | Train Loss: 0.4505562 Vali Loss: 0.3785233 Test Loss: 0.3785233
Validation loss decreased (0.396052 --> 0.378523).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1749448776245117
Epoch: 6, Steps: 46 | Train Loss: 0.4406851 Vali Loss: 0.3695980 Test Loss: 0.3695980
Validation loss decreased (0.378523 --> 0.369598).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0634894371032715
Epoch: 7, Steps: 46 | Train Loss: 0.4531500 Vali Loss: 0.3665722 Test Loss: 0.3665722
Validation loss decreased (0.369598 --> 0.366572).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0214333534240723
Epoch: 8, Steps: 46 | Train Loss: 0.4253743 Vali Loss: 0.3641617 Test Loss: 0.3641617
Validation loss decreased (0.366572 --> 0.364162).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0514042377471924
Epoch: 9, Steps: 46 | Train Loss: 0.4432639 Vali Loss: 0.3629012 Test Loss: 0.3629012
Validation loss decreased (0.364162 --> 0.362901).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0304341316223145
Epoch: 10, Steps: 46 | Train Loss: 0.4236487 Vali Loss: 0.3618374 Test Loss: 0.3618374
Validation loss decreased (0.362901 --> 0.361837).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.07475992292165756, mae:0.21804000437259674, smape:21.592582762241364, dtw:not calculated
horizon:2 mse:0.20125390589237213, mae:0.3804188072681427, smape:37.09131479263306, dtw:not calculated
horizon:3 mse:0.19519726932048798, mae:0.3721667230129242, smape:36.296769976615906, dtw:not calculated
horizon:4 mse:0.3071456551551819, mae:0.450662761926651, smape:43.244338035583496, dtw:not calculated
horizon:5 mse:0.4399462640285492, mae:0.5581222176551819, smape:52.83244252204895, dtw:not calculated
horizon:6 mse:0.28909561038017273, mae:0.4473850131034851, smape:43.075284361839294, dtw:not calculated
horizon:7 mse:0.7208218574523926, mae:0.7115718126296997, smape:65.21944403648376, dtw:not calculated
horizon:8 mse:0.3567321300506592, mae:0.5001751184463501, smape:47.81160354614258, dtw:not calculated
horizon:9 mse:0.33929017186164856, mae:0.4832465946674347, smape:46.20794057846069, dtw:not calculated
horizon:10 mse:0.5028461813926697, mae:0.5843668580055237, smape:54.72894906997681, dtw:not calculated
horizon:11 mse:0.5027351975440979, mae:0.5983288288116455, smape:56.22053146362305, dtw:not calculated
horizon:12 mse:0.4122246503829956, mae:0.5151686072349548, smape:48.64911139011383, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2512331008911133, mae:0.4044658839702606, smape:39.02212381362915, dtw:not calculated
average metrics: horizon upto:12 mse:0.3618374168872833, mae:0.48497116565704346, smape:46.080854535102844, dtw:not calculated
===============================================================================
average of horizons: mse:0.3618374168872833, mae:0.48497116565704346, smape:46.080854535102844, dtw:not calculated
mean smape over horizons:  46.08085937798023
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=16, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4267516136169434
Epoch: 1, Steps: 46 | Train Loss: 0.8134897 Vali Loss: 0.6577187 Test Loss: 0.6577187
Validation loss decreased (inf --> 0.657719).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0180835723876953
Epoch: 2, Steps: 46 | Train Loss: 0.5481821 Vali Loss: 0.4657362 Test Loss: 0.4657362
Validation loss decreased (0.657719 --> 0.465736).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0338335037231445
Epoch: 3, Steps: 46 | Train Loss: 0.4456088 Vali Loss: 0.3969778 Test Loss: 0.3969778
Validation loss decreased (0.465736 --> 0.396978).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9812922477722168
Epoch: 4, Steps: 46 | Train Loss: 0.4008818 Vali Loss: 0.3714117 Test Loss: 0.3714117
Validation loss decreased (0.396978 --> 0.371412).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0599963665008545
Epoch: 5, Steps: 46 | Train Loss: 0.4126964 Vali Loss: 0.3569262 Test Loss: 0.3569262
Validation loss decreased (0.371412 --> 0.356926).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0018694400787354
Epoch: 6, Steps: 46 | Train Loss: 0.4116473 Vali Loss: 0.3524194 Test Loss: 0.3524194
Validation loss decreased (0.356926 --> 0.352419).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.1210074424743652
Epoch: 7, Steps: 46 | Train Loss: 0.4153872 Vali Loss: 0.3466498 Test Loss: 0.3466498
Validation loss decreased (0.352419 --> 0.346650).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0658602714538574
Epoch: 8, Steps: 46 | Train Loss: 0.3892397 Vali Loss: 0.3459671 Test Loss: 0.3459671
Validation loss decreased (0.346650 --> 0.345967).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0293762683868408
Epoch: 9, Steps: 46 | Train Loss: 0.3816614 Vali Loss: 0.3453232 Test Loss: 0.3453232
Validation loss decreased (0.345967 --> 0.345323).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9894797801971436
Epoch: 10, Steps: 46 | Train Loss: 0.3863036 Vali Loss: 0.3460358 Test Loss: 0.3460358
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06394778192043304, mae:0.19701065123081207, smape:19.52241361141205, dtw:not calculated
horizon:2 mse:0.10952652245759964, mae:0.27209872007369995, smape:26.825392246246338, dtw:not calculated
horizon:3 mse:0.1568044275045395, mae:0.3067430257797241, smape:29.95416522026062, dtw:not calculated
horizon:4 mse:0.28673267364501953, mae:0.45063310861587524, smape:43.421417474746704, dtw:not calculated
horizon:5 mse:0.25015369057655334, mae:0.41027966141700745, smape:39.641591906547546, dtw:not calculated
horizon:6 mse:0.3304348587989807, mae:0.46949657797813416, smape:44.89484429359436, dtw:not calculated
horizon:7 mse:0.47070831060409546, mae:0.5872395634651184, smape:55.50132393836975, dtw:not calculated
horizon:8 mse:0.5327840447425842, mae:0.6068050861358643, smape:56.751471757888794, dtw:not calculated
horizon:9 mse:0.33027249574661255, mae:0.48427414894104004, smape:46.47703766822815, dtw:not calculated
horizon:10 mse:0.38483962416648865, mae:0.5164555907249451, smape:49.14103448390961, dtw:not calculated
horizon:11 mse:0.606784462928772, mae:0.6244769096374512, smape:57.487547397613525, dtw:not calculated
horizon:12 mse:0.6208896636962891, mae:0.6694480776786804, smape:62.213367223739624, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.1995999962091446, mae:0.35104361176490784, smape:34.04330313205719, dtw:not calculated
average metrics: horizon upto:12 mse:0.34532323479652405, mae:0.46624672412872314, smape:44.31930184364319, dtw:not calculated
===============================================================================
average of horizons: mse:0.34532323479652405, mae:0.46624672412872314, smape:44.31930184364319, dtw:not calculated
mean smape over horizons:  44.31930060187975
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=16, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3772954940795898
Epoch: 1, Steps: 46 | Train Loss: 0.7864735 Vali Loss: 0.6732199 Test Loss: 0.6732199
Validation loss decreased (inf --> 0.673220).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9549071788787842
Epoch: 2, Steps: 46 | Train Loss: 0.5158927 Vali Loss: 0.4748518 Test Loss: 0.4748518
Validation loss decreased (0.673220 --> 0.474852).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0730092525482178
Epoch: 3, Steps: 46 | Train Loss: 0.4346766 Vali Loss: 0.4041723 Test Loss: 0.4041723
Validation loss decreased (0.474852 --> 0.404172).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0438523292541504
Epoch: 4, Steps: 46 | Train Loss: 0.3895018 Vali Loss: 0.3783782 Test Loss: 0.3783782
Validation loss decreased (0.404172 --> 0.378378).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9610259532928467
Epoch: 5, Steps: 46 | Train Loss: 0.3944095 Vali Loss: 0.3651246 Test Loss: 0.3651246
Validation loss decreased (0.378378 --> 0.365125).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0413663387298584
Epoch: 6, Steps: 46 | Train Loss: 0.3986978 Vali Loss: 0.3596113 Test Loss: 0.3596113
Validation loss decreased (0.365125 --> 0.359611).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0868005752563477
Epoch: 7, Steps: 46 | Train Loss: 0.4055161 Vali Loss: 0.3546850 Test Loss: 0.3546850
Validation loss decreased (0.359611 --> 0.354685).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0578112602233887
Epoch: 8, Steps: 46 | Train Loss: 0.3828821 Vali Loss: 0.3528768 Test Loss: 0.3528768
Validation loss decreased (0.354685 --> 0.352877).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0528895854949951
Epoch: 9, Steps: 46 | Train Loss: 0.3737949 Vali Loss: 0.3510550 Test Loss: 0.3510550
Validation loss decreased (0.352877 --> 0.351055).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0060763359069824
Epoch: 10, Steps: 46 | Train Loss: 0.3781760 Vali Loss: 0.3527544 Test Loss: 0.3527544
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04306454211473465, mae:0.15723657608032227, smape:15.629871189594269, dtw:not calculated
horizon:2 mse:0.09782464057207108, mae:0.25091230869293213, smape:24.7565358877182, dtw:not calculated
horizon:3 mse:0.15028609335422516, mae:0.30357950925827026, smape:29.67148721218109, dtw:not calculated
horizon:4 mse:0.24746230244636536, mae:0.41521012783050537, smape:40.190619230270386, dtw:not calculated
horizon:5 mse:0.23627425730228424, mae:0.40165382623672485, smape:38.897186517715454, dtw:not calculated
horizon:6 mse:0.35483625531196594, mae:0.49201419949531555, smape:46.937671303749084, dtw:not calculated
horizon:7 mse:0.45141974091529846, mae:0.5594249963760376, smape:52.80648469924927, dtw:not calculated
horizon:8 mse:0.4633285105228424, mae:0.5728011727333069, smape:54.08588647842407, dtw:not calculated
horizon:9 mse:0.36249977350234985, mae:0.5107617378234863, smape:48.847681283950806, dtw:not calculated
horizon:10 mse:0.41112178564071655, mae:0.5358966588973999, smape:50.81642270088196, dtw:not calculated
horizon:11 mse:0.7056635618209839, mae:0.6717843413352966, smape:61.13303303718567, dtw:not calculated
horizon:12 mse:0.6888781189918518, mae:0.7107374668121338, smape:65.68083763122559, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.18829134106636047, mae:0.3367677628993988, smape:32.680559158325195, dtw:not calculated
average metrics: horizon upto:12 mse:0.35105496644973755, mae:0.46516770124435425, smape:44.121143221855164, dtw:not calculated
===============================================================================
average of horizons: mse:0.35105496644973755, mae:0.46516770124435425, smape:44.121143221855164, dtw:not calculated
mean smape over horizons:  44.12114309767882
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=16, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3811402320861816
Epoch: 1, Steps: 46 | Train Loss: 0.9555299 Vali Loss: 0.7817250 Test Loss: 0.7817250
Validation loss decreased (inf --> 0.781725).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9134547710418701
Epoch: 2, Steps: 46 | Train Loss: 0.6405464 Vali Loss: 0.6037903 Test Loss: 0.6037903
Validation loss decreased (0.781725 --> 0.603790).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9590630531311035
Epoch: 3, Steps: 46 | Train Loss: 0.5302995 Vali Loss: 0.5308430 Test Loss: 0.5308430
Validation loss decreased (0.603790 --> 0.530843).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9519872665405273
Epoch: 4, Steps: 46 | Train Loss: 0.4962962 Vali Loss: 0.4967600 Test Loss: 0.4967600
Validation loss decreased (0.530843 --> 0.496760).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0068962574005127
Epoch: 5, Steps: 46 | Train Loss: 0.4653581 Vali Loss: 0.4819495 Test Loss: 0.4819495
Validation loss decreased (0.496760 --> 0.481950).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9301669597625732
Epoch: 6, Steps: 46 | Train Loss: 0.4763119 Vali Loss: 0.4726521 Test Loss: 0.4726521
Validation loss decreased (0.481950 --> 0.472652).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9641416072845459
Epoch: 7, Steps: 46 | Train Loss: 0.4655329 Vali Loss: 0.4695539 Test Loss: 0.4695539
Validation loss decreased (0.472652 --> 0.469554).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0188887119293213
Epoch: 8, Steps: 46 | Train Loss: 0.4489948 Vali Loss: 0.4660535 Test Loss: 0.4660535
Validation loss decreased (0.469554 --> 0.466053).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0219862461090088
Epoch: 9, Steps: 46 | Train Loss: 0.4625063 Vali Loss: 0.4656551 Test Loss: 0.4656551
Validation loss decreased (0.466053 --> 0.465655).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9709951877593994
Epoch: 10, Steps: 46 | Train Loss: 0.4643286 Vali Loss: 0.4675057 Test Loss: 0.4675057
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.057230520993471146, mae:0.19526797533035278, smape:19.399575889110565, dtw:not calculated
horizon:2 mse:0.3471604585647583, mae:0.5000812411308289, smape:47.93257713317871, dtw:not calculated
horizon:3 mse:0.20098388195037842, mae:0.3575236201286316, smape:34.737154841423035, dtw:not calculated
horizon:4 mse:0.3032386004924774, mae:0.4600106477737427, smape:44.239094853401184, dtw:not calculated
horizon:5 mse:0.47218775749206543, mae:0.5680582523345947, smape:53.494977951049805, dtw:not calculated
horizon:6 mse:0.5463131070137024, mae:0.620574951171875, smape:58.00068974494934, dtw:not calculated
horizon:7 mse:0.4006718695163727, mae:0.4977840483188629, smape:46.878957748413086, dtw:not calculated
horizon:8 mse:0.3270808756351471, mae:0.47496578097343445, smape:45.531538128852844, dtw:not calculated
horizon:9 mse:0.918343186378479, mae:0.8013973236083984, smape:71.88555598258972, dtw:not calculated
horizon:10 mse:0.9982272982597351, mae:0.843360424041748, smape:75.21220445632935, dtw:not calculated
horizon:11 mse:0.5591306686401367, mae:0.6198088526725769, smape:57.6770544052124, dtw:not calculated
horizon:12 mse:0.45729315280914307, mae:0.5676109194755554, smape:53.55798006057739, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3211857080459595, mae:0.4502527713775635, smape:42.9673433303833, dtw:not calculated
average metrics: horizon upto:12 mse:0.4656551480293274, mae:0.5422036051750183, smape:50.71227550506592, dtw:not calculated
===============================================================================
average of horizons: mse:0.4656551480293274, mae:0.5422036051750183, smape:50.71227550506592, dtw:not calculated
mean smape over horizons:  50.71228009959062
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=16, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.468097448348999
Epoch: 1, Steps: 46 | Train Loss: 0.9447876 Vali Loss: 0.7493234 Test Loss: 0.7493234
Validation loss decreased (inf --> 0.749323).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9530186653137207
Epoch: 2, Steps: 46 | Train Loss: 0.6319056 Vali Loss: 0.5737558 Test Loss: 0.5737558
Validation loss decreased (0.749323 --> 0.573756).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9934005737304688
Epoch: 3, Steps: 46 | Train Loss: 0.5191967 Vali Loss: 0.5029835 Test Loss: 0.5029835
Validation loss decreased (0.573756 --> 0.502983).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0018599033355713
Epoch: 4, Steps: 46 | Train Loss: 0.4832060 Vali Loss: 0.4716182 Test Loss: 0.4716182
Validation loss decreased (0.502983 --> 0.471618).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0513920783996582
Epoch: 5, Steps: 46 | Train Loss: 0.4590647 Vali Loss: 0.4554983 Test Loss: 0.4554983
Validation loss decreased (0.471618 --> 0.455498).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.082737922668457
Epoch: 6, Steps: 46 | Train Loss: 0.4720494 Vali Loss: 0.4480509 Test Loss: 0.4480509
Validation loss decreased (0.455498 --> 0.448051).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.01173996925354
Epoch: 7, Steps: 46 | Train Loss: 0.4550410 Vali Loss: 0.4467160 Test Loss: 0.4467160
Validation loss decreased (0.448051 --> 0.446716).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.972245454788208
Epoch: 8, Steps: 46 | Train Loss: 0.4417325 Vali Loss: 0.4422411 Test Loss: 0.4422411
Validation loss decreased (0.446716 --> 0.442241).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9923043251037598
Epoch: 9, Steps: 46 | Train Loss: 0.4493709 Vali Loss: 0.4414770 Test Loss: 0.4414770
Validation loss decreased (0.442241 --> 0.441477).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9505159854888916
Epoch: 10, Steps: 46 | Train Loss: 0.4512942 Vali Loss: 0.4432457 Test Loss: 0.4432457
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06576801091432571, mae:0.21125271916389465, smape:20.966263115406036, dtw:not calculated
horizon:2 mse:0.27679988741874695, mae:0.45144182443618774, smape:43.66343915462494, dtw:not calculated
horizon:3 mse:0.24678948521614075, mae:0.3833714723587036, smape:36.86950206756592, dtw:not calculated
horizon:4 mse:0.28377142548561096, mae:0.4471033811569214, smape:43.13003122806549, dtw:not calculated
horizon:5 mse:0.42350926995277405, mae:0.5424279570579529, smape:51.39769911766052, dtw:not calculated
horizon:6 mse:0.5088621973991394, mae:0.5926916599273682, smape:55.53259253501892, dtw:not calculated
horizon:7 mse:0.37022864818573, mae:0.47424378991127014, smape:44.81980800628662, dtw:not calculated
horizon:8 mse:0.28502508997917175, mae:0.44643065333366394, smape:43.01874339580536, dtw:not calculated
horizon:9 mse:0.8457674980163574, mae:0.7658967971801758, smape:69.2075788974762, dtw:not calculated
horizon:10 mse:0.9565893411636353, mae:0.8477368950843811, smape:76.39498114585876, dtw:not calculated
horizon:11 mse:0.5559566020965576, mae:0.624039888381958, smape:58.18798542022705, dtw:not calculated
horizon:12 mse:0.4786565601825714, mae:0.5697395205497742, smape:53.45674157142639, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.30091673135757446, mae:0.43804818391799927, smape:41.926586627960205, dtw:not calculated
average metrics: horizon upto:12 mse:0.4414770007133484, mae:0.5296980738639832, smape:49.7204452753067, dtw:not calculated
===============================================================================
average of horizons: mse:0.4414770007133484, mae:0.5296980738639832, smape:49.7204452753067, dtw:not calculated
mean smape over horizons:  49.72044713795185
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=16, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4536924362182617
Epoch: 1, Steps: 46 | Train Loss: 0.8962078 Vali Loss: 0.6946793 Test Loss: 0.6946793
Validation loss decreased (inf --> 0.694679).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0380661487579346
Epoch: 2, Steps: 46 | Train Loss: 0.5804052 Vali Loss: 0.5221931 Test Loss: 0.5221931
Validation loss decreased (0.694679 --> 0.522193).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9822964668273926
Epoch: 3, Steps: 46 | Train Loss: 0.4912804 Vali Loss: 0.4516809 Test Loss: 0.4516809
Validation loss decreased (0.522193 --> 0.451681).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0457148551940918
Epoch: 4, Steps: 46 | Train Loss: 0.4640277 Vali Loss: 0.4229563 Test Loss: 0.4229563
Validation loss decreased (0.451681 --> 0.422956).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0826423168182373
Epoch: 5, Steps: 46 | Train Loss: 0.4401383 Vali Loss: 0.4072744 Test Loss: 0.4072744
Validation loss decreased (0.422956 --> 0.407274).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1086013317108154
Epoch: 6, Steps: 46 | Train Loss: 0.4541215 Vali Loss: 0.4001630 Test Loss: 0.4001630
Validation loss decreased (0.407274 --> 0.400163).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9983372688293457
Epoch: 7, Steps: 46 | Train Loss: 0.4360146 Vali Loss: 0.3995345 Test Loss: 0.3995345
Validation loss decreased (0.400163 --> 0.399534).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0161662101745605
Epoch: 8, Steps: 46 | Train Loss: 0.4213575 Vali Loss: 0.3947584 Test Loss: 0.3947584
Validation loss decreased (0.399534 --> 0.394758).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.123246669769287
Epoch: 9, Steps: 46 | Train Loss: 0.4306545 Vali Loss: 0.3933682 Test Loss: 0.3933682
Validation loss decreased (0.394758 --> 0.393368).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1274902820587158
Epoch: 10, Steps: 46 | Train Loss: 0.4290808 Vali Loss: 0.3967178 Test Loss: 0.3967178
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0625651404261589, mae:0.19950465857982635, smape:19.79364901781082, dtw:not calculated
horizon:2 mse:0.172238290309906, mae:0.3509937524795532, smape:34.35715734958649, dtw:not calculated
horizon:3 mse:0.22716140747070312, mae:0.36012259125709534, smape:34.67317223548889, dtw:not calculated
horizon:4 mse:0.28088000416755676, mae:0.4497317671775818, smape:43.423980474472046, dtw:not calculated
horizon:5 mse:0.35111501812934875, mae:0.4940376579761505, smape:47.1924751996994, dtw:not calculated
horizon:6 mse:0.45210954546928406, mae:0.5532177090644836, smape:52.12951302528381, dtw:not calculated
horizon:7 mse:0.37943920493125916, mae:0.48610129952430725, smape:45.95758616924286, dtw:not calculated
horizon:8 mse:0.2639928460121155, mae:0.4309026598930359, smape:41.62060618400574, dtw:not calculated
horizon:9 mse:0.8157739043235779, mae:0.7581225633621216, smape:68.82750391960144, dtw:not calculated
horizon:10 mse:0.7228111028671265, mae:0.7288001775741577, smape:67.10689067840576, dtw:not calculated
horizon:11 mse:0.5055127739906311, mae:0.5948034524917603, smape:55.752938985824585, dtw:not calculated
horizon:12 mse:0.4868195056915283, mae:0.5708375573158264, smape:53.46275568008423, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.25767824053764343, mae:0.40126803517341614, smape:38.59499394893646, dtw:not calculated
average metrics: horizon upto:12 mse:0.3933682143688202, mae:0.4980979561805725, smape:47.02484905719757, dtw:not calculated
===============================================================================
average of horizons: mse:0.3933682143688202, mae:0.4980979561805725, smape:47.02484905719757, dtw:not calculated
mean smape over horizons:  47.02485240995884
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5107653141021729
Epoch: 1, Steps: 46 | Train Loss: 0.8449842 Vali Loss: 0.7357032 Test Loss: 0.7357032
Validation loss decreased (inf --> 0.735703).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0781733989715576
Epoch: 2, Steps: 46 | Train Loss: 0.5587401 Vali Loss: 0.4724373 Test Loss: 0.4724373
Validation loss decreased (0.735703 --> 0.472437).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1175386905670166
Epoch: 3, Steps: 46 | Train Loss: 0.4592905 Vali Loss: 0.3836326 Test Loss: 0.3836326
Validation loss decreased (0.472437 --> 0.383633).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1559009552001953
Epoch: 4, Steps: 46 | Train Loss: 0.4281449 Vali Loss: 0.3511481 Test Loss: 0.3511481
Validation loss decreased (0.383633 --> 0.351148).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0490262508392334
Epoch: 5, Steps: 46 | Train Loss: 0.4188596 Vali Loss: 0.3342960 Test Loss: 0.3342960
Validation loss decreased (0.351148 --> 0.334296).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.001344919204712
Epoch: 6, Steps: 46 | Train Loss: 0.4112144 Vali Loss: 0.3292367 Test Loss: 0.3292367
Validation loss decreased (0.334296 --> 0.329237).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.1035027503967285
Epoch: 7, Steps: 46 | Train Loss: 0.4007957 Vali Loss: 0.3256575 Test Loss: 0.3256575
Validation loss decreased (0.329237 --> 0.325658).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9684996604919434
Epoch: 8, Steps: 46 | Train Loss: 0.4038002 Vali Loss: 0.3210355 Test Loss: 0.3210355
Validation loss decreased (0.325658 --> 0.321035).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.003359079360962
Epoch: 9, Steps: 46 | Train Loss: 0.4002761 Vali Loss: 0.3216150 Test Loss: 0.3216150
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.005537986755371
Epoch: 10, Steps: 46 | Train Loss: 0.3952150 Vali Loss: 0.3212814 Test Loss: 0.3212814
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.10753767937421799, mae:0.25172725319862366, smape:24.745790660381317, dtw:not calculated
horizon:2 mse:0.3997456431388855, mae:0.539420485496521, smape:51.39949917793274, dtw:not calculated
horizon:3 mse:0.214773491024971, mae:0.3727286159992218, smape:36.151328682899475, dtw:not calculated
horizon:4 mse:0.22942745685577393, mae:0.3966100811958313, smape:38.452500104904175, dtw:not calculated
horizon:5 mse:0.2608696520328522, mae:0.4265659749507904, smape:41.20805263519287, dtw:not calculated
horizon:6 mse:0.3077796399593353, mae:0.43576833605766296, smape:41.581135988235474, dtw:not calculated
horizon:7 mse:0.36834314465522766, mae:0.4957898259162903, smape:47.13809788227081, dtw:not calculated
horizon:8 mse:0.47651052474975586, mae:0.5881773233413696, smape:55.504006147384644, dtw:not calculated
horizon:9 mse:0.4941738247871399, mae:0.5811448097229004, smape:54.52805161476135, dtw:not calculated
horizon:10 mse:0.40593454241752625, mae:0.5205899477005005, smape:49.312615394592285, dtw:not calculated
horizon:11 mse:0.26836976408958435, mae:0.4385748505592346, smape:42.411041259765625, dtw:not calculated
horizon:12 mse:0.3189603388309479, mae:0.4682888388633728, smape:44.92848217487335, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.25335559248924255, mae:0.40380343794822693, smape:38.923051953315735, dtw:not calculated
average metrics: horizon upto:12 mse:0.3210354745388031, mae:0.4596155285835266, smape:43.94671618938446, dtw:not calculated
===============================================================================
average of horizons: mse:0.3210354745388031, mae:0.4596155285835266, smape:43.94671618938446, dtw:not calculated
mean smape over horizons:  43.946716810266174
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3990962505340576
Epoch: 1, Steps: 46 | Train Loss: 0.7990101 Vali Loss: 0.6884801 Test Loss: 0.6884801
Validation loss decreased (inf --> 0.688480).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0542168617248535
Epoch: 2, Steps: 46 | Train Loss: 0.5132537 Vali Loss: 0.4697244 Test Loss: 0.4697244
Validation loss decreased (0.688480 --> 0.469724).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.001347303390503
Epoch: 3, Steps: 46 | Train Loss: 0.4439705 Vali Loss: 0.3917899 Test Loss: 0.3917899
Validation loss decreased (0.469724 --> 0.391790).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0528295040130615
Epoch: 4, Steps: 46 | Train Loss: 0.4238991 Vali Loss: 0.3595482 Test Loss: 0.3595482
Validation loss decreased (0.391790 --> 0.359548).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0223722457885742
Epoch: 5, Steps: 46 | Train Loss: 0.4081277 Vali Loss: 0.3410909 Test Loss: 0.3410909
Validation loss decreased (0.359548 --> 0.341091).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0229556560516357
Epoch: 6, Steps: 46 | Train Loss: 0.4017027 Vali Loss: 0.3348095 Test Loss: 0.3348095
Validation loss decreased (0.341091 --> 0.334809).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9687583446502686
Epoch: 7, Steps: 46 | Train Loss: 0.3921569 Vali Loss: 0.3336279 Test Loss: 0.3336279
Validation loss decreased (0.334809 --> 0.333628).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9635982513427734
Epoch: 8, Steps: 46 | Train Loss: 0.3937210 Vali Loss: 0.3327094 Test Loss: 0.3327094
Validation loss decreased (0.333628 --> 0.332709).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.118415355682373
Epoch: 9, Steps: 46 | Train Loss: 0.3987323 Vali Loss: 0.3302951 Test Loss: 0.3302951
Validation loss decreased (0.332709 --> 0.330295).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9751083850860596
Epoch: 10, Steps: 46 | Train Loss: 0.3928852 Vali Loss: 0.3302791 Test Loss: 0.3302791
Validation loss decreased (0.330295 --> 0.330279).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.08705851435661316, mae:0.2359992265701294, smape:23.31669181585312, dtw:not calculated
horizon:2 mse:0.3450292944908142, mae:0.49637240171432495, smape:47.51713573932648, dtw:not calculated
horizon:3 mse:0.1965242475271225, mae:0.37113872170448303, smape:36.175477504730225, dtw:not calculated
horizon:4 mse:0.18616020679473877, mae:0.34125691652297974, smape:33.170002698898315, dtw:not calculated
horizon:5 mse:0.34108617901802063, mae:0.4862297773361206, smape:46.50049805641174, dtw:not calculated
horizon:6 mse:0.28800293803215027, mae:0.44681641459465027, smape:43.002453446388245, dtw:not calculated
horizon:7 mse:0.3229302763938904, mae:0.4785782992839813, smape:45.93196213245392, dtw:not calculated
horizon:8 mse:0.4444393217563629, mae:0.5570316314697266, smape:52.639394998550415, dtw:not calculated
horizon:9 mse:0.31393709778785706, mae:0.46970099210739136, smape:45.11435031890869, dtw:not calculated
horizon:10 mse:0.4113919734954834, mae:0.5466552972793579, smape:52.00706124305725, dtw:not calculated
horizon:11 mse:0.41534993052482605, mae:0.5231065154075623, smape:49.456945061683655, dtw:not calculated
horizon:12 mse:0.6114389896392822, mae:0.6412307620048523, smape:59.240883588790894, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.24064356088638306, mae:0.3963022232055664, smape:38.2803738117218, dtw:not calculated
average metrics: horizon upto:12 mse:0.33027908205986023, mae:0.4661763906478882, smape:44.506070017814636, dtw:not calculated
===============================================================================
average of horizons: mse:0.33027908205986023, mae:0.4661763906478882, smape:44.506070017814636, dtw:not calculated
mean smape over horizons:  44.50607138375441
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4794549942016602
Epoch: 1, Steps: 46 | Train Loss: 0.8530004 Vali Loss: 0.7821196 Test Loss: 0.7821196
Validation loss decreased (inf --> 0.782120).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.059682846069336
Epoch: 2, Steps: 46 | Train Loss: 0.5812912 Vali Loss: 0.5652688 Test Loss: 0.5652688
Validation loss decreased (0.782120 --> 0.565269).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0766699314117432
Epoch: 3, Steps: 46 | Train Loss: 0.4957238 Vali Loss: 0.4830340 Test Loss: 0.4830340
Validation loss decreased (0.565269 --> 0.483034).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9943256378173828
Epoch: 4, Steps: 46 | Train Loss: 0.4516965 Vali Loss: 0.4514371 Test Loss: 0.4514371
Validation loss decreased (0.483034 --> 0.451437).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9905679225921631
Epoch: 5, Steps: 46 | Train Loss: 0.4538714 Vali Loss: 0.4324414 Test Loss: 0.4324414
Validation loss decreased (0.451437 --> 0.432441).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0438203811645508
Epoch: 6, Steps: 46 | Train Loss: 0.4454936 Vali Loss: 0.4243164 Test Loss: 0.4243164
Validation loss decreased (0.432441 --> 0.424316).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0059640407562256
Epoch: 7, Steps: 46 | Train Loss: 0.4444786 Vali Loss: 0.4192764 Test Loss: 0.4192764
Validation loss decreased (0.424316 --> 0.419276).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0331346988677979
Epoch: 8, Steps: 46 | Train Loss: 0.4242925 Vali Loss: 0.4176856 Test Loss: 0.4176856
Validation loss decreased (0.419276 --> 0.417686).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0386700630187988
Epoch: 9, Steps: 46 | Train Loss: 0.4300821 Vali Loss: 0.4171443 Test Loss: 0.4171443
Validation loss decreased (0.417686 --> 0.417144).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0177927017211914
Epoch: 10, Steps: 46 | Train Loss: 0.4335856 Vali Loss: 0.4171584 Test Loss: 0.4171584
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05612422525882721, mae:0.18059732019901276, smape:17.90947914123535, dtw:not calculated
horizon:2 mse:0.20696686208248138, mae:0.3862147033214569, smape:37.62477934360504, dtw:not calculated
horizon:3 mse:0.21338091790676117, mae:0.3895471692085266, smape:37.93109357357025, dtw:not calculated
horizon:4 mse:0.39277708530426025, mae:0.5355545878410339, smape:51.092445850372314, dtw:not calculated
horizon:5 mse:0.3030458688735962, mae:0.44106796383857727, smape:42.26791858673096, dtw:not calculated
horizon:6 mse:0.5170029997825623, mae:0.6163036823272705, smape:57.99090266227722, dtw:not calculated
horizon:7 mse:0.6560682058334351, mae:0.6847794651985168, smape:63.279348611831665, dtw:not calculated
horizon:8 mse:0.6066532135009766, mae:0.6572813391685486, smape:61.08548641204834, dtw:not calculated
horizon:9 mse:0.5105265378952026, mae:0.563865602016449, smape:52.47231125831604, dtw:not calculated
horizon:10 mse:0.7550951838493347, mae:0.7052449584007263, smape:63.96705508232117, dtw:not calculated
horizon:11 mse:0.35169413685798645, mae:0.481253981590271, smape:45.84936499595642, dtw:not calculated
horizon:12 mse:0.43639692664146423, mae:0.5438855290412903, smape:51.33436322212219, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2815496325492859, mae:0.42488089203834534, smape:40.802767872810364, dtw:not calculated
average metrics: horizon upto:12 mse:0.4171443283557892, mae:0.5154663920402527, smape:48.567041754722595, dtw:not calculated
===============================================================================
average of horizons: mse:0.4171443283557892, mae:0.5154663920402527, smape:48.567041754722595, dtw:not calculated
mean smape over horizons:  48.56704572836558
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4166784286499023
Epoch: 1, Steps: 46 | Train Loss: 0.8716785 Vali Loss: 0.7576789 Test Loss: 0.7576789
Validation loss decreased (inf --> 0.757679).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0360901355743408
Epoch: 2, Steps: 46 | Train Loss: 0.5767133 Vali Loss: 0.5683017 Test Loss: 0.5683017
Validation loss decreased (0.757679 --> 0.568302).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0823023319244385
Epoch: 3, Steps: 46 | Train Loss: 0.4939794 Vali Loss: 0.4899877 Test Loss: 0.4899877
Validation loss decreased (0.568302 --> 0.489988).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.094940185546875
Epoch: 4, Steps: 46 | Train Loss: 0.4665958 Vali Loss: 0.4560374 Test Loss: 0.4560374
Validation loss decreased (0.489988 --> 0.456037).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0571787357330322
Epoch: 5, Steps: 46 | Train Loss: 0.4614915 Vali Loss: 0.4391654 Test Loss: 0.4391654
Validation loss decreased (0.456037 --> 0.439165).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1263575553894043
Epoch: 6, Steps: 46 | Train Loss: 0.4508269 Vali Loss: 0.4299680 Test Loss: 0.4299680
Validation loss decreased (0.439165 --> 0.429968).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.069310188293457
Epoch: 7, Steps: 46 | Train Loss: 0.4595679 Vali Loss: 0.4262080 Test Loss: 0.4262080
Validation loss decreased (0.429968 --> 0.426208).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0095925331115723
Epoch: 8, Steps: 46 | Train Loss: 0.4414491 Vali Loss: 0.4242750 Test Loss: 0.4242750
Validation loss decreased (0.426208 --> 0.424275).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.07820725440979
Epoch: 9, Steps: 46 | Train Loss: 0.4507251 Vali Loss: 0.4251252 Test Loss: 0.4251252
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.128321886062622
Epoch: 10, Steps: 46 | Train Loss: 0.4510759 Vali Loss: 0.4232484 Test Loss: 0.4232484
Validation loss decreased (0.424275 --> 0.423248).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05980437621474266, mae:0.18960131704807281, smape:18.797658383846283, dtw:not calculated
horizon:2 mse:0.2674693763256073, mae:0.43930885195732117, smape:42.53600537776947, dtw:not calculated
horizon:3 mse:0.16329006850719452, mae:0.3336670994758606, smape:32.634928822517395, dtw:not calculated
horizon:4 mse:0.47969692945480347, mae:0.5940724611282349, smape:56.13734722137451, dtw:not calculated
horizon:5 mse:0.33657410740852356, mae:0.4836404323577881, smape:46.31030857563019, dtw:not calculated
horizon:6 mse:0.43363744020462036, mae:0.5529834628105164, smape:52.41246223449707, dtw:not calculated
horizon:7 mse:0.661058783531189, mae:0.681428849697113, smape:62.806206941604614, dtw:not calculated
horizon:8 mse:0.41628286242485046, mae:0.5647634267807007, smape:53.87325882911682, dtw:not calculated
horizon:9 mse:0.6162751317024231, mae:0.6544911861419678, smape:60.62026619911194, dtw:not calculated
horizon:10 mse:0.7099394798278809, mae:0.707979142665863, smape:64.97296094894409, dtw:not calculated
horizon:11 mse:0.40723344683647156, mae:0.4944831430912018, smape:46.47566378116608, dtw:not calculated
horizon:12 mse:0.5277190208435059, mae:0.6035071015357971, smape:56.449174880981445, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.290078729391098, mae:0.43221229314804077, smape:41.4714515209198, dtw:not calculated
average metrics: horizon upto:12 mse:0.42324841022491455, mae:0.524993896484375, smape:49.502187967300415, dtw:not calculated
===============================================================================
average of horizons: mse:0.42324841022491455, mae:0.524993896484375, smape:49.502187967300415, dtw:not calculated
mean smape over horizons:  49.502186849713326
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3647642135620117
Epoch: 1, Steps: 46 | Train Loss: 0.8280720 Vali Loss: 0.7626865 Test Loss: 0.7626865
Validation loss decreased (inf --> 0.762686).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0157387256622314
Epoch: 2, Steps: 46 | Train Loss: 0.5559026 Vali Loss: 0.5766222 Test Loss: 0.5766222
Validation loss decreased (0.762686 --> 0.576622).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0379512310028076
Epoch: 3, Steps: 46 | Train Loss: 0.4779095 Vali Loss: 0.4994916 Test Loss: 0.4994916
Validation loss decreased (0.576622 --> 0.499492).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0143532752990723
Epoch: 4, Steps: 46 | Train Loss: 0.4507378 Vali Loss: 0.4654472 Test Loss: 0.4654472
Validation loss decreased (0.499492 --> 0.465447).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.1195042133331299
Epoch: 5, Steps: 46 | Train Loss: 0.4484323 Vali Loss: 0.4486445 Test Loss: 0.4486445
Validation loss decreased (0.465447 --> 0.448644).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9698145389556885
Epoch: 6, Steps: 46 | Train Loss: 0.4425019 Vali Loss: 0.4406827 Test Loss: 0.4406827
Validation loss decreased (0.448644 --> 0.440683).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.925105094909668
Epoch: 7, Steps: 46 | Train Loss: 0.4435761 Vali Loss: 0.4382395 Test Loss: 0.4382395
Validation loss decreased (0.440683 --> 0.438239).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0495336055755615
Epoch: 8, Steps: 46 | Train Loss: 0.4267265 Vali Loss: 0.4360805 Test Loss: 0.4360805
Validation loss decreased (0.438239 --> 0.436080).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.980644702911377
Epoch: 9, Steps: 46 | Train Loss: 0.4395191 Vali Loss: 0.4337248 Test Loss: 0.4337248
Validation loss decreased (0.436080 --> 0.433725).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0596272945404053
Epoch: 10, Steps: 46 | Train Loss: 0.4411462 Vali Loss: 0.4327077 Test Loss: 0.4327077
Validation loss decreased (0.433725 --> 0.432708).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03938061743974686, mae:0.15947604179382324, smape:15.882007777690887, dtw:not calculated
horizon:2 mse:0.24746941030025482, mae:0.4276752173900604, smape:41.55309498310089, dtw:not calculated
horizon:3 mse:0.14216282963752747, mae:0.3085356056690216, smape:30.268779397010803, dtw:not calculated
horizon:4 mse:0.40817704796791077, mae:0.550621747970581, smape:52.46102213859558, dtw:not calculated
horizon:5 mse:0.31472519040107727, mae:0.4722414016723633, smape:45.38881480693817, dtw:not calculated
horizon:6 mse:0.4659714698791504, mae:0.5689235925674438, smape:53.66207957267761, dtw:not calculated
horizon:7 mse:0.7560415863990784, mae:0.7318943738937378, smape:66.8239951133728, dtw:not calculated
horizon:8 mse:0.46517065167427063, mae:0.5999398827552795, smape:56.93855881690979, dtw:not calculated
horizon:9 mse:0.6819784641265869, mae:0.6906975507736206, smape:63.54469656944275, dtw:not calculated
horizon:10 mse:0.6189300417900085, mae:0.6619583964347839, smape:61.4124059677124, dtw:not calculated
horizon:11 mse:0.46123260259628296, mae:0.5237067341804504, smape:48.80675971508026, dtw:not calculated
horizon:12 mse:0.5912528038024902, mae:0.6407825946807861, smape:59.52795743942261, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2696477770805359, mae:0.41457894444465637, smape:39.86929953098297, dtw:not calculated
average metrics: horizon upto:12 mse:0.4327077269554138, mae:0.5280377864837646, smape:49.68917965888977, dtw:not calculated
===============================================================================
average of horizons: mse:0.4327077269554138, mae:0.5280377864837646, smape:49.68917965888977, dtw:not calculated
mean smape over horizons:  49.689181024829544
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.401789665222168
Epoch: 1, Steps: 46 | Train Loss: 0.7920180 Vali Loss: 0.7631068 Test Loss: 0.7631068
Validation loss decreased (inf --> 0.763107).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1121070384979248
Epoch: 2, Steps: 46 | Train Loss: 0.5378712 Vali Loss: 0.5698199 Test Loss: 0.5698199
Validation loss decreased (0.763107 --> 0.569820).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0514211654663086
Epoch: 3, Steps: 46 | Train Loss: 0.4670910 Vali Loss: 0.4937885 Test Loss: 0.4937885
Validation loss decreased (0.569820 --> 0.493789).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.054328441619873
Epoch: 4, Steps: 46 | Train Loss: 0.4428150 Vali Loss: 0.4591956 Test Loss: 0.4591956
Validation loss decreased (0.493789 --> 0.459196).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0656945705413818
Epoch: 5, Steps: 46 | Train Loss: 0.4438830 Vali Loss: 0.4432250 Test Loss: 0.4432250
Validation loss decreased (0.459196 --> 0.443225).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0153100490570068
Epoch: 6, Steps: 46 | Train Loss: 0.4422811 Vali Loss: 0.4344240 Test Loss: 0.4344240
Validation loss decreased (0.443225 --> 0.434424).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9532482624053955
Epoch: 7, Steps: 46 | Train Loss: 0.4372729 Vali Loss: 0.4322653 Test Loss: 0.4322653
Validation loss decreased (0.434424 --> 0.432265).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9449729919433594
Epoch: 8, Steps: 46 | Train Loss: 0.4172531 Vali Loss: 0.4294124 Test Loss: 0.4294124
Validation loss decreased (0.432265 --> 0.429412).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9672050476074219
Epoch: 9, Steps: 46 | Train Loss: 0.4356659 Vali Loss: 0.4273148 Test Loss: 0.4273148
Validation loss decreased (0.429412 --> 0.427315).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0640242099761963
Epoch: 10, Steps: 46 | Train Loss: 0.4389257 Vali Loss: 0.4267634 Test Loss: 0.4267634
Validation loss decreased (0.427315 --> 0.426763).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.039114486426115036, mae:0.15698881447315216, smape:15.631911158561707, dtw:not calculated
horizon:2 mse:0.2698940336704254, mae:0.4572657346725464, smape:44.37118470668793, dtw:not calculated
horizon:3 mse:0.157818004488945, mae:0.33426937460899353, smape:32.77137279510498, dtw:not calculated
horizon:4 mse:0.2807556688785553, mae:0.4400373101234436, smape:42.40408539772034, dtw:not calculated
horizon:5 mse:0.3210604786872864, mae:0.48115816712379456, smape:46.2470680475235, dtw:not calculated
horizon:6 mse:0.47236868739128113, mae:0.5615471601486206, smape:52.7868390083313, dtw:not calculated
horizon:7 mse:0.8053481578826904, mae:0.7635457515716553, smape:69.48703527450562, dtw:not calculated
horizon:8 mse:0.5372305512428284, mae:0.6425662040710449, smape:60.501617193222046, dtw:not calculated
horizon:9 mse:0.6564638018608093, mae:0.69254469871521, smape:64.11190629005432, dtw:not calculated
horizon:10 mse:0.5163185596466064, mae:0.612798273563385, smape:57.623451948165894, dtw:not calculated
horizon:11 mse:0.46715831756591797, mae:0.5207968354225159, smape:48.42596352100372, dtw:not calculated
horizon:12 mse:0.5976310968399048, mae:0.6467562913894653, smape:60.081106424331665, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2568352520465851, mae:0.40521112084388733, smape:39.03541564941406, dtw:not calculated
average metrics: horizon upto:12 mse:0.4267634451389313, mae:0.5258561968803406, smape:49.53696131706238, dtw:not calculated
===============================================================================
average of horizons: mse:0.4267634451389313, mae:0.5258561968803406, smape:49.53696131706238, dtw:not calculated
mean smape over horizons:  49.53696181376775
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4357366561889648
Epoch: 1, Steps: 46 | Train Loss: 0.7485751 Vali Loss: 0.7571337 Test Loss: 0.7571337
Validation loss decreased (inf --> 0.757134).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1200244426727295
Epoch: 2, Steps: 46 | Train Loss: 0.5194651 Vali Loss: 0.5508436 Test Loss: 0.5508436
Validation loss decreased (0.757134 --> 0.550844).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0861806869506836
Epoch: 3, Steps: 46 | Train Loss: 0.4527204 Vali Loss: 0.4740014 Test Loss: 0.4740014
Validation loss decreased (0.550844 --> 0.474001).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0109038352966309
Epoch: 4, Steps: 46 | Train Loss: 0.4296824 Vali Loss: 0.4403632 Test Loss: 0.4403632
Validation loss decreased (0.474001 --> 0.440363).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.1513009071350098
Epoch: 5, Steps: 46 | Train Loss: 0.4296044 Vali Loss: 0.4244289 Test Loss: 0.4244289
Validation loss decreased (0.440363 --> 0.424429).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0906834602355957
Epoch: 6, Steps: 46 | Train Loss: 0.4351719 Vali Loss: 0.4151755 Test Loss: 0.4151755
Validation loss decreased (0.424429 --> 0.415175).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0571904182434082
Epoch: 7, Steps: 46 | Train Loss: 0.4248008 Vali Loss: 0.4136305 Test Loss: 0.4136305
Validation loss decreased (0.415175 --> 0.413631).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9927113056182861
Epoch: 8, Steps: 46 | Train Loss: 0.4068541 Vali Loss: 0.4117005 Test Loss: 0.4117005
Validation loss decreased (0.413631 --> 0.411701).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9426662921905518
Epoch: 9, Steps: 46 | Train Loss: 0.4272513 Vali Loss: 0.4081757 Test Loss: 0.4081757
Validation loss decreased (0.411701 --> 0.408176).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0797717571258545
Epoch: 10, Steps: 46 | Train Loss: 0.4290174 Vali Loss: 0.4080795 Test Loss: 0.4080795
Validation loss decreased (0.408176 --> 0.408080).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0404183566570282, mae:0.15739788115024567, smape:15.661604702472687, dtw:not calculated
horizon:2 mse:0.24375450611114502, mae:0.4268326461315155, smape:41.49935245513916, dtw:not calculated
horizon:3 mse:0.16340519487857819, mae:0.33404791355133057, smape:32.699453830718994, dtw:not calculated
horizon:4 mse:0.22301438450813293, mae:0.38522183895111084, smape:37.34780550003052, dtw:not calculated
horizon:5 mse:0.33557406067848206, mae:0.4935426414012909, smape:47.378820180892944, dtw:not calculated
horizon:6 mse:0.457322895526886, mae:0.544100821018219, smape:51.14263892173767, dtw:not calculated
horizon:7 mse:0.7462180256843567, mae:0.7304538488388062, smape:66.81796312332153, dtw:not calculated
horizon:8 mse:0.6076546311378479, mae:0.6801690459251404, smape:63.520610332489014, dtw:not calculated
horizon:9 mse:0.6030130982398987, mae:0.6680225133895874, smape:62.23989129066467, dtw:not calculated
horizon:10 mse:0.4191143810749054, mae:0.5609107613563538, smape:53.38556170463562, dtw:not calculated
horizon:11 mse:0.4643357992172241, mae:0.523797869682312, smape:48.768338561058044, dtw:not calculated
horizon:12 mse:0.5931298136711121, mae:0.6534098386764526, smape:60.85408926010132, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2439148724079132, mae:0.39019066095352173, smape:37.62161135673523, dtw:not calculated
average metrics: horizon upto:12 mse:0.408079594373703, mae:0.513158917427063, smape:48.443010449409485, dtw:not calculated
===============================================================================
average of horizons: mse:0.408079594373703, mae:0.513158917427063, smape:48.443010449409485, dtw:not calculated
mean smape over horizons:  48.443010821938515
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=18, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4354157447814941
Epoch: 1, Steps: 46 | Train Loss: 0.7139860 Vali Loss: 0.7429030 Test Loss: 0.7429030
Validation loss decreased (inf --> 0.742903).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1009941101074219
Epoch: 2, Steps: 46 | Train Loss: 0.5035575 Vali Loss: 0.5267633 Test Loss: 0.5267633
Validation loss decreased (0.742903 --> 0.526763).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9843733310699463
Epoch: 3, Steps: 46 | Train Loss: 0.4386865 Vali Loss: 0.4510022 Test Loss: 0.4510022
Validation loss decreased (0.526763 --> 0.451002).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.017549753189087
Epoch: 4, Steps: 46 | Train Loss: 0.4167458 Vali Loss: 0.4178334 Test Loss: 0.4178334
Validation loss decreased (0.451002 --> 0.417833).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0203356742858887
Epoch: 5, Steps: 46 | Train Loss: 0.4171162 Vali Loss: 0.4026792 Test Loss: 0.4026792
Validation loss decreased (0.417833 --> 0.402679).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0443229675292969
Epoch: 6, Steps: 46 | Train Loss: 0.4223412 Vali Loss: 0.3912064 Test Loss: 0.3912064
Validation loss decreased (0.402679 --> 0.391206).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.216385841369629
Epoch: 7, Steps: 46 | Train Loss: 0.4129539 Vali Loss: 0.3916253 Test Loss: 0.3916253
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0019710063934326
Epoch: 8, Steps: 46 | Train Loss: 0.3945867 Vali Loss: 0.3888309 Test Loss: 0.3888309
Validation loss decreased (0.391206 --> 0.388831).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.1021616458892822
Epoch: 9, Steps: 46 | Train Loss: 0.4164129 Vali Loss: 0.3869376 Test Loss: 0.3869376
Validation loss decreased (0.388831 --> 0.386938).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0724339485168457
Epoch: 10, Steps: 46 | Train Loss: 0.4185398 Vali Loss: 0.3855335 Test Loss: 0.3855335
Validation loss decreased (0.386938 --> 0.385533).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.030906368046998978, mae:0.14221929013729095, smape:14.182421565055847, dtw:not calculated
horizon:2 mse:0.22261345386505127, mae:0.41026028990745544, smape:40.00644087791443, dtw:not calculated
horizon:3 mse:0.1548551619052887, mae:0.3246789872646332, smape:31.815001368522644, dtw:not calculated
horizon:4 mse:0.2045971304178238, mae:0.3642357587814331, smape:35.36560833454132, dtw:not calculated
horizon:5 mse:0.3178251385688782, mae:0.4751603603363037, smape:45.66355645656586, dtw:not calculated
horizon:6 mse:0.43205201625823975, mae:0.5246706008911133, smape:49.430468678474426, dtw:not calculated
horizon:7 mse:0.7129135727882385, mae:0.7163817286491394, smape:65.80114960670471, dtw:not calculated
horizon:8 mse:0.617846667766571, mae:0.6805803179740906, smape:63.41512203216553, dtw:not calculated
horizon:9 mse:0.5102807283401489, mae:0.6135382652282715, smape:57.7129065990448, dtw:not calculated
horizon:10 mse:0.3748033940792084, mae:0.5298725366592407, smape:50.65590143203735, dtw:not calculated
horizon:11 mse:0.46206697821617126, mae:0.5232652425765991, smape:48.75093698501587, dtw:not calculated
horizon:12 mse:0.5856414437294006, mae:0.6566768288612366, smape:61.302971839904785, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.22714152932167053, mae:0.3735375702381134, smape:36.07724905014038, dtw:not calculated
average metrics: horizon upto:12 mse:0.38553354144096375, mae:0.4967949688434601, smape:47.008538246154785, dtw:not calculated
===============================================================================
average of horizons: mse:0.38553354144096375, mae:0.4967949688434601, smape:47.008538246154785, dtw:not calculated
mean smape over horizons:  47.008540481328964
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4798126220703125
Epoch: 1, Steps: 46 | Train Loss: 0.8556377 Vali Loss: 0.7355802 Test Loss: 0.7355802
Validation loss decreased (inf --> 0.735580).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0464882850646973
Epoch: 2, Steps: 46 | Train Loss: 0.5604128 Vali Loss: 0.4762840 Test Loss: 0.4762840
Validation loss decreased (0.735580 --> 0.476284).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9687342643737793
Epoch: 3, Steps: 46 | Train Loss: 0.4649613 Vali Loss: 0.3908782 Test Loss: 0.3908782
Validation loss decreased (0.476284 --> 0.390878).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9721605777740479
Epoch: 4, Steps: 46 | Train Loss: 0.4339813 Vali Loss: 0.3556458 Test Loss: 0.3556458
Validation loss decreased (0.390878 --> 0.355646).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0882556438446045
Epoch: 5, Steps: 46 | Train Loss: 0.4206105 Vali Loss: 0.3418880 Test Loss: 0.3418880
Validation loss decreased (0.355646 --> 0.341888).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0472347736358643
Epoch: 6, Steps: 46 | Train Loss: 0.4236033 Vali Loss: 0.3337007 Test Loss: 0.3337007
Validation loss decreased (0.341888 --> 0.333701).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0115747451782227
Epoch: 7, Steps: 46 | Train Loss: 0.4215925 Vali Loss: 0.3289413 Test Loss: 0.3289413
Validation loss decreased (0.333701 --> 0.328941).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0455334186553955
Epoch: 8, Steps: 46 | Train Loss: 0.4239781 Vali Loss: 0.3289964 Test Loss: 0.3289964
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0604186058044434
Epoch: 9, Steps: 46 | Train Loss: 0.4139053 Vali Loss: 0.3283382 Test Loss: 0.3283382
Validation loss decreased (0.328941 --> 0.328338).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.084003210067749
Epoch: 10, Steps: 46 | Train Loss: 0.4197124 Vali Loss: 0.3278081 Test Loss: 0.3278081
Validation loss decreased (0.328338 --> 0.327808).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.08910562843084335, mae:0.22134006023406982, smape:21.80626690387726, dtw:not calculated
horizon:2 mse:0.19153021275997162, mae:0.37227973341941833, smape:36.37144863605499, dtw:not calculated
horizon:3 mse:0.22352655231952667, mae:0.39939990639686584, smape:38.81596922874451, dtw:not calculated
horizon:4 mse:0.23537100851535797, mae:0.4061824381351471, smape:39.393430948257446, dtw:not calculated
horizon:5 mse:0.37464380264282227, mae:0.5194767713546753, smape:49.58388805389404, dtw:not calculated
horizon:6 mse:0.4363178610801697, mae:0.571647047996521, smape:54.31745648384094, dtw:not calculated
horizon:7 mse:0.41285479068756104, mae:0.5261102318763733, smape:49.76242482662201, dtw:not calculated
horizon:8 mse:0.5338453650474548, mae:0.5957379341125488, smape:55.49302101135254, dtw:not calculated
horizon:9 mse:0.42097482085227966, mae:0.5481166839599609, smape:51.982665061950684, dtw:not calculated
horizon:10 mse:0.3906387984752655, mae:0.530684769153595, smape:50.55811405181885, dtw:not calculated
horizon:11 mse:0.3299049139022827, mae:0.46675482392311096, smape:44.59441602230072, dtw:not calculated
horizon:12 mse:0.2949826121330261, mae:0.4412039518356323, smape:42.36196279525757, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2584158778190613, mae:0.4150543212890625, smape:40.0480717420578, dtw:not calculated
average metrics: horizon upto:12 mse:0.3278080224990845, mae:0.46657782793045044, smape:44.586753845214844, dtw:not calculated
===============================================================================
average of horizons: mse:0.3278080224990845, mae:0.46657782793045044, smape:44.586753845214844, dtw:not calculated
mean smape over horizons:  44.58675533533096
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3687427043914795
Epoch: 1, Steps: 46 | Train Loss: 0.8508635 Vali Loss: 0.7194021 Test Loss: 0.7194021
Validation loss decreased (inf --> 0.719402).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.01204252243042
Epoch: 2, Steps: 46 | Train Loss: 0.5509261 Vali Loss: 0.5133576 Test Loss: 0.5133576
Validation loss decreased (0.719402 --> 0.513358).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0183098316192627
Epoch: 3, Steps: 46 | Train Loss: 0.4771302 Vali Loss: 0.4353071 Test Loss: 0.4353071
Validation loss decreased (0.513358 --> 0.435307).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0242023468017578
Epoch: 4, Steps: 46 | Train Loss: 0.4368847 Vali Loss: 0.3980919 Test Loss: 0.3980919
Validation loss decreased (0.435307 --> 0.398092).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.950904130935669
Epoch: 5, Steps: 46 | Train Loss: 0.4311762 Vali Loss: 0.3840393 Test Loss: 0.3840393
Validation loss decreased (0.398092 --> 0.384039).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9491682052612305
Epoch: 6, Steps: 46 | Train Loss: 0.4367425 Vali Loss: 0.3731710 Test Loss: 0.3731710
Validation loss decreased (0.384039 --> 0.373171).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9924068450927734
Epoch: 7, Steps: 46 | Train Loss: 0.4204054 Vali Loss: 0.3715175 Test Loss: 0.3715175
Validation loss decreased (0.373171 --> 0.371517).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.088592290878296
Epoch: 8, Steps: 46 | Train Loss: 0.4158525 Vali Loss: 0.3686848 Test Loss: 0.3686848
Validation loss decreased (0.371517 --> 0.368685).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.129389762878418
Epoch: 9, Steps: 46 | Train Loss: 0.3974783 Vali Loss: 0.3662310 Test Loss: 0.3662310
Validation loss decreased (0.368685 --> 0.366231).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0289406776428223
Epoch: 10, Steps: 46 | Train Loss: 0.4120880 Vali Loss: 0.3652546 Test Loss: 0.3652546
Validation loss decreased (0.366231 --> 0.365255).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05460832267999649, mae:0.17922013998031616, smape:17.78489053249359, dtw:not calculated
horizon:2 mse:0.5144546031951904, mae:0.6202003359794617, smape:58.444494009017944, dtw:not calculated
horizon:3 mse:0.2668452560901642, mae:0.42318835854530334, smape:40.83408713340759, dtw:not calculated
horizon:4 mse:0.32131755352020264, mae:0.4616765081882477, smape:44.189879298210144, dtw:not calculated
horizon:5 mse:0.21644771099090576, mae:0.3728937804698944, smape:36.139070987701416, dtw:not calculated
horizon:6 mse:0.27348917722702026, mae:0.428949236869812, smape:41.32731854915619, dtw:not calculated
horizon:7 mse:0.41673991084098816, mae:0.5423767566680908, smape:51.49356126785278, dtw:not calculated
horizon:8 mse:0.4707612991333008, mae:0.5679107308387756, smape:53.48033905029297, dtw:not calculated
horizon:9 mse:0.35182884335517883, mae:0.48218244314193726, smape:45.93952000141144, dtw:not calculated
horizon:10 mse:0.4311564564704895, mae:0.5607516765594482, smape:53.27302813529968, dtw:not calculated
horizon:11 mse:0.5015320181846619, mae:0.5772331953048706, smape:53.942662477493286, dtw:not calculated
horizon:12 mse:0.5638744235038757, mae:0.6232564449310303, smape:58.06507468223572, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2745271325111389, mae:0.41435471177101135, smape:39.78662192821503, dtw:not calculated
average metrics: horizon upto:12 mse:0.36525461077690125, mae:0.48665329813957214, smape:46.24282717704773, dtw:not calculated
===============================================================================
average of horizons: mse:0.36525461077690125, mae:0.48665329813957214, smape:46.24282717704773, dtw:not calculated
mean smape over horizons:  46.24282717704773
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3683488368988037
Epoch: 1, Steps: 46 | Train Loss: 0.9023006 Vali Loss: 0.7510123 Test Loss: 0.7510123
Validation loss decreased (inf --> 0.751012).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.093559980392456
Epoch: 2, Steps: 46 | Train Loss: 0.5502269 Vali Loss: 0.5368667 Test Loss: 0.5368667
Validation loss decreased (0.751012 --> 0.536867).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1272940635681152
Epoch: 3, Steps: 46 | Train Loss: 0.4736075 Vali Loss: 0.4578887 Test Loss: 0.4578887
Validation loss decreased (0.536867 --> 0.457889).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1563012599945068
Epoch: 4, Steps: 46 | Train Loss: 0.4301175 Vali Loss: 0.4190345 Test Loss: 0.4190345
Validation loss decreased (0.457889 --> 0.419035).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9902114868164062
Epoch: 5, Steps: 46 | Train Loss: 0.4281688 Vali Loss: 0.4030304 Test Loss: 0.4030304
Validation loss decreased (0.419035 --> 0.403030).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0710980892181396
Epoch: 6, Steps: 46 | Train Loss: 0.4243580 Vali Loss: 0.3941053 Test Loss: 0.3941053
Validation loss decreased (0.403030 --> 0.394105).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0293431282043457
Epoch: 7, Steps: 46 | Train Loss: 0.4105543 Vali Loss: 0.3897899 Test Loss: 0.3897899
Validation loss decreased (0.394105 --> 0.389790).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0411276817321777
Epoch: 8, Steps: 46 | Train Loss: 0.4046689 Vali Loss: 0.3874574 Test Loss: 0.3874574
Validation loss decreased (0.389790 --> 0.387457).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0007102489471436
Epoch: 9, Steps: 46 | Train Loss: 0.3950860 Vali Loss: 0.3849979 Test Loss: 0.3849979
Validation loss decreased (0.387457 --> 0.384998).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0060415267944336
Epoch: 10, Steps: 46 | Train Loss: 0.4077787 Vali Loss: 0.3854205 Test Loss: 0.3854205
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03889964520931244, mae:0.15413996577262878, smape:15.340505540370941, dtw:not calculated
horizon:2 mse:0.42332738637924194, mae:0.5618556141853333, smape:53.43409180641174, dtw:not calculated
horizon:3 mse:0.2934458553791046, mae:0.46141156554222107, smape:44.52430009841919, dtw:not calculated
horizon:4 mse:0.3642542064189911, mae:0.48490846157073975, smape:46.04779779911041, dtw:not calculated
horizon:5 mse:0.2102043479681015, mae:0.36584392189979553, smape:35.45909523963928, dtw:not calculated
horizon:6 mse:0.23507292568683624, mae:0.3972715437412262, smape:38.46859037876129, dtw:not calculated
horizon:7 mse:0.4585253596305847, mae:0.5833718180656433, smape:55.26165962219238, dtw:not calculated
horizon:8 mse:0.4846646785736084, mae:0.5903300046920776, smape:55.65509796142578, dtw:not calculated
horizon:9 mse:0.3514651954174042, mae:0.4790857434272766, smape:45.62768638134003, dtw:not calculated
horizon:10 mse:0.5654009580612183, mae:0.6441306471824646, smape:60.30925512313843, dtw:not calculated
horizon:11 mse:0.5729372501373291, mae:0.6040903329849243, smape:55.82270622253418, dtw:not calculated
horizon:12 mse:0.6217763423919678, mae:0.6616831421852112, smape:61.364006996154785, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2608673870563507, mae:0.4042385220527649, smape:38.8790637254715, dtw:not calculated
average metrics: horizon upto:12 mse:0.3849978744983673, mae:0.49901023507118225, smape:47.27623462677002, dtw:not calculated
===============================================================================
average of horizons: mse:0.3849978744983673, mae:0.49901023507118225, smape:47.27623462677002, dtw:not calculated
mean smape over horizons:  47.27623276412487
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4068021774291992
Epoch: 1, Steps: 46 | Train Loss: 0.7600919 Vali Loss: 0.6942817 Test Loss: 0.6942817
Validation loss decreased (inf --> 0.694282).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1219251155853271
Epoch: 2, Steps: 46 | Train Loss: 0.5657824 Vali Loss: 0.5124688 Test Loss: 0.5124688
Validation loss decreased (0.694282 --> 0.512469).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0017189979553223
Epoch: 3, Steps: 46 | Train Loss: 0.4865345 Vali Loss: 0.4425470 Test Loss: 0.4425470
Validation loss decreased (0.512469 --> 0.442547).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0318427085876465
Epoch: 4, Steps: 46 | Train Loss: 0.4415787 Vali Loss: 0.4097860 Test Loss: 0.4097860
Validation loss decreased (0.442547 --> 0.409786).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0576348304748535
Epoch: 5, Steps: 46 | Train Loss: 0.4201626 Vali Loss: 0.3962801 Test Loss: 0.3962801
Validation loss decreased (0.409786 --> 0.396280).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0360162258148193
Epoch: 6, Steps: 46 | Train Loss: 0.4350354 Vali Loss: 0.3885208 Test Loss: 0.3885208
Validation loss decreased (0.396280 --> 0.388521).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0834126472473145
Epoch: 7, Steps: 46 | Train Loss: 0.4247568 Vali Loss: 0.3849969 Test Loss: 0.3849969
Validation loss decreased (0.388521 --> 0.384997).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0620217323303223
Epoch: 8, Steps: 46 | Train Loss: 0.4111517 Vali Loss: 0.3799843 Test Loss: 0.3799843
Validation loss decreased (0.384997 --> 0.379984).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9931297302246094
Epoch: 9, Steps: 46 | Train Loss: 0.4287062 Vali Loss: 0.3813642 Test Loss: 0.3813642
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9647266864776611
Epoch: 10, Steps: 46 | Train Loss: 0.4260323 Vali Loss: 0.3823114 Test Loss: 0.3823114
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.043280623853206635, mae:0.15996386110782623, smape:15.9064382314682, dtw:not calculated
horizon:2 mse:0.3367571532726288, mae:0.5053681135177612, smape:48.62923324108124, dtw:not calculated
horizon:3 mse:0.38959866762161255, mae:0.5276115536689758, smape:50.32173991203308, dtw:not calculated
horizon:4 mse:0.3648771643638611, mae:0.5039093494415283, smape:48.11055660247803, dtw:not calculated
horizon:5 mse:0.3276219069957733, mae:0.46270614862442017, smape:44.20492649078369, dtw:not calculated
horizon:6 mse:0.5382087826728821, mae:0.622397780418396, smape:58.324265480041504, dtw:not calculated
horizon:7 mse:0.493308424949646, mae:0.5972465872764587, smape:56.25749230384827, dtw:not calculated
horizon:8 mse:0.4549427330493927, mae:0.5628686547279358, smape:53.07408571243286, dtw:not calculated
horizon:9 mse:0.4207296669483185, mae:0.543846607208252, smape:51.58286690711975, dtw:not calculated
horizon:10 mse:0.2914313077926636, mae:0.4461001753807068, smape:42.9118812084198, dtw:not calculated
horizon:11 mse:0.5700991153717041, mae:0.6221003532409668, smape:57.843488454818726, dtw:not calculated
horizon:12 mse:0.3289564251899719, mae:0.4822833836078644, smape:46.261268854141235, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3333907425403595, mae:0.46365946531295776, smape:44.24952268600464, dtw:not calculated
average metrics: horizon upto:12 mse:0.3799843192100525, mae:0.5030335187911987, smape:47.78568744659424, dtw:not calculated
===============================================================================
average of horizons: mse:0.3799843192100525, mae:0.5030335187911987, smape:47.78568744659424, dtw:not calculated
mean smape over horizons:  47.78568694988886
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5259487628936768
Epoch: 1, Steps: 46 | Train Loss: 0.7677378 Vali Loss: 0.7108454 Test Loss: 0.7108454
Validation loss decreased (inf --> 0.710845).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9996495246887207
Epoch: 2, Steps: 46 | Train Loss: 0.5674826 Vali Loss: 0.5355264 Test Loss: 0.5355264
Validation loss decreased (0.710845 --> 0.535526).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0911002159118652
Epoch: 3, Steps: 46 | Train Loss: 0.4882870 Vali Loss: 0.4682440 Test Loss: 0.4682440
Validation loss decreased (0.535526 --> 0.468244).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1113219261169434
Epoch: 4, Steps: 46 | Train Loss: 0.4443834 Vali Loss: 0.4359469 Test Loss: 0.4359469
Validation loss decreased (0.468244 --> 0.435947).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9943583011627197
Epoch: 5, Steps: 46 | Train Loss: 0.4226547 Vali Loss: 0.4227588 Test Loss: 0.4227588
Validation loss decreased (0.435947 --> 0.422759).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0005550384521484
Epoch: 6, Steps: 46 | Train Loss: 0.4376173 Vali Loss: 0.4167730 Test Loss: 0.4167730
Validation loss decreased (0.422759 --> 0.416773).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0384626388549805
Epoch: 7, Steps: 46 | Train Loss: 0.4298532 Vali Loss: 0.4121087 Test Loss: 0.4121087
Validation loss decreased (0.416773 --> 0.412109).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0179691314697266
Epoch: 8, Steps: 46 | Train Loss: 0.4136655 Vali Loss: 0.4063919 Test Loss: 0.4063919
Validation loss decreased (0.412109 --> 0.406392).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9674692153930664
Epoch: 9, Steps: 46 | Train Loss: 0.4401585 Vali Loss: 0.4079977 Test Loss: 0.4079977
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.981834888458252
Epoch: 10, Steps: 46 | Train Loss: 0.4333248 Vali Loss: 0.4083971 Test Loss: 0.4083971
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.037085819989442825, mae:0.15272951126098633, smape:15.209653973579407, dtw:not calculated
horizon:2 mse:0.3841387629508972, mae:0.5458492040634155, smape:52.32137441635132, dtw:not calculated
horizon:3 mse:0.42709580063819885, mae:0.5626882910728455, smape:53.54206562042236, dtw:not calculated
horizon:4 mse:0.34680381417274475, mae:0.4968368113040924, smape:47.62096703052521, dtw:not calculated
horizon:5 mse:0.4428519904613495, mae:0.5445411801338196, smape:51.37351751327515, dtw:not calculated
horizon:6 mse:0.4945109486579895, mae:0.5959168672561646, smape:56.1004638671875, dtw:not calculated
horizon:7 mse:0.4759458601474762, mae:0.5938937067985535, smape:56.132709980010986, dtw:not calculated
horizon:8 mse:0.48888251185417175, mae:0.5852774381637573, smape:55.00556826591492, dtw:not calculated
horizon:9 mse:0.46181026101112366, mae:0.5795736908912659, smape:54.831165075302124, dtw:not calculated
horizon:10 mse:0.2874389886856079, mae:0.4477892518043518, smape:43.140026926994324, dtw:not calculated
horizon:11 mse:0.6882545948028564, mae:0.6957213878631592, smape:63.98629546165466, dtw:not calculated
horizon:12 mse:0.34188389778137207, mae:0.49138444662094116, smape:47.062188386917114, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3554145395755768, mae:0.48309364914894104, smape:46.028006076812744, dtw:not calculated
average metrics: horizon upto:12 mse:0.4063919186592102, mae:0.524350106716156, smape:49.69382882118225, dtw:not calculated
===============================================================================
average of horizons: mse:0.4063919186592102, mae:0.524350106716156, smape:49.69382882118225, dtw:not calculated
mean smape over horizons:  49.693833043177925
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4782423973083496
Epoch: 1, Steps: 46 | Train Loss: 0.7719976 Vali Loss: 0.6957028 Test Loss: 0.6957028
Validation loss decreased (inf --> 0.695703).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0580883026123047
Epoch: 2, Steps: 46 | Train Loss: 0.5589078 Vali Loss: 0.5142273 Test Loss: 0.5142273
Validation loss decreased (0.695703 --> 0.514227).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0802385807037354
Epoch: 3, Steps: 46 | Train Loss: 0.4838828 Vali Loss: 0.4505011 Test Loss: 0.4505011
Validation loss decreased (0.514227 --> 0.450501).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9605598449707031
Epoch: 4, Steps: 46 | Train Loss: 0.4384649 Vali Loss: 0.4203162 Test Loss: 0.4203162
Validation loss decreased (0.450501 --> 0.420316).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0390150547027588
Epoch: 5, Steps: 46 | Train Loss: 0.4189245 Vali Loss: 0.4096224 Test Loss: 0.4096224
Validation loss decreased (0.420316 --> 0.409622).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0227811336517334
Epoch: 6, Steps: 46 | Train Loss: 0.4320831 Vali Loss: 0.4021361 Test Loss: 0.4021361
Validation loss decreased (0.409622 --> 0.402136).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0069241523742676
Epoch: 7, Steps: 46 | Train Loss: 0.4272560 Vali Loss: 0.3985056 Test Loss: 0.3985056
Validation loss decreased (0.402136 --> 0.398506).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9933063983917236
Epoch: 8, Steps: 46 | Train Loss: 0.4143365 Vali Loss: 0.3943077 Test Loss: 0.3943077
Validation loss decreased (0.398506 --> 0.394308).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.014660358428955
Epoch: 9, Steps: 46 | Train Loss: 0.4390560 Vali Loss: 0.3955672 Test Loss: 0.3955672
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.077141284942627
Epoch: 10, Steps: 46 | Train Loss: 0.4285294 Vali Loss: 0.3942107 Test Loss: 0.3942107
Validation loss decreased (0.394308 --> 0.394211).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04579177126288414, mae:0.16714230179786682, smape:16.615261137485504, dtw:not calculated
horizon:2 mse:0.33546724915504456, mae:0.5070059895515442, smape:48.838457465171814, dtw:not calculated
horizon:3 mse:0.38965722918510437, mae:0.5296456813812256, smape:50.51963925361633, dtw:not calculated
horizon:4 mse:0.30852454900741577, mae:0.46950244903564453, smape:45.2452689409256, dtw:not calculated
horizon:5 mse:0.5191730856895447, mae:0.6046901941299438, smape:56.71543478965759, dtw:not calculated
horizon:6 mse:0.4637519121170044, mae:0.576194703578949, smape:54.40148711204529, dtw:not calculated
horizon:7 mse:0.4349380433559418, mae:0.5640419125556946, smape:53.52606773376465, dtw:not calculated
horizon:8 mse:0.49530428647994995, mae:0.6004642844200134, smape:56.556326150894165, dtw:not calculated
horizon:9 mse:0.4176590144634247, mae:0.5504273772239685, smape:52.29718089103699, dtw:not calculated
horizon:10 mse:0.2829335033893585, mae:0.4454144537448883, smape:42.94268488883972, dtw:not calculated
horizon:11 mse:0.6524519920349121, mae:0.6783118844032288, smape:62.63083815574646, dtw:not calculated
horizon:12 mse:0.3848760426044464, mae:0.5292884707450867, smape:50.52153468132019, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.34372764825820923, mae:0.47569695115089417, smape:45.38925886154175, dtw:not calculated
average metrics: horizon upto:12 mse:0.39421069622039795, mae:0.5185107588768005, smape:49.23418164253235, dtw:not calculated
===============================================================================
average of horizons: mse:0.39421069622039795, mae:0.5185107588768005, smape:49.23418164253235, dtw:not calculated
mean smape over horizons:  49.234181766708694
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5082390308380127
Epoch: 1, Steps: 46 | Train Loss: 0.7561088 Vali Loss: 0.6651831 Test Loss: 0.6651831
Validation loss decreased (inf --> 0.665183).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.061493158340454
Epoch: 2, Steps: 46 | Train Loss: 0.5357718 Vali Loss: 0.4693836 Test Loss: 0.4693836
Validation loss decreased (0.665183 --> 0.469384).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0145909786224365
Epoch: 3, Steps: 46 | Train Loss: 0.4624961 Vali Loss: 0.4048904 Test Loss: 0.4048904
Validation loss decreased (0.469384 --> 0.404890).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.987062931060791
Epoch: 4, Steps: 46 | Train Loss: 0.4177495 Vali Loss: 0.3769248 Test Loss: 0.3769248
Validation loss decreased (0.404890 --> 0.376925).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9562113285064697
Epoch: 5, Steps: 46 | Train Loss: 0.3987846 Vali Loss: 0.3672402 Test Loss: 0.3672402
Validation loss decreased (0.376925 --> 0.367240).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0315771102905273
Epoch: 6, Steps: 46 | Train Loss: 0.4140722 Vali Loss: 0.3601729 Test Loss: 0.3601729
Validation loss decreased (0.367240 --> 0.360173).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9559123516082764
Epoch: 7, Steps: 46 | Train Loss: 0.4089203 Vali Loss: 0.3555745 Test Loss: 0.3555745
Validation loss decreased (0.360173 --> 0.355575).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0123512744903564
Epoch: 8, Steps: 46 | Train Loss: 0.3940379 Vali Loss: 0.3522370 Test Loss: 0.3522370
Validation loss decreased (0.355575 --> 0.352237).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0425477027893066
Epoch: 9, Steps: 46 | Train Loss: 0.4189637 Vali Loss: 0.3529230 Test Loss: 0.3529230
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9935026168823242
Epoch: 10, Steps: 46 | Train Loss: 0.4062549 Vali Loss: 0.3513058 Test Loss: 0.3513058
Validation loss decreased (0.352237 --> 0.351306).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04208722338080406, mae:0.15750113129615784, smape:15.659552812576294, dtw:not calculated
horizon:2 mse:0.2542854845523834, mae:0.43656763434410095, smape:42.40566790103912, dtw:not calculated
horizon:3 mse:0.3132924437522888, mae:0.4745490849018097, smape:45.66062688827515, dtw:not calculated
horizon:4 mse:0.2614876925945282, mae:0.43599194288253784, smape:42.27930307388306, dtw:not calculated
horizon:5 mse:0.5331897735595703, mae:0.6166945099830627, smape:57.811689376831055, dtw:not calculated
horizon:6 mse:0.38335853815078735, mae:0.5216190814971924, smape:49.678534269332886, dtw:not calculated
horizon:7 mse:0.35500621795654297, mae:0.5014448165893555, smape:47.92640209197998, dtw:not calculated
horizon:8 mse:0.43774524331092834, mae:0.5607286691665649, smape:53.14012169837952, dtw:not calculated
horizon:9 mse:0.3713690936565399, mae:0.513533353805542, smape:48.988813161849976, dtw:not calculated
horizon:10 mse:0.3062833547592163, mae:0.4630337357521057, smape:44.51092779636383, dtw:not calculated
horizon:11 mse:0.5726485848426819, mae:0.6387076377868652, smape:59.548377990722656, dtw:not calculated
horizon:12 mse:0.38491636514663696, mae:0.5285667777061462, smape:50.43655037879944, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2979501783847809, mae:0.440487265586853, smape:42.24922955036163, dtw:not calculated
average metrics: horizon upto:12 mse:0.35130584239959717, mae:0.4874115586280823, smape:46.50388062000275, dtw:not calculated
===============================================================================
average of horizons: mse:0.35130584239959717, mae:0.4874115586280823, smape:46.50388062000275, dtw:not calculated
mean smape over horizons:  46.50388062000275
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5038514137268066
Epoch: 1, Steps: 46 | Train Loss: 0.7695740 Vali Loss: 0.7673581 Test Loss: 0.7673581
Validation loss decreased (inf --> 0.767358).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9652805328369141
Epoch: 2, Steps: 46 | Train Loss: 0.5723299 Vali Loss: 0.5962430 Test Loss: 0.5962430
Validation loss decreased (0.767358 --> 0.596243).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.073256015777588
Epoch: 3, Steps: 46 | Train Loss: 0.5033254 Vali Loss: 0.5158306 Test Loss: 0.5158306
Validation loss decreased (0.596243 --> 0.515831).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0697016716003418
Epoch: 4, Steps: 46 | Train Loss: 0.4731902 Vali Loss: 0.4794283 Test Loss: 0.4794283
Validation loss decreased (0.515831 --> 0.479428).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.037459373474121
Epoch: 5, Steps: 46 | Train Loss: 0.4570874 Vali Loss: 0.4629876 Test Loss: 0.4629876
Validation loss decreased (0.479428 --> 0.462988).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9374115467071533
Epoch: 6, Steps: 46 | Train Loss: 0.4689932 Vali Loss: 0.4551097 Test Loss: 0.4551097
Validation loss decreased (0.462988 --> 0.455110).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9738502502441406
Epoch: 7, Steps: 46 | Train Loss: 0.4672071 Vali Loss: 0.4513674 Test Loss: 0.4513674
Validation loss decreased (0.455110 --> 0.451367).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.044764757156372
Epoch: 8, Steps: 46 | Train Loss: 0.4452606 Vali Loss: 0.4491940 Test Loss: 0.4491940
Validation loss decreased (0.451367 --> 0.449194).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0316977500915527
Epoch: 9, Steps: 46 | Train Loss: 0.4579979 Vali Loss: 0.4484109 Test Loss: 0.4484109
Validation loss decreased (0.449194 --> 0.448411).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0615079402923584
Epoch: 10, Steps: 46 | Train Loss: 0.4541900 Vali Loss: 0.4474501 Test Loss: 0.4474501
Validation loss decreased (0.448411 --> 0.447450).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04159710556268692, mae:0.15371479094028473, smape:15.281225740909576, dtw:not calculated
horizon:2 mse:0.21592845022678375, mae:0.39301204681396484, smape:38.259029388427734, dtw:not calculated
horizon:3 mse:0.4597465693950653, mae:0.5722163319587708, smape:54.1187047958374, dtw:not calculated
horizon:4 mse:0.4072272479534149, mae:0.5306715965270996, smape:50.37236213684082, dtw:not calculated
horizon:5 mse:0.381754606962204, mae:0.5202146172523499, smape:49.583834409713745, dtw:not calculated
horizon:6 mse:0.4972153604030609, mae:0.5798773169517517, smape:54.28853631019592, dtw:not calculated
horizon:7 mse:0.5021939873695374, mae:0.5857717990875244, smape:54.94610071182251, dtw:not calculated
horizon:8 mse:0.48476970195770264, mae:0.5682873725891113, smape:53.223997354507446, dtw:not calculated
horizon:9 mse:0.5487011671066284, mae:0.5940523147583008, smape:55.116790533065796, dtw:not calculated
horizon:10 mse:0.8654164671897888, mae:0.7699811458587646, smape:69.22724843025208, dtw:not calculated
horizon:11 mse:0.46656334400177, mae:0.5567846298217773, smape:52.285581827163696, dtw:not calculated
horizon:12 mse:0.4982864558696747, mae:0.6032402515411377, smape:56.81557059288025, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3339115381240845, mae:0.4582844376564026, smape:43.650615215301514, dtw:not calculated
average metrics: horizon upto:12 mse:0.44745001196861267, mae:0.5356520414352417, smape:50.293248891830444, dtw:not calculated
===============================================================================
average of horizons: mse:0.44745001196861267, mae:0.5356520414352417, smape:50.293248891830444, dtw:not calculated
mean smape over horizons:  50.293248519301414
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=20, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.47414231300354
Epoch: 1, Steps: 46 | Train Loss: 0.7703832 Vali Loss: 0.7601566 Test Loss: 0.7601566
Validation loss decreased (inf --> 0.760157).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0751235485076904
Epoch: 2, Steps: 46 | Train Loss: 0.5650329 Vali Loss: 0.5865035 Test Loss: 0.5865035
Validation loss decreased (0.760157 --> 0.586504).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1284945011138916
Epoch: 3, Steps: 46 | Train Loss: 0.4963494 Vali Loss: 0.5059726 Test Loss: 0.5059726
Validation loss decreased (0.586504 --> 0.505973).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0333969593048096
Epoch: 4, Steps: 46 | Train Loss: 0.4682162 Vali Loss: 0.4699960 Test Loss: 0.4699960
Validation loss decreased (0.505973 --> 0.469996).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9239614009857178
Epoch: 5, Steps: 46 | Train Loss: 0.4495491 Vali Loss: 0.4522579 Test Loss: 0.4522579
Validation loss decreased (0.469996 --> 0.452258).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1047141551971436
Epoch: 6, Steps: 46 | Train Loss: 0.4618414 Vali Loss: 0.4456533 Test Loss: 0.4456533
Validation loss decreased (0.452258 --> 0.445653).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.1258063316345215
Epoch: 7, Steps: 46 | Train Loss: 0.4597091 Vali Loss: 0.4404753 Test Loss: 0.4404753
Validation loss decreased (0.445653 --> 0.440475).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.123194932937622
Epoch: 8, Steps: 46 | Train Loss: 0.4386181 Vali Loss: 0.4382342 Test Loss: 0.4382342
Validation loss decreased (0.440475 --> 0.438234).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0214710235595703
Epoch: 9, Steps: 46 | Train Loss: 0.4490678 Vali Loss: 0.4383701 Test Loss: 0.4383701
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0814213752746582
Epoch: 10, Steps: 46 | Train Loss: 0.4435540 Vali Loss: 0.4369763 Test Loss: 0.4369763
Validation loss decreased (0.438234 --> 0.436976).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.032811716198921204, mae:0.141556054353714, smape:14.105375111103058, dtw:not calculated
horizon:2 mse:0.20353958010673523, mae:0.3837358355522156, smape:37.4258428812027, dtw:not calculated
horizon:3 mse:0.389251708984375, mae:0.5291399359703064, smape:50.46783685684204, dtw:not calculated
horizon:4 mse:0.44000905752182007, mae:0.5501358509063721, smape:52.00745463371277, dtw:not calculated
horizon:5 mse:0.37704408168792725, mae:0.5120608806610107, smape:48.787546157836914, dtw:not calculated
horizon:6 mse:0.5182138681411743, mae:0.5866765975952148, smape:54.72569465637207, dtw:not calculated
horizon:7 mse:0.47023189067840576, mae:0.5687630772590637, smape:53.55951189994812, dtw:not calculated
horizon:8 mse:0.47226035594940186, mae:0.558858335018158, smape:52.41073966026306, dtw:not calculated
horizon:9 mse:0.5331470370292664, mae:0.5890442728996277, smape:54.817360639572144, dtw:not calculated
horizon:10 mse:0.8921605944633484, mae:0.7875215411186218, smape:70.71949243545532, dtw:not calculated
horizon:11 mse:0.4624401926994324, mae:0.551301121711731, smape:51.75809860229492, dtw:not calculated
horizon:12 mse:0.45260506868362427, mae:0.574022650718689, smape:54.352712631225586, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.32681167125701904, mae:0.4505508542060852, smape:42.919957637786865, dtw:not calculated
average metrics: horizon upto:12 mse:0.43697628378868103, mae:0.527734637260437, smape:49.59481060504913, dtw:not calculated
===============================================================================
average of horizons: mse:0.43697628378868103, mae:0.527734637260437, smape:49.59481060504913, dtw:not calculated
mean smape over horizons:  49.59480551381906
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4004006385803223
Epoch: 1, Steps: 46 | Train Loss: 0.6744615 Vali Loss: 0.7130924 Test Loss: 0.7130924
Validation loss decreased (inf --> 0.713092).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1166050434112549
Epoch: 2, Steps: 46 | Train Loss: 0.5535286 Vali Loss: 0.4595088 Test Loss: 0.4595088
Validation loss decreased (0.713092 --> 0.459509).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9631257057189941
Epoch: 3, Steps: 46 | Train Loss: 0.4597831 Vali Loss: 0.3673799 Test Loss: 0.3673799
Validation loss decreased (0.459509 --> 0.367380).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9903988838195801
Epoch: 4, Steps: 46 | Train Loss: 0.4238974 Vali Loss: 0.3360717 Test Loss: 0.3360717
Validation loss decreased (0.367380 --> 0.336072).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9692628383636475
Epoch: 5, Steps: 46 | Train Loss: 0.4074880 Vali Loss: 0.3207498 Test Loss: 0.3207498
Validation loss decreased (0.336072 --> 0.320750).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9940154552459717
Epoch: 6, Steps: 46 | Train Loss: 0.4171851 Vali Loss: 0.3136281 Test Loss: 0.3136281
Validation loss decreased (0.320750 --> 0.313628).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0559172630310059
Epoch: 7, Steps: 46 | Train Loss: 0.3972474 Vali Loss: 0.3134077 Test Loss: 0.3134077
Validation loss decreased (0.313628 --> 0.313408).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.947272777557373
Epoch: 8, Steps: 46 | Train Loss: 0.3862781 Vali Loss: 0.3111950 Test Loss: 0.3111950
Validation loss decreased (0.313408 --> 0.311195).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9329080581665039
Epoch: 9, Steps: 46 | Train Loss: 0.3941982 Vali Loss: 0.3082988 Test Loss: 0.3082988
Validation loss decreased (0.311195 --> 0.308299).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0246739387512207
Epoch: 10, Steps: 46 | Train Loss: 0.3903332 Vali Loss: 0.3089587 Test Loss: 0.3089587
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.07073557376861572, mae:0.2065957486629486, smape:20.45595794916153, dtw:not calculated
horizon:2 mse:0.2827032804489136, mae:0.4430236518383026, smape:42.69829988479614, dtw:not calculated
horizon:3 mse:0.36517783999443054, mae:0.5131520628929138, smape:49.07727837562561, dtw:not calculated
horizon:4 mse:0.28328362107276917, mae:0.43297508358955383, smape:41.66568219661713, dtw:not calculated
horizon:5 mse:0.2624114751815796, mae:0.41599684953689575, smape:40.102121233940125, dtw:not calculated
horizon:6 mse:0.2604319751262665, mae:0.41127869486808777, smape:39.64219689369202, dtw:not calculated
horizon:7 mse:0.37664270401000977, mae:0.5104297399520874, smape:48.64968955516815, dtw:not calculated
horizon:8 mse:0.2560291290283203, mae:0.40736669301986694, smape:39.29375112056732, dtw:not calculated
horizon:9 mse:0.28386953473091125, mae:0.44269028306007385, smape:42.629098892211914, dtw:not calculated
horizon:10 mse:0.35961562395095825, mae:0.4850553870201111, smape:46.13592028617859, dtw:not calculated
horizon:11 mse:0.5196972489356995, mae:0.6089597344398499, smape:57.196587324142456, dtw:not calculated
horizon:12 mse:0.3789878189563751, mae:0.511493980884552, smape:48.703765869140625, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2541239559650421, mae:0.4038369953632355, smape:38.94025683403015, dtw:not calculated
average metrics: horizon upto:12 mse:0.308298796415329, mae:0.4490848183631897, smape:43.020859360694885, dtw:not calculated
===============================================================================
average of horizons: mse:0.308298796415329, mae:0.4490848183631897, smape:43.020859360694885, dtw:not calculated
mean smape over horizons:  43.02086246510347
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4432332515716553
Epoch: 1, Steps: 46 | Train Loss: 0.7607878 Vali Loss: 0.8004397 Test Loss: 0.8004397
Validation loss decreased (inf --> 0.800440).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.05513596534729
Epoch: 2, Steps: 46 | Train Loss: 0.5405424 Vali Loss: 0.5555959 Test Loss: 0.5555959
Validation loss decreased (0.800440 --> 0.555596).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0211310386657715
Epoch: 3, Steps: 46 | Train Loss: 0.4703856 Vali Loss: 0.4655222 Test Loss: 0.4655222
Validation loss decreased (0.555596 --> 0.465522).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9883973598480225
Epoch: 4, Steps: 46 | Train Loss: 0.4453579 Vali Loss: 0.4279035 Test Loss: 0.4279035
Validation loss decreased (0.465522 --> 0.427903).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9571104049682617
Epoch: 5, Steps: 46 | Train Loss: 0.4449848 Vali Loss: 0.4106611 Test Loss: 0.4106611
Validation loss decreased (0.427903 --> 0.410661).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0385475158691406
Epoch: 6, Steps: 46 | Train Loss: 0.4212940 Vali Loss: 0.4006842 Test Loss: 0.4006842
Validation loss decreased (0.410661 --> 0.400684).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0023329257965088
Epoch: 7, Steps: 46 | Train Loss: 0.4277269 Vali Loss: 0.3954953 Test Loss: 0.3954953
Validation loss decreased (0.400684 --> 0.395495).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0450968742370605
Epoch: 8, Steps: 46 | Train Loss: 0.4100631 Vali Loss: 0.3943078 Test Loss: 0.3943078
Validation loss decreased (0.395495 --> 0.394308).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0575032234191895
Epoch: 9, Steps: 46 | Train Loss: 0.4109407 Vali Loss: 0.3927223 Test Loss: 0.3927223
Validation loss decreased (0.394308 --> 0.392722).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0197362899780273
Epoch: 10, Steps: 46 | Train Loss: 0.4237261 Vali Loss: 0.3916240 Test Loss: 0.3916240
Validation loss decreased (0.392722 --> 0.391624).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05869969353079796, mae:0.19135834276676178, smape:18.99188607931137, dtw:not calculated
horizon:2 mse:0.26511693000793457, mae:0.4448224604129791, smape:43.07704567909241, dtw:not calculated
horizon:3 mse:0.17485062777996063, mae:0.3334917426109314, smape:32.52796530723572, dtw:not calculated
horizon:4 mse:0.31254637241363525, mae:0.46310269832611084, smape:44.44442689418793, dtw:not calculated
horizon:5 mse:0.2600911259651184, mae:0.39867469668388367, smape:38.312506675720215, dtw:not calculated
horizon:6 mse:0.27677810192108154, mae:0.4355872869491577, smape:42.02470779418945, dtw:not calculated
horizon:7 mse:0.4835103154182434, mae:0.5642988085746765, smape:52.81456112861633, dtw:not calculated
horizon:8 mse:0.43949517607688904, mae:0.5411646366119385, smape:51.02106332778931, dtw:not calculated
horizon:9 mse:0.6836066246032715, mae:0.6945144534111023, smape:63.98702263832092, dtw:not calculated
horizon:10 mse:0.3364483416080475, mae:0.474465012550354, smape:45.30887007713318, dtw:not calculated
horizon:11 mse:0.8461557030677795, mae:0.7632079124450684, smape:68.88507008552551, dtw:not calculated
horizon:12 mse:0.5621885657310486, mae:0.6181772351264954, smape:57.52114653587341, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.22468048334121704, mae:0.3778395354747772, smape:36.56308948993683, dtw:not calculated
average metrics: horizon upto:12 mse:0.39162394404411316, mae:0.4935721457004547, smape:46.576353907585144, dtw:not calculated
===============================================================================
average of horizons: mse:0.39162394404411316, mae:0.4935721457004547, smape:46.576353907585144, dtw:not calculated
mean smape over horizons:  46.57635601858298
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4835870265960693
Epoch: 1, Steps: 46 | Train Loss: 0.9046961 Vali Loss: 0.6111751 Test Loss: 0.6111751
Validation loss decreased (inf --> 0.611175).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.929945707321167
Epoch: 2, Steps: 46 | Train Loss: 0.5891781 Vali Loss: 0.4259253 Test Loss: 0.4259253
Validation loss decreased (0.611175 --> 0.425925).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0326251983642578
Epoch: 3, Steps: 46 | Train Loss: 0.4660329 Vali Loss: 0.3635272 Test Loss: 0.3635272
Validation loss decreased (0.425925 --> 0.363527).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9947662353515625
Epoch: 4, Steps: 46 | Train Loss: 0.4344997 Vali Loss: 0.3340240 Test Loss: 0.3340240
Validation loss decreased (0.363527 --> 0.334024).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.037198781967163
Epoch: 5, Steps: 46 | Train Loss: 0.4177285 Vali Loss: 0.3269798 Test Loss: 0.3269798
Validation loss decreased (0.334024 --> 0.326980).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.994920015335083
Epoch: 6, Steps: 46 | Train Loss: 0.4374440 Vali Loss: 0.3175629 Test Loss: 0.3175629
Validation loss decreased (0.326980 --> 0.317563).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0748260021209717
Epoch: 7, Steps: 46 | Train Loss: 0.4189256 Vali Loss: 0.3115780 Test Loss: 0.3115780
Validation loss decreased (0.317563 --> 0.311578).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0153899192810059
Epoch: 8, Steps: 46 | Train Loss: 0.4171614 Vali Loss: 0.3082897 Test Loss: 0.3082897
Validation loss decreased (0.311578 --> 0.308290).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0402591228485107
Epoch: 9, Steps: 46 | Train Loss: 0.4106873 Vali Loss: 0.3101134 Test Loss: 0.3101134
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0867924690246582
Epoch: 10, Steps: 46 | Train Loss: 0.4177905 Vali Loss: 0.3067986 Test Loss: 0.3067986
Validation loss decreased (0.308290 --> 0.306799).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06316550076007843, mae:0.18467922508716583, smape:18.27162057161331, dtw:not calculated
horizon:2 mse:0.15264268219470978, mae:0.33090248703956604, smape:32.465559244155884, dtw:not calculated
horizon:3 mse:0.6211050748825073, mae:0.6839851140975952, smape:63.723450899124146, dtw:not calculated
horizon:4 mse:0.23890191316604614, mae:0.39518654346466064, smape:38.21603059768677, dtw:not calculated
horizon:5 mse:0.23808367550373077, mae:0.39715296030044556, smape:38.42768371105194, dtw:not calculated
horizon:6 mse:0.24377110600471497, mae:0.400448739528656, smape:38.7153297662735, dtw:not calculated
horizon:7 mse:0.4259093701839447, mae:0.5331886410713196, smape:50.39327144622803, dtw:not calculated
horizon:8 mse:0.2738666236400604, mae:0.4457302689552307, smape:43.062928318977356, dtw:not calculated
horizon:9 mse:0.43998852372169495, mae:0.5458114743232727, smape:51.481568813323975, dtw:not calculated
horizon:10 mse:0.31008341908454895, mae:0.4492455720901489, smape:43.01265776157379, dtw:not calculated
horizon:11 mse:0.3726996183395386, mae:0.5167332887649536, smape:49.36549961566925, dtw:not calculated
horizon:12 mse:0.30136579275131226, mae:0.44617706537246704, smape:42.821839451789856, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.25961166620254517, mae:0.39872583746910095, smape:38.303279876708984, dtw:not calculated
average metrics: horizon upto:12 mse:0.3067986071109772, mae:0.4441034495830536, smape:42.49645471572876, dtw:not calculated
===============================================================================
average of horizons: mse:0.3067986071109772, mae:0.4441034495830536, smape:42.49645471572876, dtw:not calculated
mean smape over horizons:  42.496453349788986
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4501113891601562
Epoch: 1, Steps: 46 | Train Loss: 0.8680221 Vali Loss: 0.5936109 Test Loss: 0.5936109
Validation loss decreased (inf --> 0.593611).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0953097343444824
Epoch: 2, Steps: 46 | Train Loss: 0.5523475 Vali Loss: 0.4153848 Test Loss: 0.4153848
Validation loss decreased (0.593611 --> 0.415385).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.069305658340454
Epoch: 3, Steps: 46 | Train Loss: 0.4338665 Vali Loss: 0.3519660 Test Loss: 0.3519660
Validation loss decreased (0.415385 --> 0.351966).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.150501012802124
Epoch: 4, Steps: 46 | Train Loss: 0.4036796 Vali Loss: 0.3237412 Test Loss: 0.3237412
Validation loss decreased (0.351966 --> 0.323741).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0735406875610352
Epoch: 5, Steps: 46 | Train Loss: 0.3890256 Vali Loss: 0.3157706 Test Loss: 0.3157706
Validation loss decreased (0.323741 --> 0.315771).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0760669708251953
Epoch: 6, Steps: 46 | Train Loss: 0.4004630 Vali Loss: 0.3068681 Test Loss: 0.3068681
Validation loss decreased (0.315771 --> 0.306868).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.1928963661193848
Epoch: 7, Steps: 46 | Train Loss: 0.3869600 Vali Loss: 0.3031555 Test Loss: 0.3031555
Validation loss decreased (0.306868 --> 0.303156).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0220580101013184
Epoch: 8, Steps: 46 | Train Loss: 0.3849777 Vali Loss: 0.2987809 Test Loss: 0.2987809
Validation loss decreased (0.303156 --> 0.298781).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0478565692901611
Epoch: 9, Steps: 46 | Train Loss: 0.3828723 Vali Loss: 0.2995055 Test Loss: 0.2995055
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0748298168182373
Epoch: 10, Steps: 46 | Train Loss: 0.3901648 Vali Loss: 0.2981330 Test Loss: 0.2981330
Validation loss decreased (0.298781 --> 0.298133).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03392175957560539, mae:0.1437368094921112, smape:14.316491782665253, dtw:not calculated
horizon:2 mse:0.12506504356861115, mae:0.2911154329776764, smape:28.6348819732666, dtw:not calculated
horizon:3 mse:0.528857946395874, mae:0.6278631687164307, smape:59.07718539237976, dtw:not calculated
horizon:4 mse:0.23597301542758942, mae:0.4043557345867157, smape:39.19971287250519, dtw:not calculated
horizon:5 mse:0.2303982675075531, mae:0.3942634165287018, smape:38.195762038230896, dtw:not calculated
horizon:6 mse:0.22733353078365326, mae:0.3979140520095825, smape:38.64341676235199, dtw:not calculated
horizon:7 mse:0.43187153339385986, mae:0.5475346446037292, smape:51.81114077568054, dtw:not calculated
horizon:8 mse:0.27237454056739807, mae:0.4407484829425812, smape:42.56214499473572, dtw:not calculated
horizon:9 mse:0.4224855303764343, mae:0.5361132025718689, smape:50.690603256225586, dtw:not calculated
horizon:10 mse:0.33507436513900757, mae:0.46696949005126953, smape:44.52357888221741, dtw:not calculated
horizon:11 mse:0.430497944355011, mae:0.558140754699707, smape:52.976346015930176, dtw:not calculated
horizon:12 mse:0.3037421703338623, mae:0.44691407680511475, smape:42.87997782230377, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2302582561969757, mae:0.376541405916214, smape:36.34457290172577, dtw:not calculated
average metrics: horizon upto:12 mse:0.2981329560279846, mae:0.43797245621681213, smape:41.95927083492279, dtw:not calculated
===============================================================================
average of horizons: mse:0.2981329560279846, mae:0.43797245621681213, smape:41.95927083492279, dtw:not calculated
mean smape over horizons:  41.95927021404108
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4344873428344727
Epoch: 1, Steps: 46 | Train Loss: 0.8627878 Vali Loss: 0.5785229 Test Loss: 0.5785229
Validation loss decreased (inf --> 0.578523).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.033672571182251
Epoch: 2, Steps: 46 | Train Loss: 0.5415981 Vali Loss: 0.4012449 Test Loss: 0.4012449
Validation loss decreased (0.578523 --> 0.401245).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1753153800964355
Epoch: 3, Steps: 46 | Train Loss: 0.4279527 Vali Loss: 0.3443138 Test Loss: 0.3443138
Validation loss decreased (0.401245 --> 0.344314).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.071413516998291
Epoch: 4, Steps: 46 | Train Loss: 0.3949025 Vali Loss: 0.3187312 Test Loss: 0.3187312
Validation loss decreased (0.344314 --> 0.318731).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0578677654266357
Epoch: 5, Steps: 46 | Train Loss: 0.3817658 Vali Loss: 0.3111642 Test Loss: 0.3111642
Validation loss decreased (0.318731 --> 0.311164).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.005291223526001
Epoch: 6, Steps: 46 | Train Loss: 0.3886966 Vali Loss: 0.3031769 Test Loss: 0.3031769
Validation loss decreased (0.311164 --> 0.303177).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0760083198547363
Epoch: 7, Steps: 46 | Train Loss: 0.3788914 Vali Loss: 0.2994563 Test Loss: 0.2994563
Validation loss decreased (0.303177 --> 0.299456).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0816495418548584
Epoch: 8, Steps: 46 | Train Loss: 0.3792653 Vali Loss: 0.2964754 Test Loss: 0.2964754
Validation loss decreased (0.299456 --> 0.296475).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.069669246673584
Epoch: 9, Steps: 46 | Train Loss: 0.3742912 Vali Loss: 0.2972561 Test Loss: 0.2972561
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0452661514282227
Epoch: 10, Steps: 46 | Train Loss: 0.3829379 Vali Loss: 0.2953725 Test Loss: 0.2953725
Validation loss decreased (0.296475 --> 0.295373).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.038640096783638, mae:0.1533513069152832, smape:15.263718366622925, dtw:not calculated
horizon:2 mse:0.11389703303575516, mae:0.26841288805007935, smape:26.407232880592346, dtw:not calculated
horizon:3 mse:0.33300939202308655, mae:0.5001799464225769, smape:48.12318980693817, dtw:not calculated
horizon:4 mse:0.24216414988040924, mae:0.41331833600997925, smape:40.06776809692383, dtw:not calculated
horizon:5 mse:0.2561972439289093, mae:0.4247909486293793, smape:41.084298491477966, dtw:not calculated
horizon:6 mse:0.23731431365013123, mae:0.4142025113105774, smape:40.216654539108276, dtw:not calculated
horizon:7 mse:0.40501195192337036, mae:0.5329776406288147, smape:50.59003829956055, dtw:not calculated
horizon:8 mse:0.27637845277786255, mae:0.44584017992019653, smape:43.048280477523804, dtw:not calculated
horizon:9 mse:0.4631483554840088, mae:0.5699411630630493, smape:53.74284386634827, dtw:not calculated
horizon:10 mse:0.3701592683792114, mae:0.4903011918067932, smape:46.51338458061218, dtw:not calculated
horizon:11 mse:0.5002176761627197, mae:0.6100426316261292, smape:57.52526521682739, dtw:not calculated
horizon:12 mse:0.3083326816558838, mae:0.45273691415786743, smape:43.429046869277954, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.20353704690933228, mae:0.36237600445747375, smape:35.19380986690521, dtw:not calculated
average metrics: horizon upto:12 mse:0.29537254571914673, mae:0.43967461585998535, smape:42.16764271259308, dtw:not calculated
===============================================================================
average of horizons: mse:0.29537254571914673, mae:0.43967461585998535, smape:42.16764271259308, dtw:not calculated
mean smape over horizons:  42.16764345765114
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3845553398132324
Epoch: 1, Steps: 46 | Train Loss: 0.8414037 Vali Loss: 0.5604250 Test Loss: 0.5604250
Validation loss decreased (inf --> 0.560425).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.941765308380127
Epoch: 2, Steps: 46 | Train Loss: 0.5230151 Vali Loss: 0.3870296 Test Loss: 0.3870296
Validation loss decreased (0.560425 --> 0.387030).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1296050548553467
Epoch: 3, Steps: 46 | Train Loss: 0.4228386 Vali Loss: 0.3350780 Test Loss: 0.3350780
Validation loss decreased (0.387030 --> 0.335078).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0826940536499023
Epoch: 4, Steps: 46 | Train Loss: 0.3929438 Vali Loss: 0.3099994 Test Loss: 0.3099994
Validation loss decreased (0.335078 --> 0.309999).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9183192253112793
Epoch: 5, Steps: 46 | Train Loss: 0.3799570 Vali Loss: 0.3048916 Test Loss: 0.3048916
Validation loss decreased (0.309999 --> 0.304892).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0736401081085205
Epoch: 6, Steps: 46 | Train Loss: 0.3873482 Vali Loss: 0.2978684 Test Loss: 0.2978684
Validation loss decreased (0.304892 --> 0.297868).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0167956352233887
Epoch: 7, Steps: 46 | Train Loss: 0.3748547 Vali Loss: 0.2948330 Test Loss: 0.2948330
Validation loss decreased (0.297868 --> 0.294833).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0516383647918701
Epoch: 8, Steps: 46 | Train Loss: 0.3759075 Vali Loss: 0.2913109 Test Loss: 0.2913109
Validation loss decreased (0.294833 --> 0.291311).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9631845951080322
Epoch: 9, Steps: 46 | Train Loss: 0.3746311 Vali Loss: 0.2920509 Test Loss: 0.2920509
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0620536804199219
Epoch: 10, Steps: 46 | Train Loss: 0.3810149 Vali Loss: 0.2903588 Test Loss: 0.2903588
Validation loss decreased (0.291311 --> 0.290359).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.054066114127635956, mae:0.1806061714887619, smape:17.921195924282074, dtw:not calculated
horizon:2 mse:0.10233963280916214, mae:0.24779370427131653, smape:24.397990107536316, dtw:not calculated
horizon:3 mse:0.25523635745048523, mae:0.42626166343688965, smape:41.28658473491669, dtw:not calculated
horizon:4 mse:0.2372683882713318, mae:0.4068610966205597, smape:39.45864140987396, dtw:not calculated
horizon:5 mse:0.2827715277671814, mae:0.44765087962150574, smape:43.16380321979523, dtw:not calculated
horizon:6 mse:0.2464510053396225, mae:0.42107367515563965, smape:40.81975817680359, dtw:not calculated
horizon:7 mse:0.37403807044029236, mae:0.505513608455658, smape:48.07252287864685, dtw:not calculated
horizon:8 mse:0.25303012132644653, mae:0.42059704661369324, smape:40.6781792640686, dtw:not calculated
horizon:9 mse:0.46341562271118164, mae:0.5708492398262024, smape:53.84072661399841, dtw:not calculated
horizon:10 mse:0.3932415544986725, mae:0.5025632977485657, smape:47.49288856983185, dtw:not calculated
horizon:11 mse:0.5132266283035278, mae:0.6202139258384705, smape:58.43392014503479, dtw:not calculated
horizon:12 mse:0.30922117829322815, mae:0.4545097053050995, smape:43.612462282180786, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.19635552167892456, mae:0.35504117608070374, smape:34.50799882411957, dtw:not calculated
average metrics: horizon upto:12 mse:0.2903588116168976, mae:0.4337078332901001, smape:41.5982186794281, dtw:not calculated
===============================================================================
average of horizons: mse:0.2903588116168976, mae:0.4337078332901001, smape:41.5982186794281, dtw:not calculated
mean smape over horizons:  41.59822277724743
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3692102432250977
Epoch: 1, Steps: 46 | Train Loss: 0.7917343 Vali Loss: 0.9288817 Test Loss: 0.9288817
Validation loss decreased (inf --> 0.928882).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9757959842681885
Epoch: 2, Steps: 46 | Train Loss: 0.6046509 Vali Loss: 0.7019575 Test Loss: 0.7019575
Validation loss decreased (0.928882 --> 0.701957).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.047776222229004
Epoch: 3, Steps: 46 | Train Loss: 0.5174384 Vali Loss: 0.6105912 Test Loss: 0.6105912
Validation loss decreased (0.701957 --> 0.610591).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0547914505004883
Epoch: 4, Steps: 46 | Train Loss: 0.5061160 Vali Loss: 0.5707194 Test Loss: 0.5707194
Validation loss decreased (0.610591 --> 0.570719).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0493857860565186
Epoch: 5, Steps: 46 | Train Loss: 0.4841678 Vali Loss: 0.5542147 Test Loss: 0.5542147
Validation loss decreased (0.570719 --> 0.554215).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9533679485321045
Epoch: 6, Steps: 46 | Train Loss: 0.4730474 Vali Loss: 0.5443795 Test Loss: 0.5443795
Validation loss decreased (0.554215 --> 0.544380).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9290556907653809
Epoch: 7, Steps: 46 | Train Loss: 0.4749049 Vali Loss: 0.5407894 Test Loss: 0.5407894
Validation loss decreased (0.544380 --> 0.540789).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.030454158782959
Epoch: 8, Steps: 46 | Train Loss: 0.4802881 Vali Loss: 0.5358494 Test Loss: 0.5358494
Validation loss decreased (0.540789 --> 0.535849).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0080363750457764
Epoch: 9, Steps: 46 | Train Loss: 0.4919499 Vali Loss: 0.5358315 Test Loss: 0.5358315
Validation loss decreased (0.535849 --> 0.535832).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.022268533706665
Epoch: 10, Steps: 46 | Train Loss: 0.4726649 Vali Loss: 0.5359881 Test Loss: 0.5359881
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06321613490581512, mae:0.1970449984073639, smape:19.53037828207016, dtw:not calculated
horizon:2 mse:0.28799816966056824, mae:0.4575241506099701, smape:44.18719708919525, dtw:not calculated
horizon:3 mse:0.34544897079467773, mae:0.49118733406066895, smape:47.007691860198975, dtw:not calculated
horizon:4 mse:0.3692227900028229, mae:0.4743424654006958, smape:44.94716823101044, dtw:not calculated
horizon:5 mse:0.2824397087097168, mae:0.4354318082332611, smape:41.94574058055878, dtw:not calculated
horizon:6 mse:0.4761446714401245, mae:0.6027339696884155, smape:57.110244035720825, dtw:not calculated
horizon:7 mse:0.4321489930152893, mae:0.5280216932296753, smape:49.70654547214508, dtw:not calculated
horizon:8 mse:0.46460145711898804, mae:0.5369935631752014, smape:50.20710229873657, dtw:not calculated
horizon:9 mse:0.5447202324867249, mae:0.6183174252510071, smape:57.755088806152344, dtw:not calculated
horizon:10 mse:0.8160324692726135, mae:0.7405554056167603, smape:66.8413758277893, dtw:not calculated
horizon:11 mse:1.2004637718200684, mae:0.9592459797859192, smape:84.65428352355957, dtw:not calculated
horizon:12 mse:1.1475400924682617, mae:0.863762378692627, smape:74.90642070770264, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3040784001350403, mae:0.44304412603378296, smape:42.454734444618225, dtw:not calculated
average metrics: horizon upto:12 mse:0.5358314514160156, mae:0.5754300951957703, smape:53.2332718372345, dtw:not calculated
===============================================================================
average of horizons: mse:0.5358314514160156, mae:0.5754300951957703, smape:53.2332718372345, dtw:not calculated
mean smape over horizons:  53.233269726236664
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4823191165924072
Epoch: 1, Steps: 46 | Train Loss: 0.7833878 Vali Loss: 0.9317406 Test Loss: 0.9317406
Validation loss decreased (inf --> 0.931741).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9492847919464111
Epoch: 2, Steps: 46 | Train Loss: 0.5903364 Vali Loss: 0.6979592 Test Loss: 0.6979592
Validation loss decreased (0.931741 --> 0.697959).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0040318965911865
Epoch: 3, Steps: 46 | Train Loss: 0.5024854 Vali Loss: 0.6021202 Test Loss: 0.6021202
Validation loss decreased (0.697959 --> 0.602120).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0105705261230469
Epoch: 4, Steps: 46 | Train Loss: 0.4903154 Vali Loss: 0.5618349 Test Loss: 0.5618349
Validation loss decreased (0.602120 --> 0.561835).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.1405160427093506
Epoch: 5, Steps: 46 | Train Loss: 0.4692548 Vali Loss: 0.5444158 Test Loss: 0.5444158
Validation loss decreased (0.561835 --> 0.544416).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0244567394256592
Epoch: 6, Steps: 46 | Train Loss: 0.4574349 Vali Loss: 0.5344503 Test Loss: 0.5344503
Validation loss decreased (0.544416 --> 0.534450).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0755565166473389
Epoch: 7, Steps: 46 | Train Loss: 0.4622427 Vali Loss: 0.5317228 Test Loss: 0.5317228
Validation loss decreased (0.534450 --> 0.531723).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.1262409687042236
Epoch: 8, Steps: 46 | Train Loss: 0.4630146 Vali Loss: 0.5247192 Test Loss: 0.5247192
Validation loss decreased (0.531723 --> 0.524719).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.1202590465545654
Epoch: 9, Steps: 46 | Train Loss: 0.4729563 Vali Loss: 0.5250375 Test Loss: 0.5250375
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.096463680267334
Epoch: 10, Steps: 46 | Train Loss: 0.4556838 Vali Loss: 0.5270652 Test Loss: 0.5270652
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03953827545046806, mae:0.16122855246067047, smape:16.051502525806427, dtw:not calculated
horizon:2 mse:0.26210537552833557, mae:0.4350632131099701, smape:42.143452167510986, dtw:not calculated
horizon:3 mse:0.33896514773368835, mae:0.4845089316368103, smape:46.40139937400818, dtw:not calculated
horizon:4 mse:0.3447692096233368, mae:0.4568887948989868, smape:43.41014325618744, dtw:not calculated
horizon:5 mse:0.2818576693534851, mae:0.43600842356681824, smape:42.00595319271088, dtw:not calculated
horizon:6 mse:0.474478155374527, mae:0.606111466884613, smape:57.49642252922058, dtw:not calculated
horizon:7 mse:0.4316049814224243, mae:0.5224288105964661, smape:49.12551939487457, dtw:not calculated
horizon:8 mse:0.44884365797042847, mae:0.5281237959861755, smape:49.50003921985626, dtw:not calculated
horizon:9 mse:0.517042338848114, mae:0.5982099175453186, smape:56.01001977920532, dtw:not calculated
horizon:10 mse:0.8009195923805237, mae:0.7336254715919495, smape:66.31801724433899, dtw:not calculated
horizon:11 mse:1.1339845657348633, mae:0.9342109560966492, smape:82.98221826553345, dtw:not calculated
horizon:12 mse:1.222521424293518, mae:0.9037295579910278, smape:78.07386517524719, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2902856469154358, mae:0.4299682378768921, smape:41.25148057937622, dtw:not calculated
average metrics: horizon upto:12 mse:0.52471923828125, mae:0.5666781067848206, smape:52.459877729415894, dtw:not calculated
===============================================================================
average of horizons: mse:0.52471923828125, mae:0.5666781067848206, smape:52.459877729415894, dtw:not calculated
mean smape over horizons:  52.45987934370836
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4222416877746582
Epoch: 1, Steps: 46 | Train Loss: 0.7930927 Vali Loss: 0.9348780 Test Loss: 0.9348780
Validation loss decreased (inf --> 0.934878).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1317055225372314
Epoch: 2, Steps: 46 | Train Loss: 0.5910407 Vali Loss: 0.6924405 Test Loss: 0.6924405
Validation loss decreased (0.934878 --> 0.692441).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0618901252746582
Epoch: 3, Steps: 46 | Train Loss: 0.4984153 Vali Loss: 0.5947415 Test Loss: 0.5947415
Validation loss decreased (0.692441 --> 0.594741).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0309298038482666
Epoch: 4, Steps: 46 | Train Loss: 0.4832535 Vali Loss: 0.5533853 Test Loss: 0.5533853
Validation loss decreased (0.594741 --> 0.553385).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0233945846557617
Epoch: 5, Steps: 46 | Train Loss: 0.4627626 Vali Loss: 0.5334905 Test Loss: 0.5334905
Validation loss decreased (0.553385 --> 0.533490).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1275169849395752
Epoch: 6, Steps: 46 | Train Loss: 0.4532814 Vali Loss: 0.5252796 Test Loss: 0.5252796
Validation loss decreased (0.533490 --> 0.525280).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0897843837738037
Epoch: 7, Steps: 46 | Train Loss: 0.4574544 Vali Loss: 0.5215359 Test Loss: 0.5215359
Validation loss decreased (0.525280 --> 0.521536).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0532615184783936
Epoch: 8, Steps: 46 | Train Loss: 0.4539219 Vali Loss: 0.5138418 Test Loss: 0.5138418
Validation loss decreased (0.521536 --> 0.513842).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.053121566772461
Epoch: 9, Steps: 46 | Train Loss: 0.4648272 Vali Loss: 0.5145942 Test Loss: 0.5145942
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.025989055633545
Epoch: 10, Steps: 46 | Train Loss: 0.4458826 Vali Loss: 0.5180539 Test Loss: 0.5180539
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.028668977320194244, mae:0.1364486664533615, smape:13.610020279884338, dtw:not calculated
horizon:2 mse:0.22774887084960938, mae:0.4038902819156647, smape:39.294350147247314, dtw:not calculated
horizon:3 mse:0.34019994735717773, mae:0.4836917221546173, smape:46.307072043418884, dtw:not calculated
horizon:4 mse:0.34010738134384155, mae:0.4578648805618286, smape:43.54361295700073, dtw:not calculated
horizon:5 mse:0.29420173168182373, mae:0.4496900141239166, smape:43.26896369457245, dtw:not calculated
horizon:6 mse:0.4635522961616516, mae:0.6015984416007996, smape:57.16204643249512, dtw:not calculated
horizon:7 mse:0.4773038923740387, mae:0.5479946136474609, smape:51.209843158721924, dtw:not calculated
horizon:8 mse:0.4427075982093811, mae:0.5248470306396484, smape:49.27590191364288, dtw:not calculated
horizon:9 mse:0.49035292863845825, mae:0.5760846734046936, smape:54.03032302856445, dtw:not calculated
horizon:10 mse:0.7756143808364868, mae:0.7276681065559387, smape:66.03609919548035, dtw:not calculated
horizon:11 mse:1.0635770559310913, mae:0.9041966795921326, smape:80.84149956703186, dtw:not calculated
horizon:12 mse:1.22206711769104, mae:0.9104243516921997, smape:78.83837223052979, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.28241318464279175, mae:0.4221973419189453, smape:40.53100943565369, dtw:not calculated
average metrics: horizon upto:12 mse:0.5138419270515442, mae:0.560366690158844, smape:51.95150375366211, dtw:not calculated
===============================================================================
average of horizons: mse:0.5138419270515442, mae:0.560366690158844, smape:51.95150375366211, dtw:not calculated
mean smape over horizons:  51.95150872071584
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=22, stride=22
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3428051471710205
Epoch: 1, Steps: 46 | Train Loss: 0.8179008 Vali Loss: 0.9343407 Test Loss: 0.9343407
Validation loss decreased (inf --> 0.934341).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.8942794799804688
Epoch: 2, Steps: 46 | Train Loss: 0.5920476 Vali Loss: 0.6914473 Test Loss: 0.6914473
Validation loss decreased (0.934341 --> 0.691447).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0843324661254883
Epoch: 3, Steps: 46 | Train Loss: 0.4979340 Vali Loss: 0.5908340 Test Loss: 0.5908340
Validation loss decreased (0.691447 --> 0.590834).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0797357559204102
Epoch: 4, Steps: 46 | Train Loss: 0.4820617 Vali Loss: 0.5468532 Test Loss: 0.5468532
Validation loss decreased (0.590834 --> 0.546853).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0502934455871582
Epoch: 5, Steps: 46 | Train Loss: 0.4619007 Vali Loss: 0.5272167 Test Loss: 0.5272167
Validation loss decreased (0.546853 --> 0.527217).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.007089614868164
Epoch: 6, Steps: 46 | Train Loss: 0.4537363 Vali Loss: 0.5191100 Test Loss: 0.5191100
Validation loss decreased (0.527217 --> 0.519110).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0855329036712646
Epoch: 7, Steps: 46 | Train Loss: 0.4569140 Vali Loss: 0.5137982 Test Loss: 0.5137982
Validation loss decreased (0.519110 --> 0.513798).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0237157344818115
Epoch: 8, Steps: 46 | Train Loss: 0.4522065 Vali Loss: 0.5066705 Test Loss: 0.5066705
Validation loss decreased (0.513798 --> 0.506671).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0439066886901855
Epoch: 9, Steps: 46 | Train Loss: 0.4638771 Vali Loss: 0.5065200 Test Loss: 0.5065200
Validation loss decreased (0.506671 --> 0.506520).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9967443943023682
Epoch: 10, Steps: 46 | Train Loss: 0.4417782 Vali Loss: 0.5116270 Test Loss: 0.5116270
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0297702644020319, mae:0.13567936420440674, smape:13.526484370231628, dtw:not calculated
horizon:2 mse:0.17098009586334229, mae:0.3495417535305023, smape:34.241825342178345, dtw:not calculated
horizon:3 mse:0.32829153537750244, mae:0.4774796664714813, smape:45.78332602977753, dtw:not calculated
horizon:4 mse:0.3579327166080475, mae:0.4735269248485565, smape:44.96046602725983, dtw:not calculated
horizon:5 mse:0.3092690706253052, mae:0.460946649312973, smape:44.254887104034424, dtw:not calculated
horizon:6 mse:0.46646496653556824, mae:0.6025649309158325, smape:57.21459984779358, dtw:not calculated
horizon:7 mse:0.5341495275497437, mae:0.5855898857116699, smape:54.42078709602356, dtw:not calculated
horizon:8 mse:0.47960248589515686, mae:0.5544466972351074, smape:51.92722678184509, dtw:not calculated
horizon:9 mse:0.4771084487438202, mae:0.5645986795425415, smape:53.00067663192749, dtw:not calculated
horizon:10 mse:0.7417017221450806, mae:0.7182732224464417, smape:65.53582549095154, dtw:not calculated
horizon:11 mse:0.9948847889900208, mae:0.8705351948738098, smape:78.2645583152771, dtw:not calculated
horizon:12 mse:1.1880848407745361, mae:0.8991972804069519, smape:78.1760573387146, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.27711808681488037, mae:0.41662320494651794, smape:39.996930956840515, dtw:not calculated
average metrics: horizon upto:12 mse:0.5065200328826904, mae:0.5576983690261841, smape:51.77556276321411, dtw:not calculated
===============================================================================
average of horizons: mse:0.5065200328826904, mae:0.5576983690261841, smape:51.77556276321411, dtw:not calculated
mean smape over horizons:  51.77556003133456
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3996450901031494
Epoch: 1, Steps: 46 | Train Loss: 0.7635161 Vali Loss: 0.7943372 Test Loss: 0.7943372
Validation loss decreased (inf --> 0.794337).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.082723617553711
Epoch: 2, Steps: 46 | Train Loss: 0.5458649 Vali Loss: 0.5012382 Test Loss: 0.5012382
Validation loss decreased (0.794337 --> 0.501238).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.97737717628479
Epoch: 3, Steps: 46 | Train Loss: 0.4598885 Vali Loss: 0.3979821 Test Loss: 0.3979821
Validation loss decreased (0.501238 --> 0.397982).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0459918975830078
Epoch: 4, Steps: 46 | Train Loss: 0.4089102 Vali Loss: 0.3624263 Test Loss: 0.3624263
Validation loss decreased (0.397982 --> 0.362426).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0684282779693604
Epoch: 5, Steps: 46 | Train Loss: 0.4034957 Vali Loss: 0.3401243 Test Loss: 0.3401243
Validation loss decreased (0.362426 --> 0.340124).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0400071144104004
Epoch: 6, Steps: 46 | Train Loss: 0.4124721 Vali Loss: 0.3338728 Test Loss: 0.3338728
Validation loss decreased (0.340124 --> 0.333873).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0819318294525146
Epoch: 7, Steps: 46 | Train Loss: 0.3864664 Vali Loss: 0.3307538 Test Loss: 0.3307538
Validation loss decreased (0.333873 --> 0.330754).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0513041019439697
Epoch: 8, Steps: 46 | Train Loss: 0.3983138 Vali Loss: 0.3278047 Test Loss: 0.3278047
Validation loss decreased (0.330754 --> 0.327805).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.1226489543914795
Epoch: 9, Steps: 46 | Train Loss: 0.3940482 Vali Loss: 0.3279078 Test Loss: 0.3279078
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.017359733581543
Epoch: 10, Steps: 46 | Train Loss: 0.3888261 Vali Loss: 0.3256737 Test Loss: 0.3256737
Validation loss decreased (0.327805 --> 0.325674).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.054642654955387115, mae:0.17390766739845276, smape:17.23991185426712, dtw:not calculated
horizon:2 mse:0.2598770558834076, mae:0.41876330971717834, smape:40.42714834213257, dtw:not calculated
horizon:3 mse:0.22208832204341888, mae:0.3977357745170593, smape:38.679009675979614, dtw:not calculated
horizon:4 mse:0.21955351531505585, mae:0.37742412090301514, smape:36.60590350627899, dtw:not calculated
horizon:5 mse:0.3033061921596527, mae:0.4607505798339844, smape:44.265738129615784, dtw:not calculated
horizon:6 mse:0.28786489367485046, mae:0.4422711431980133, smape:42.54169166088104, dtw:not calculated
horizon:7 mse:0.4052105247974396, mae:0.5206597447395325, smape:49.27661120891571, dtw:not calculated
horizon:8 mse:0.584887683391571, mae:0.6273974776268005, smape:58.14472436904907, dtw:not calculated
horizon:9 mse:0.5703517198562622, mae:0.6315352320671082, smape:58.822834491729736, dtw:not calculated
horizon:10 mse:0.30217939615249634, mae:0.46718212962150574, smape:45.01682221889496, dtw:not calculated
horizon:11 mse:0.3340281546115875, mae:0.4845736622810364, smape:46.456339955329895, dtw:not calculated
horizon:12 mse:0.36409422755241394, mae:0.5051784515380859, smape:48.25354218482971, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.22455543279647827, mae:0.37847545742988586, smape:36.62656545639038, dtw:not calculated
average metrics: horizon upto:12 mse:0.3256736993789673, mae:0.4589482545852661, smape:43.810856342315674, dtw:not calculated
===============================================================================
average of horizons: mse:0.3256736993789673, mae:0.4589482545852661, smape:43.810856342315674, dtw:not calculated
mean smape over horizons:  43.81085646649202
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.525075912475586
Epoch: 1, Steps: 46 | Train Loss: 0.7881311 Vali Loss: 0.8348679 Test Loss: 0.8348679
Validation loss decreased (inf --> 0.834868).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0666210651397705
Epoch: 2, Steps: 46 | Train Loss: 0.6136044 Vali Loss: 0.5587711 Test Loss: 0.5587711
Validation loss decreased (0.834868 --> 0.558771).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0064060688018799
Epoch: 3, Steps: 46 | Train Loss: 0.5079898 Vali Loss: 0.4625014 Test Loss: 0.4625014
Validation loss decreased (0.558771 --> 0.462501).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9654891490936279
Epoch: 4, Steps: 46 | Train Loss: 0.4636104 Vali Loss: 0.4237444 Test Loss: 0.4237444
Validation loss decreased (0.462501 --> 0.423744).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0146052837371826
Epoch: 5, Steps: 46 | Train Loss: 0.4497551 Vali Loss: 0.4049337 Test Loss: 0.4049337
Validation loss decreased (0.423744 --> 0.404934).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.033773422241211
Epoch: 6, Steps: 46 | Train Loss: 0.4454635 Vali Loss: 0.3988287 Test Loss: 0.3988287
Validation loss decreased (0.404934 --> 0.398829).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.997272253036499
Epoch: 7, Steps: 46 | Train Loss: 0.4549088 Vali Loss: 0.3906667 Test Loss: 0.3906667
Validation loss decreased (0.398829 --> 0.390667).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.999030351638794
Epoch: 8, Steps: 46 | Train Loss: 0.4295305 Vali Loss: 0.3909681 Test Loss: 0.3909681
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0103731155395508
Epoch: 9, Steps: 46 | Train Loss: 0.4565966 Vali Loss: 0.3900930 Test Loss: 0.3900930
Validation loss decreased (0.390667 --> 0.390093).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0780220031738281
Epoch: 10, Steps: 46 | Train Loss: 0.4311421 Vali Loss: 0.3898948 Test Loss: 0.3898948
Validation loss decreased (0.390093 --> 0.389895).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0919211283326149, mae:0.23785793781280518, smape:23.461201786994934, dtw:not calculated
horizon:2 mse:0.2412116527557373, mae:0.40212827920913696, smape:38.92751336097717, dtw:not calculated
horizon:3 mse:0.18408095836639404, mae:0.3344087600708008, smape:32.50051736831665, dtw:not calculated
horizon:4 mse:0.4274541139602661, mae:0.5680649876594543, smape:54.084670543670654, dtw:not calculated
horizon:5 mse:0.43594980239868164, mae:0.5620246529579163, smape:53.332704305648804, dtw:not calculated
horizon:6 mse:0.3926079571247101, mae:0.5082095265388489, smape:48.15029799938202, dtw:not calculated
horizon:7 mse:0.4702098071575165, mae:0.5502718687057495, smape:51.50841474533081, dtw:not calculated
horizon:8 mse:0.5770105123519897, mae:0.6517534255981445, smape:60.947299003601074, dtw:not calculated
horizon:9 mse:0.5832606554031372, mae:0.6290361881256104, smape:58.31901431083679, dtw:not calculated
horizon:10 mse:0.4235419034957886, mae:0.5660368800163269, smape:53.90228629112244, dtw:not calculated
horizon:11 mse:0.43437445163726807, mae:0.5529930591583252, smape:52.37709879875183, dtw:not calculated
horizon:12 mse:0.4171155095100403, mae:0.5540075898170471, smape:52.733200788497925, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2955375909805298, mae:0.4354490041732788, smape:41.7428195476532, dtw:not calculated
average metrics: horizon upto:12 mse:0.38989484310150146, mae:0.5097327828407288, smape:48.353683948516846, dtw:not calculated
===============================================================================
average of horizons: mse:0.38989484310150146, mae:0.5097327828407288, smape:48.353683948516846, dtw:not calculated
mean smape over horizons:  48.35368494192759
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4704535007476807
Epoch: 1, Steps: 46 | Train Loss: 0.7376719 Vali Loss: 0.7959616 Test Loss: 0.7959616
Validation loss decreased (inf --> 0.795962).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0132994651794434
Epoch: 2, Steps: 46 | Train Loss: 0.5578554 Vali Loss: 0.5601142 Test Loss: 0.5601142
Validation loss decreased (0.795962 --> 0.560114).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.9946117401123047
Epoch: 3, Steps: 46 | Train Loss: 0.4948802 Vali Loss: 0.4660858 Test Loss: 0.4660858
Validation loss decreased (0.560114 --> 0.466086).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.038257360458374
Epoch: 4, Steps: 46 | Train Loss: 0.4498104 Vali Loss: 0.4253886 Test Loss: 0.4253886
Validation loss decreased (0.466086 --> 0.425389).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9745490550994873
Epoch: 5, Steps: 46 | Train Loss: 0.4539290 Vali Loss: 0.4045943 Test Loss: 0.4045943
Validation loss decreased (0.425389 --> 0.404594).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0607483386993408
Epoch: 6, Steps: 46 | Train Loss: 0.4459118 Vali Loss: 0.3963298 Test Loss: 0.3963298
Validation loss decreased (0.404594 --> 0.396330).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.031935691833496
Epoch: 7, Steps: 46 | Train Loss: 0.4413467 Vali Loss: 0.3921845 Test Loss: 0.3921845
Validation loss decreased (0.396330 --> 0.392184).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.9453914165496826
Epoch: 8, Steps: 46 | Train Loss: 0.4237908 Vali Loss: 0.3920857 Test Loss: 0.3920857
Validation loss decreased (0.392184 --> 0.392086).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.011240005493164
Epoch: 9, Steps: 46 | Train Loss: 0.4266444 Vali Loss: 0.3878808 Test Loss: 0.3878808
Validation loss decreased (0.392086 --> 0.387881).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0397617816925049
Epoch: 10, Steps: 46 | Train Loss: 0.4453417 Vali Loss: 0.3882956 Test Loss: 0.3882956
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04584111273288727, mae:0.16488562524318695, smape:16.388793289661407, dtw:not calculated
horizon:2 mse:0.37654587626457214, mae:0.52559894323349, smape:50.27732253074646, dtw:not calculated
horizon:3 mse:0.2473485916852951, mae:0.416027694940567, smape:40.292516350746155, dtw:not calculated
horizon:4 mse:0.3922445774078369, mae:0.5402392745018005, smape:51.574742794036865, dtw:not calculated
horizon:5 mse:0.20195938646793365, mae:0.36607810854911804, smape:35.61748564243317, dtw:not calculated
horizon:6 mse:0.4301702082157135, mae:0.5525095462799072, smape:52.37135291099548, dtw:not calculated
horizon:7 mse:0.3425762355327606, mae:0.4641849398612976, smape:44.18310523033142, dtw:not calculated
horizon:8 mse:0.5208323001861572, mae:0.5921512842178345, smape:55.26978373527527, dtw:not calculated
horizon:9 mse:0.3268471956253052, mae:0.47417211532592773, smape:45.43306827545166, dtw:not calculated
horizon:10 mse:0.7671288847923279, mae:0.7246566414833069, smape:65.93831777572632, dtw:not calculated
horizon:11 mse:0.48176121711730957, mae:0.5772626399993896, smape:54.21507954597473, dtw:not calculated
horizon:12 mse:0.5213138461112976, mae:0.5855535268783569, smape:54.60291504859924, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.28235164284706116, mae:0.42755651473999023, smape:41.08703434467316, dtw:not calculated
average metrics: horizon upto:12 mse:0.38788077235221863, mae:0.4986100494861603, smape:47.180378437042236, dtw:not calculated
===============================================================================
average of horizons: mse:0.38788077235221863, mae:0.4986100494861603, smape:47.180378437042236, dtw:not calculated
mean smape over horizons:  47.18037359416485
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5451381206512451
Epoch: 1, Steps: 46 | Train Loss: 0.7253215 Vali Loss: 0.8397945 Test Loss: 0.8397945
Validation loss decreased (inf --> 0.839795).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0396804809570312
Epoch: 2, Steps: 46 | Train Loss: 0.5567076 Vali Loss: 0.6045406 Test Loss: 0.6045406
Validation loss decreased (0.839795 --> 0.604541).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0509183406829834
Epoch: 3, Steps: 46 | Train Loss: 0.4942511 Vali Loss: 0.5119265 Test Loss: 0.5119265
Validation loss decreased (0.604541 --> 0.511927).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0867650508880615
Epoch: 4, Steps: 46 | Train Loss: 0.4525048 Vali Loss: 0.4668461 Test Loss: 0.4668461
Validation loss decreased (0.511927 --> 0.466846).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0367975234985352
Epoch: 5, Steps: 46 | Train Loss: 0.4548467 Vali Loss: 0.4466751 Test Loss: 0.4466751
Validation loss decreased (0.466846 --> 0.446675).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0495994091033936
Epoch: 6, Steps: 46 | Train Loss: 0.4488635 Vali Loss: 0.4380536 Test Loss: 0.4380536
Validation loss decreased (0.446675 --> 0.438054).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0062730312347412
Epoch: 7, Steps: 46 | Train Loss: 0.4360428 Vali Loss: 0.4350391 Test Loss: 0.4350391
Validation loss decreased (0.438054 --> 0.435039).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.1085388660430908
Epoch: 8, Steps: 46 | Train Loss: 0.4268006 Vali Loss: 0.4343829 Test Loss: 0.4343829
Validation loss decreased (0.435039 --> 0.434383).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0345754623413086
Epoch: 9, Steps: 46 | Train Loss: 0.4337228 Vali Loss: 0.4298205 Test Loss: 0.4298205
Validation loss decreased (0.434383 --> 0.429820).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1438343524932861
Epoch: 10, Steps: 46 | Train Loss: 0.4475654 Vali Loss: 0.4294908 Test Loss: 0.4294908
Validation loss decreased (0.429820 --> 0.429491).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.034328658133745193, mae:0.1393556296825409, smape:13.871659338474274, dtw:not calculated
horizon:2 mse:0.34551024436950684, mae:0.5029165744781494, smape:48.28745126724243, dtw:not calculated
horizon:3 mse:0.24700206518173218, mae:0.42068958282470703, smape:40.800273418426514, dtw:not calculated
horizon:4 mse:0.38713744282722473, mae:0.5301061868667603, smape:50.55333375930786, dtw:not calculated
horizon:5 mse:0.21167919039726257, mae:0.37849757075309753, smape:36.81676387786865, dtw:not calculated
horizon:6 mse:0.49159640073776245, mae:0.5938572883605957, smape:55.96340298652649, dtw:not calculated
horizon:7 mse:0.4284021854400635, mae:0.5239779949188232, smape:49.346160888671875, dtw:not calculated
horizon:8 mse:0.6129164695739746, mae:0.6541972160339355, smape:60.61095595359802, dtw:not calculated
horizon:9 mse:0.3290582597255707, mae:0.480650931596756, smape:46.07400298118591, dtw:not calculated
horizon:10 mse:0.9373610615730286, mae:0.7981253266334534, smape:71.26190066337585, dtw:not calculated
horizon:11 mse:0.5330332517623901, mae:0.6119503378868103, smape:57.2088360786438, dtw:not calculated
horizon:12 mse:0.595863938331604, mae:0.6411236524581909, smape:59.45354104042053, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.28620898723602295, mae:0.42757049202919006, smape:41.04881286621094, dtw:not calculated
average metrics: horizon upto:12 mse:0.42949074506759644, mae:0.5229540467262268, smape:49.18735921382904, dtw:not calculated
===============================================================================
average of horizons: mse:0.42949074506759644, mae:0.5229540467262268, smape:49.18735921382904, dtw:not calculated
mean smape over horizons:  49.187356854478516
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.46462082862854
Epoch: 1, Steps: 46 | Train Loss: 0.7049046 Vali Loss: 0.8496647 Test Loss: 0.8496647
Validation loss decreased (inf --> 0.849665).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.000429391860962
Epoch: 2, Steps: 46 | Train Loss: 0.5427812 Vali Loss: 0.6130540 Test Loss: 0.6130540
Validation loss decreased (0.849665 --> 0.613054).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.007932186126709
Epoch: 3, Steps: 46 | Train Loss: 0.4811352 Vali Loss: 0.5234973 Test Loss: 0.5234973
Validation loss decreased (0.613054 --> 0.523497).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.047074317932129
Epoch: 4, Steps: 46 | Train Loss: 0.4438008 Vali Loss: 0.4805006 Test Loss: 0.4805006
Validation loss decreased (0.523497 --> 0.480501).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9508938789367676
Epoch: 5, Steps: 46 | Train Loss: 0.4486582 Vali Loss: 0.4610955 Test Loss: 0.4610955
Validation loss decreased (0.480501 --> 0.461096).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9865672588348389
Epoch: 6, Steps: 46 | Train Loss: 0.4436799 Vali Loss: 0.4535937 Test Loss: 0.4535937
Validation loss decreased (0.461096 --> 0.453594).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.083399772644043
Epoch: 7, Steps: 46 | Train Loss: 0.4293561 Vali Loss: 0.4491812 Test Loss: 0.4491812
Validation loss decreased (0.453594 --> 0.449181).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.1100385189056396
Epoch: 8, Steps: 46 | Train Loss: 0.4213824 Vali Loss: 0.4500044 Test Loss: 0.4500044
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.056210994720459
Epoch: 9, Steps: 46 | Train Loss: 0.4281760 Vali Loss: 0.4437624 Test Loss: 0.4437624
Validation loss decreased (0.449181 --> 0.443762).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9971306324005127
Epoch: 10, Steps: 46 | Train Loss: 0.4412842 Vali Loss: 0.4426360 Test Loss: 0.4426360
Validation loss decreased (0.443762 --> 0.442636).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.029365623369812965, mae:0.12826281785964966, smape:12.779051065444946, dtw:not calculated
horizon:2 mse:0.3501928150653839, mae:0.5084084868431091, smape:48.797017335891724, dtw:not calculated
horizon:3 mse:0.22041098773479462, mae:0.4023006558418274, smape:39.178869128227234, dtw:not calculated
horizon:4 mse:0.3293290138244629, mae:0.48078298568725586, smape:46.0917204618454, dtw:not calculated
horizon:5 mse:0.22940507531166077, mae:0.39723852276802063, smape:38.55910301208496, dtw:not calculated
horizon:6 mse:0.47358468174934387, mae:0.5905925035476685, smape:55.8677613735199, dtw:not calculated
horizon:7 mse:0.49854370951652527, mae:0.5724749565124512, smape:53.55321168899536, dtw:not calculated
horizon:8 mse:0.6398645639419556, mae:0.6723568439483643, smape:62.171030044555664, dtw:not calculated
horizon:9 mse:0.3380216360092163, mae:0.4907230734825134, smape:47.01180458068848, dtw:not calculated
horizon:10 mse:1.0203633308410645, mae:0.8394927382469177, smape:74.44981336593628, dtw:not calculated
horizon:11 mse:0.5543693900108337, mae:0.6254569888114929, smape:58.3552360534668, dtw:not calculated
horizon:12 mse:0.6281813979148865, mae:0.6575525999069214, smape:60.72772741317749, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.27204805612564087, mae:0.4179309606552124, smape:40.212252736091614, dtw:not calculated
average metrics: horizon upto:12 mse:0.44263601303100586, mae:0.5304703116416931, smape:49.79519248008728, dtw:not calculated
===============================================================================
average of horizons: mse:0.44263601303100586, mae:0.5304703116416931, smape:49.79519248008728, dtw:not calculated
mean smape over horizons:  49.79519546031952
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3786191940307617
Epoch: 1, Steps: 46 | Train Loss: 0.7935818 Vali Loss: 0.7802010 Test Loss: 0.7802010
Validation loss decreased (inf --> 0.780201).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9746811389923096
Epoch: 2, Steps: 46 | Train Loss: 0.5967013 Vali Loss: 0.5669271 Test Loss: 0.5669271
Validation loss decreased (0.780201 --> 0.566927).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.957763671875
Epoch: 3, Steps: 46 | Train Loss: 0.5345114 Vali Loss: 0.4859574 Test Loss: 0.4859574
Validation loss decreased (0.566927 --> 0.485957).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0377774238586426
Epoch: 4, Steps: 46 | Train Loss: 0.4917030 Vali Loss: 0.4498262 Test Loss: 0.4498262
Validation loss decreased (0.485957 --> 0.449826).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.060959815979004
Epoch: 5, Steps: 46 | Train Loss: 0.4739855 Vali Loss: 0.4317650 Test Loss: 0.4317650
Validation loss decreased (0.449826 --> 0.431765).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1040923595428467
Epoch: 6, Steps: 46 | Train Loss: 0.4642611 Vali Loss: 0.4228083 Test Loss: 0.4228083
Validation loss decreased (0.431765 --> 0.422808).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.1175575256347656
Epoch: 7, Steps: 46 | Train Loss: 0.4727219 Vali Loss: 0.4177995 Test Loss: 0.4177995
Validation loss decreased (0.422808 --> 0.417799).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0089004039764404
Epoch: 8, Steps: 46 | Train Loss: 0.4649119 Vali Loss: 0.4186289 Test Loss: 0.4186289
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0008254051208496
Epoch: 9, Steps: 46 | Train Loss: 0.4748930 Vali Loss: 0.4157469 Test Loss: 0.4157469
Validation loss decreased (0.417799 --> 0.415747).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.045591115951538
Epoch: 10, Steps: 46 | Train Loss: 0.4787106 Vali Loss: 0.4155761 Test Loss: 0.4155761
Validation loss decreased (0.415747 --> 0.415576).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.06792662292718887, mae:0.19820567965507507, smape:19.623352587223053, dtw:not calculated
horizon:2 mse:0.5227822661399841, mae:0.61299067735672, smape:57.53421187400818, dtw:not calculated
horizon:3 mse:0.41539156436920166, mae:0.5510837435722351, smape:52.426451444625854, dtw:not calculated
horizon:4 mse:0.2990604043006897, mae:0.46416547894477844, smape:44.709476828575134, dtw:not calculated
horizon:5 mse:0.3898046314716339, mae:0.5205536484718323, smape:49.50967729091644, dtw:not calculated
horizon:6 mse:0.4792455732822418, mae:0.5511575937271118, smape:51.47849917411804, dtw:not calculated
horizon:7 mse:0.37722575664520264, mae:0.5145115852355957, smape:49.034520983695984, dtw:not calculated
horizon:8 mse:0.38468775153160095, mae:0.5176093578338623, smape:49.27908182144165, dtw:not calculated
horizon:9 mse:0.6078534126281738, mae:0.6586834192276001, smape:61.13786697387695, dtw:not calculated
horizon:10 mse:0.3879876732826233, mae:0.5090388655662537, smape:48.29525649547577, dtw:not calculated
horizon:11 mse:0.6662399172782898, mae:0.679475724697113, smape:62.53849267959595, dtw:not calculated
horizon:12 mse:0.3887075185775757, mae:0.4986771047115326, smape:47.21971154212952, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.36236852407455444, mae:0.4830261766910553, smape:45.88027894496918, dtw:not calculated
average metrics: horizon upto:12 mse:0.41557610034942627, mae:0.5230127573013306, smape:49.39888119697571, dtw:not calculated
===============================================================================
average of horizons: mse:0.41557610034942627, mae:0.5230127573013306, smape:49.39888119697571, dtw:not calculated
mean smape over horizons:  49.39888330797354
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5053939819335938
Epoch: 1, Steps: 46 | Train Loss: 0.7846096 Vali Loss: 0.8108410 Test Loss: 0.8108410
Validation loss decreased (inf --> 0.810841).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.052431344985962
Epoch: 2, Steps: 46 | Train Loss: 0.5960605 Vali Loss: 0.6004142 Test Loss: 0.6004142
Validation loss decreased (0.810841 --> 0.600414).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.963749885559082
Epoch: 3, Steps: 46 | Train Loss: 0.5376217 Vali Loss: 0.5169708 Test Loss: 0.5169708
Validation loss decreased (0.600414 --> 0.516971).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1242337226867676
Epoch: 4, Steps: 46 | Train Loss: 0.4944692 Vali Loss: 0.4782974 Test Loss: 0.4782974
Validation loss decreased (0.516971 --> 0.478297).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0430908203125
Epoch: 5, Steps: 46 | Train Loss: 0.4736052 Vali Loss: 0.4607870 Test Loss: 0.4607870
Validation loss decreased (0.478297 --> 0.460787).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.057448387145996
Epoch: 6, Steps: 46 | Train Loss: 0.4665578 Vali Loss: 0.4508571 Test Loss: 0.4508571
Validation loss decreased (0.460787 --> 0.450857).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0686373710632324
Epoch: 7, Steps: 46 | Train Loss: 0.4726482 Vali Loss: 0.4461239 Test Loss: 0.4461239
Validation loss decreased (0.450857 --> 0.446124).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0561270713806152
Epoch: 8, Steps: 46 | Train Loss: 0.4664624 Vali Loss: 0.4461851 Test Loss: 0.4461851
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0064327716827393
Epoch: 9, Steps: 46 | Train Loss: 0.4752964 Vali Loss: 0.4446937 Test Loss: 0.4446937
Validation loss decreased (0.446124 --> 0.444694).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0912165641784668
Epoch: 10, Steps: 46 | Train Loss: 0.4808964 Vali Loss: 0.4431538 Test Loss: 0.4431538
Validation loss decreased (0.444694 --> 0.443154).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.056098572909832, mae:0.177989661693573, smape:17.653888463974, dtw:not calculated
horizon:2 mse:0.5163199305534363, mae:0.6063517928123474, smape:56.945204734802246, dtw:not calculated
horizon:3 mse:0.38123631477355957, mae:0.5263766050338745, smape:50.26758313179016, dtw:not calculated
horizon:4 mse:0.29543614387512207, mae:0.45024028420448303, smape:43.30197870731354, dtw:not calculated
horizon:5 mse:0.3989829123020172, mae:0.5243117809295654, smape:49.7728705406189, dtw:not calculated
horizon:6 mse:0.5673693418502808, mae:0.5972943305969238, smape:55.133336782455444, dtw:not calculated
horizon:7 mse:0.40743348002433777, mae:0.5386771559715271, smape:51.19572877883911, dtw:not calculated
horizon:8 mse:0.40977373719215393, mae:0.5414605736732483, smape:51.46886706352234, dtw:not calculated
horizon:9 mse:0.6898307800292969, mae:0.6970614194869995, smape:64.04712200164795, dtw:not calculated
horizon:10 mse:0.4429744780063629, mae:0.5472816228866577, smape:51.57439112663269, dtw:not calculated
horizon:11 mse:0.7177078723907471, mae:0.7148028612136841, smape:65.59898257255554, dtw:not calculated
horizon:12 mse:0.4346822500228882, mae:0.5321389436721802, smape:50.13100504875183, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.36924052238464355, mae:0.4804273843765259, smape:45.512476563453674, dtw:not calculated
average metrics: horizon upto:12 mse:0.44315385818481445, mae:0.5378322601318359, smape:50.590914487838745, dtw:not calculated
===============================================================================
average of horizons: mse:0.44315385818481445, mae:0.5378322601318359, smape:50.590914487838745, dtw:not calculated
mean smape over horizons:  50.59091324607531
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3605430126190186
Epoch: 1, Steps: 46 | Train Loss: 0.7682064 Vali Loss: 0.8400371 Test Loss: 0.8400371
Validation loss decreased (inf --> 0.840037).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.9965171813964844
Epoch: 2, Steps: 46 | Train Loss: 0.5895455 Vali Loss: 0.6246296 Test Loss: 0.6246296
Validation loss decreased (0.840037 --> 0.624630).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1085431575775146
Epoch: 3, Steps: 46 | Train Loss: 0.5342186 Vali Loss: 0.5381047 Test Loss: 0.5381047
Validation loss decreased (0.624630 --> 0.538105).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0758771896362305
Epoch: 4, Steps: 46 | Train Loss: 0.4911560 Vali Loss: 0.4964770 Test Loss: 0.4964770
Validation loss decreased (0.538105 --> 0.496477).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.092754602432251
Epoch: 5, Steps: 46 | Train Loss: 0.4676359 Vali Loss: 0.4784776 Test Loss: 0.4784776
Validation loss decreased (0.496477 --> 0.478478).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0733387470245361
Epoch: 6, Steps: 46 | Train Loss: 0.4628894 Vali Loss: 0.4671363 Test Loss: 0.4671363
Validation loss decreased (0.478478 --> 0.467136).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.036656141281128
Epoch: 7, Steps: 46 | Train Loss: 0.4674568 Vali Loss: 0.4643784 Test Loss: 0.4643784
Validation loss decreased (0.467136 --> 0.464378).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.029869794845581
Epoch: 8, Steps: 46 | Train Loss: 0.4617154 Vali Loss: 0.4643014 Test Loss: 0.4643014
Validation loss decreased (0.464378 --> 0.464301).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.078463077545166
Epoch: 9, Steps: 46 | Train Loss: 0.4711231 Vali Loss: 0.4627172 Test Loss: 0.4627172
Validation loss decreased (0.464301 --> 0.462717).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0193049907684326
Epoch: 10, Steps: 46 | Train Loss: 0.4749393 Vali Loss: 0.4605419 Test Loss: 0.4605419
Validation loss decreased (0.462717 --> 0.460542).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.050756003707647324, mae:0.1696564108133316, smape:16.840752959251404, dtw:not calculated
horizon:2 mse:0.47405242919921875, mae:0.5812607407569885, smape:54.88351583480835, dtw:not calculated
horizon:3 mse:0.332714319229126, mae:0.48636752367019653, smape:46.669626235961914, dtw:not calculated
horizon:4 mse:0.2817407250404358, mae:0.4307456314563751, smape:41.440874338150024, dtw:not calculated
horizon:5 mse:0.3898947238922119, mae:0.5185657739639282, smape:49.29104447364807, dtw:not calculated
horizon:6 mse:0.6100867390632629, mae:0.6170203685760498, smape:56.64117932319641, dtw:not calculated
horizon:7 mse:0.4081438481807709, mae:0.5391982197761536, smape:51.244962215423584, dtw:not calculated
horizon:8 mse:0.4498840272426605, mae:0.5688068270683289, smape:53.833627700805664, dtw:not calculated
horizon:9 mse:0.7802229523658752, mae:0.7427738904953003, smape:67.62365698814392, dtw:not calculated
horizon:10 mse:0.5078149437904358, mae:0.587739109992981, smape:54.96648550033569, dtw:not calculated
horizon:11 mse:0.7805806994438171, mae:0.7518007755279541, smape:68.65814328193665, dtw:not calculated
horizon:12 mse:0.46061044931411743, mae:0.5451176166534424, smape:51.13576650619507, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.35654082894325256, mae:0.4672694206237793, smape:44.29449737071991, dtw:not calculated
average metrics: horizon upto:12 mse:0.46054181456565857, mae:0.5449211001396179, smape:51.102471351623535, dtw:not calculated
===============================================================================
average of horizons: mse:0.46054181456565857, mae:0.5449211001396179, smape:51.102471351623535, dtw:not calculated
mean smape over horizons:  51.10246961315473
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.43182373046875
Epoch: 1, Steps: 46 | Train Loss: 0.7492521 Vali Loss: 0.8545524 Test Loss: 0.8545524
Validation loss decreased (inf --> 0.854552).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0858120918273926
Epoch: 2, Steps: 46 | Train Loss: 0.5787754 Vali Loss: 0.6265733 Test Loss: 0.6265733
Validation loss decreased (0.854552 --> 0.626573).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0284736156463623
Epoch: 3, Steps: 46 | Train Loss: 0.5204877 Vali Loss: 0.5352321 Test Loss: 0.5352321
Validation loss decreased (0.626573 --> 0.535232).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.015347957611084
Epoch: 4, Steps: 46 | Train Loss: 0.4800054 Vali Loss: 0.4915588 Test Loss: 0.4915588
Validation loss decreased (0.535232 --> 0.491559).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0053439140319824
Epoch: 5, Steps: 46 | Train Loss: 0.4558915 Vali Loss: 0.4728393 Test Loss: 0.4728393
Validation loss decreased (0.491559 --> 0.472839).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.108123779296875
Epoch: 6, Steps: 46 | Train Loss: 0.4545359 Vali Loss: 0.4612520 Test Loss: 0.4612520
Validation loss decreased (0.472839 --> 0.461252).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0711119174957275
Epoch: 7, Steps: 46 | Train Loss: 0.4564265 Vali Loss: 0.4587521 Test Loss: 0.4587521
Validation loss decreased (0.461252 --> 0.458752).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 0.986600399017334
Epoch: 8, Steps: 46 | Train Loss: 0.4520986 Vali Loss: 0.4583282 Test Loss: 0.4583282
Validation loss decreased (0.458752 --> 0.458328).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0853829383850098
Epoch: 9, Steps: 46 | Train Loss: 0.4556926 Vali Loss: 0.4565823 Test Loss: 0.4565823
Validation loss decreased (0.458328 --> 0.456582).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0347201824188232
Epoch: 10, Steps: 46 | Train Loss: 0.4625263 Vali Loss: 0.4554057 Test Loss: 0.4554057
Validation loss decreased (0.456582 --> 0.455406).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.042732879519462585, mae:0.159014493227005, smape:15.814599394798279, dtw:not calculated
horizon:2 mse:0.4097083806991577, mae:0.5401511788368225, smape:51.4030396938324, dtw:not calculated
horizon:3 mse:0.2972760796546936, mae:0.45397958159446716, smape:43.691039085388184, dtw:not calculated
horizon:4 mse:0.2591545283794403, mae:0.40605399012565613, smape:39.139536023139954, dtw:not calculated
horizon:5 mse:0.37115272879600525, mae:0.5044041872024536, smape:48.05832803249359, dtw:not calculated
horizon:6 mse:0.58743816614151, mae:0.6072142720222473, smape:55.94673752784729, dtw:not calculated
horizon:7 mse:0.40280646085739136, mae:0.5335648059844971, smape:50.72439908981323, dtw:not calculated
horizon:8 mse:0.4718056917190552, mae:0.5826222896575928, smape:55.0024151802063, dtw:not calculated
horizon:9 mse:0.8409772515296936, mae:0.7748245596885681, smape:70.21089196205139, dtw:not calculated
horizon:10 mse:0.5342702865600586, mae:0.6033481359481812, smape:56.26155138015747, dtw:not calculated
horizon:11 mse:0.7812747955322266, mae:0.7546783089637756, smape:68.93362402915955, dtw:not calculated
horizon:12 mse:0.4662712812423706, mae:0.5403185486793518, smape:50.539785623550415, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.32791048288345337, mae:0.44513627886772156, smape:42.34221279621124, dtw:not calculated
average metrics: horizon upto:12 mse:0.45540568232536316, mae:0.5383478403091431, smape:50.47715902328491, dtw:not calculated
===============================================================================
average of horizons: mse:0.45540568232536316, mae:0.5383478403091431, smape:50.47715902328491, dtw:not calculated
mean smape over horizons:  50.477162251869835
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=22
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.56923508644104
Epoch: 1, Steps: 46 | Train Loss: 0.7403046 Vali Loss: 0.8572897 Test Loss: 0.8572897
Validation loss decreased (inf --> 0.857290).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1071908473968506
Epoch: 2, Steps: 46 | Train Loss: 0.5743199 Vali Loss: 0.6182150 Test Loss: 0.6182150
Validation loss decreased (0.857290 --> 0.618215).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.09898042678833
Epoch: 3, Steps: 46 | Train Loss: 0.5133780 Vali Loss: 0.5231602 Test Loss: 0.5231602
Validation loss decreased (0.618215 --> 0.523160).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0487451553344727
Epoch: 4, Steps: 46 | Train Loss: 0.4730711 Vali Loss: 0.4793127 Test Loss: 0.4793127
Validation loss decreased (0.523160 --> 0.479313).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0971879959106445
Epoch: 5, Steps: 46 | Train Loss: 0.4508104 Vali Loss: 0.4602987 Test Loss: 0.4602987
Validation loss decreased (0.479313 --> 0.460299).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.093186616897583
Epoch: 6, Steps: 46 | Train Loss: 0.4490741 Vali Loss: 0.4481564 Test Loss: 0.4481564
Validation loss decreased (0.460299 --> 0.448156).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.067155122756958
Epoch: 7, Steps: 46 | Train Loss: 0.4499345 Vali Loss: 0.4465136 Test Loss: 0.4465136
Validation loss decreased (0.448156 --> 0.446514).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0773093700408936
Epoch: 8, Steps: 46 | Train Loss: 0.4459266 Vali Loss: 0.4453508 Test Loss: 0.4453508
Validation loss decreased (0.446514 --> 0.445351).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0374205112457275
Epoch: 9, Steps: 46 | Train Loss: 0.4495077 Vali Loss: 0.4443629 Test Loss: 0.4443629
Validation loss decreased (0.445351 --> 0.444363).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0707721710205078
Epoch: 10, Steps: 46 | Train Loss: 0.4545319 Vali Loss: 0.4423145 Test Loss: 0.4423145
Validation loss decreased (0.444363 --> 0.442314).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.038959991186857224, mae:0.1511267125606537, smape:15.03804326057434, dtw:not calculated
horizon:2 mse:0.36615338921546936, mae:0.5108304619789124, smape:48.8663524389267, dtw:not calculated
horizon:3 mse:0.2928178012371063, mae:0.44450822472572327, smape:42.749106884002686, dtw:not calculated
horizon:4 mse:0.2604103088378906, mae:0.4057392179965973, smape:39.09294307231903, dtw:not calculated
horizon:5 mse:0.35523152351379395, mae:0.49036097526550293, smape:46.80076837539673, dtw:not calculated
horizon:6 mse:0.5645560622215271, mae:0.5963749885559082, smape:55.136847496032715, dtw:not calculated
horizon:7 mse:0.3832668364048004, mae:0.518425464630127, smape:49.38176870346069, dtw:not calculated
horizon:8 mse:0.4481777250766754, mae:0.5668557286262512, smape:53.63314151763916, dtw:not calculated
horizon:9 mse:0.8623942136764526, mae:0.7862517237663269, smape:71.1907684803009, dtw:not calculated
horizon:10 mse:0.5575318336486816, mae:0.6172544360160828, smape:57.42315649986267, dtw:not calculated
horizon:11 mse:0.7175956964492798, mae:0.7198992967605591, smape:66.11767411231995, dtw:not calculated
horizon:12 mse:0.4606780707836151, mae:0.5314139127731323, smape:49.67077970504761, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3130215108394623, mae:0.4331567585468292, smape:41.28067493438721, dtw:not calculated
average metrics: horizon upto:12 mse:0.4423144459724426, mae:0.528253436088562, smape:49.591779708862305, dtw:not calculated
===============================================================================
average of horizons: mse:0.4423144459724426, mae:0.528253436088562, smape:49.591779708862305, dtw:not calculated
mean smape over horizons:  49.59177921215693
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=24, stride=24
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3336598873138428
Epoch: 1, Steps: 46 | Train Loss: 0.7335731 Vali Loss: 0.8626744 Test Loss: 0.8626744
Validation loss decreased (inf --> 0.862674).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.004035234451294
Epoch: 2, Steps: 46 | Train Loss: 0.5773476 Vali Loss: 0.6124736 Test Loss: 0.6124736
Validation loss decreased (0.862674 --> 0.612474).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0332043170928955
Epoch: 3, Steps: 46 | Train Loss: 0.5126265 Vali Loss: 0.5125029 Test Loss: 0.5125029
Validation loss decreased (0.612474 --> 0.512503).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1435153484344482
Epoch: 4, Steps: 46 | Train Loss: 0.4722653 Vali Loss: 0.4686989 Test Loss: 0.4686989
Validation loss decreased (0.512503 --> 0.468699).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.048560619354248
Epoch: 5, Steps: 46 | Train Loss: 0.4522222 Vali Loss: 0.4490306 Test Loss: 0.4490306
Validation loss decreased (0.468699 --> 0.449031).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 0.9571568965911865
Epoch: 6, Steps: 46 | Train Loss: 0.4494654 Vali Loss: 0.4362329 Test Loss: 0.4362329
Validation loss decreased (0.449031 --> 0.436233).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0374462604522705
Epoch: 7, Steps: 46 | Train Loss: 0.4474213 Vali Loss: 0.4353476 Test Loss: 0.4353476
Validation loss decreased (0.436233 --> 0.435348).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0568737983703613
Epoch: 8, Steps: 46 | Train Loss: 0.4438827 Vali Loss: 0.4338416 Test Loss: 0.4338416
Validation loss decreased (0.435348 --> 0.433842).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0205519199371338
Epoch: 9, Steps: 46 | Train Loss: 0.4469128 Vali Loss: 0.4338982 Test Loss: 0.4338982
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0573718547821045
Epoch: 10, Steps: 46 | Train Loss: 0.4518637 Vali Loss: 0.4304539 Test Loss: 0.4304539
Validation loss decreased (0.433842 --> 0.430454).  Saving model ...
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.042928751558065414, mae:0.16155195236206055, smape:16.066449880599976, dtw:not calculated
horizon:2 mse:0.33473289012908936, mae:0.4839800000190735, smape:46.426039934158325, dtw:not calculated
horizon:3 mse:0.28336986899375916, mae:0.4275326430797577, smape:41.09787046909332, dtw:not calculated
horizon:4 mse:0.29354363679885864, mae:0.43888384103775024, smape:42.15191900730133, dtw:not calculated
horizon:5 mse:0.35988032817840576, mae:0.4947413206100464, smape:47.21308350563049, dtw:not calculated
horizon:6 mse:0.5407699942588806, mae:0.5890817046165466, smape:54.70607280731201, dtw:not calculated
horizon:7 mse:0.3617135286331177, mae:0.5042648911476135, smape:48.16577732563019, dtw:not calculated
horizon:8 mse:0.42749592661857605, mae:0.5497778654098511, smape:52.08324193954468, dtw:not calculated
horizon:9 mse:0.831271767616272, mae:0.7759154438972473, smape:70.56783437728882, dtw:not calculated
horizon:10 mse:0.5626525282859802, mae:0.6177200675010681, smape:57.40281939506531, dtw:not calculated
horizon:11 mse:0.6810716986656189, mae:0.6971259713172913, smape:64.20530080795288, dtw:not calculated
horizon:12 mse:0.44601577520370483, mae:0.525054931640625, smape:49.173229932785034, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3092042803764343, mae:0.4326285421848297, smape:41.276901960372925, dtw:not calculated
average metrics: horizon upto:12 mse:0.4304538667201996, mae:0.522135853767395, smape:49.10496771335602, dtw:not calculated
===============================================================================
average of horizons: mse:0.4304538667201996, mae:0.522135853767395, smape:49.10496771335602, dtw:not calculated
mean smape over horizons:  49.1049699485302
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.464585781097412
Epoch: 1, Steps: 46 | Train Loss: 0.8236678 Vali Loss: 0.7416827 Test Loss: 0.7416827
Validation loss decreased (inf --> 0.741683).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0000429153442383
Epoch: 2, Steps: 46 | Train Loss: 0.5646296 Vali Loss: 0.4852582 Test Loss: 0.4852582
Validation loss decreased (0.741683 --> 0.485258).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0527458190917969
Epoch: 3, Steps: 46 | Train Loss: 0.4741044 Vali Loss: 0.4037949 Test Loss: 0.4037949
Validation loss decreased (0.485258 --> 0.403795).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1051924228668213
Epoch: 4, Steps: 46 | Train Loss: 0.4228349 Vali Loss: 0.3668183 Test Loss: 0.3668183
Validation loss decreased (0.403795 --> 0.366818).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.1163408756256104
Epoch: 5, Steps: 46 | Train Loss: 0.4118085 Vali Loss: 0.3544405 Test Loss: 0.3544405
Validation loss decreased (0.366818 --> 0.354441).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0662600994110107
Epoch: 6, Steps: 46 | Train Loss: 0.4241842 Vali Loss: 0.3479963 Test Loss: 0.3479963
Validation loss decreased (0.354441 --> 0.347996).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9970901012420654
Epoch: 7, Steps: 46 | Train Loss: 0.4292044 Vali Loss: 0.3446584 Test Loss: 0.3446584
Validation loss decreased (0.347996 --> 0.344658).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.043104887008667
Epoch: 8, Steps: 46 | Train Loss: 0.4019237 Vali Loss: 0.3375943 Test Loss: 0.3375943
Validation loss decreased (0.344658 --> 0.337594).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0515179634094238
Epoch: 9, Steps: 46 | Train Loss: 0.4315091 Vali Loss: 0.3378029 Test Loss: 0.3378029
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0327954292297363
Epoch: 10, Steps: 46 | Train Loss: 0.4221891 Vali Loss: 0.3385797 Test Loss: 0.3385797
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.08890911936759949, mae:0.22830703854560852, smape:22.51436859369278, dtw:not calculated
horizon:2 mse:0.1589648723602295, mae:0.3416074514389038, smape:33.51987600326538, dtw:not calculated
horizon:3 mse:0.2783328890800476, mae:0.4454818069934845, smape:43.00754964351654, dtw:not calculated
horizon:4 mse:0.1959477961063385, mae:0.35974809527397156, smape:34.98185873031616, dtw:not calculated
horizon:5 mse:0.2692866623401642, mae:0.42797213792800903, smape:41.25295281410217, dtw:not calculated
horizon:6 mse:0.41206124424934387, mae:0.5250522494316101, smape:49.71160888671875, dtw:not calculated
horizon:7 mse:0.3472548723220825, mae:0.46486011147499084, smape:44.22178864479065, dtw:not calculated
horizon:8 mse:0.5393239855766296, mae:0.6344261169433594, smape:59.59911346435547, dtw:not calculated
horizon:9 mse:0.5630545020103455, mae:0.6172915697097778, smape:57.39502310752869, dtw:not calculated
horizon:10 mse:0.4588664174079895, mae:0.5716818571090698, smape:54.03786897659302, dtw:not calculated
horizon:11 mse:0.43560513854026794, mae:0.5294262170791626, smape:49.79511201381683, dtw:not calculated
horizon:12 mse:0.3035237193107605, mae:0.45326223969459534, smape:43.520960211753845, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.23391710221767426, mae:0.388028085231781, smape:37.49803602695465, dtw:not calculated
average metrics: horizon upto:12 mse:0.33759427070617676, mae:0.46659308671951294, smape:44.46317255496979, dtw:not calculated
===============================================================================
average of horizons: mse:0.33759427070617676, mae:0.46659308671951294, smape:44.46317255496979, dtw:not calculated
mean smape over horizons:  44.46317342420419
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4865851402282715
Epoch: 1, Steps: 46 | Train Loss: 0.7333279 Vali Loss: 0.7952116 Test Loss: 0.7952116
Validation loss decreased (inf --> 0.795212).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0349798202514648
Epoch: 2, Steps: 46 | Train Loss: 0.5514138 Vali Loss: 0.5423562 Test Loss: 0.5423562
Validation loss decreased (0.795212 --> 0.542356).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0432913303375244
Epoch: 3, Steps: 46 | Train Loss: 0.4829051 Vali Loss: 0.4401277 Test Loss: 0.4401277
Validation loss decreased (0.542356 --> 0.440128).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0773775577545166
Epoch: 4, Steps: 46 | Train Loss: 0.4372083 Vali Loss: 0.4011424 Test Loss: 0.4011424
Validation loss decreased (0.440128 --> 0.401142).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0149707794189453
Epoch: 5, Steps: 46 | Train Loss: 0.4331835 Vali Loss: 0.3842151 Test Loss: 0.3842151
Validation loss decreased (0.401142 --> 0.384215).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.062917947769165
Epoch: 6, Steps: 46 | Train Loss: 0.4285491 Vali Loss: 0.3795480 Test Loss: 0.3795480
Validation loss decreased (0.384215 --> 0.379548).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0733642578125
Epoch: 7, Steps: 46 | Train Loss: 0.4437928 Vali Loss: 0.3701233 Test Loss: 0.3701233
Validation loss decreased (0.379548 --> 0.370123).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0782887935638428
Epoch: 8, Steps: 46 | Train Loss: 0.4113941 Vali Loss: 0.3690809 Test Loss: 0.3690809
Validation loss decreased (0.370123 --> 0.369081).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0561602115631104
Epoch: 9, Steps: 46 | Train Loss: 0.4285787 Vali Loss: 0.3628020 Test Loss: 0.3628020
Validation loss decreased (0.369081 --> 0.362802).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0030028820037842
Epoch: 10, Steps: 46 | Train Loss: 0.4233131 Vali Loss: 0.3643762 Test Loss: 0.3643762
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0473262183368206, mae:0.16776087880134583, smape:16.670437157154083, dtw:not calculated
horizon:2 mse:0.27077749371528625, mae:0.43633899092674255, smape:42.16432571411133, dtw:not calculated
horizon:3 mse:0.19010300934314728, mae:0.3625708818435669, smape:35.35338044166565, dtw:not calculated
horizon:4 mse:0.4044092893600464, mae:0.5540680289268494, smape:52.8964102268219, dtw:not calculated
horizon:5 mse:0.2582848072052002, mae:0.4201796054840088, smape:40.59769809246063, dtw:not calculated
horizon:6 mse:0.4973965287208557, mae:0.5979152321815491, smape:56.27574920654297, dtw:not calculated
horizon:7 mse:0.5100371241569519, mae:0.5946711301803589, smape:55.80450892448425, dtw:not calculated
horizon:8 mse:0.32250308990478516, mae:0.4759637117385864, smape:45.696672797203064, dtw:not calculated
horizon:9 mse:0.4548968970775604, mae:0.5633172392845154, smape:53.093063831329346, dtw:not calculated
horizon:10 mse:0.3293973207473755, mae:0.4800074100494385, smape:46.04191184043884, dtw:not calculated
horizon:11 mse:0.6140983700752258, mae:0.6486825942993164, smape:60.054826736450195, dtw:not calculated
horizon:12 mse:0.45439398288726807, mae:0.5763028860092163, smape:54.57445979118347, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.2780495584011078, mae:0.42313891649246216, smape:40.659669041633606, dtw:not calculated
average metrics: horizon upto:12 mse:0.36280202865600586, mae:0.4898149073123932, smape:46.60195708274841, dtw:not calculated
===============================================================================
average of horizons: mse:0.36280202865600586, mae:0.4898149073123932, smape:46.60195708274841, dtw:not calculated
mean smape over horizons:  46.601953729987144
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4063410758972168
Epoch: 1, Steps: 46 | Train Loss: 0.6840512 Vali Loss: 0.7575871 Test Loss: 0.7575871
Validation loss decreased (inf --> 0.757587).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0745387077331543
Epoch: 2, Steps: 46 | Train Loss: 0.5227027 Vali Loss: 0.5224817 Test Loss: 0.5224817
Validation loss decreased (0.757587 --> 0.522482).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 0.8482286930084229
Epoch: 3, Steps: 46 | Train Loss: 0.4523581 Vali Loss: 0.4271514 Test Loss: 0.4271514
Validation loss decreased (0.522482 --> 0.427151).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1410977840423584
Epoch: 4, Steps: 46 | Train Loss: 0.4149313 Vali Loss: 0.3905468 Test Loss: 0.3905468
Validation loss decreased (0.427151 --> 0.390547).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9843955039978027
Epoch: 5, Steps: 46 | Train Loss: 0.4066906 Vali Loss: 0.3733211 Test Loss: 0.3733211
Validation loss decreased (0.390547 --> 0.373321).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0682988166809082
Epoch: 6, Steps: 46 | Train Loss: 0.4059307 Vali Loss: 0.3684939 Test Loss: 0.3684939
Validation loss decreased (0.373321 --> 0.368494).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.035912275314331
Epoch: 7, Steps: 46 | Train Loss: 0.4219498 Vali Loss: 0.3610201 Test Loss: 0.3610201
Validation loss decreased (0.368494 --> 0.361020).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0351638793945312
Epoch: 8, Steps: 46 | Train Loss: 0.3922748 Vali Loss: 0.3600888 Test Loss: 0.3600888
Validation loss decreased (0.361020 --> 0.360089).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0482165813446045
Epoch: 9, Steps: 46 | Train Loss: 0.4029808 Vali Loss: 0.3547077 Test Loss: 0.3547077
Validation loss decreased (0.360089 --> 0.354708).  Saving model ...
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0155210494995117
Epoch: 10, Steps: 46 | Train Loss: 0.3992151 Vali Loss: 0.3557437 Test Loss: 0.3557437
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03274645283818245, mae:0.13893038034439087, smape:13.841541111469269, dtw:not calculated
horizon:2 mse:0.2854509949684143, mae:0.4468153119087219, smape:43.07875633239746, dtw:not calculated
horizon:3 mse:0.1599382907152176, mae:0.3182137906551361, smape:31.068551540374756, dtw:not calculated
horizon:4 mse:0.3444567620754242, mae:0.5119362473487854, smape:49.20433461666107, dtw:not calculated
horizon:5 mse:0.23582996428012848, mae:0.4002133011817932, smape:38.7721061706543, dtw:not calculated
horizon:6 mse:0.42389166355133057, mae:0.5469636917114258, smape:51.84371471405029, dtw:not calculated
horizon:7 mse:0.5182550549507141, mae:0.5994628071784973, smape:56.18050694465637, dtw:not calculated
horizon:8 mse:0.33296582102775574, mae:0.4856473505496979, smape:46.57994508743286, dtw:not calculated
horizon:9 mse:0.48932212591171265, mae:0.5883297920227051, smape:55.28197884559631, dtw:not calculated
horizon:10 mse:0.3240230679512024, mae:0.4809909164905548, smape:46.19345963001251, dtw:not calculated
horizon:11 mse:0.6326526403427124, mae:0.6697697639465332, smape:62.02930212020874, dtw:not calculated
horizon:12 mse:0.4769597351551056, mae:0.5925565361976624, smape:55.97072243690491, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.24705234169960022, mae:0.39384543895721436, smape:37.968167662620544, dtw:not calculated
average metrics: horizon upto:12 mse:0.3547077178955078, mae:0.48165249824523926, smape:45.837077498435974, dtw:not calculated
===============================================================================
average of horizons: mse:0.3547077178955078, mae:0.48165249824523926, smape:45.837077498435974, dtw:not calculated
mean smape over horizons:  45.83707662920157
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4912452697753906
Epoch: 1, Steps: 46 | Train Loss: 0.8707547 Vali Loss: 0.7444408 Test Loss: 0.7444408
Validation loss decreased (inf --> 0.744441).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0747699737548828
Epoch: 2, Steps: 46 | Train Loss: 0.5930905 Vali Loss: 0.5478915 Test Loss: 0.5478915
Validation loss decreased (0.744441 --> 0.547891).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.118345022201538
Epoch: 3, Steps: 46 | Train Loss: 0.5029639 Vali Loss: 0.4620937 Test Loss: 0.4620937
Validation loss decreased (0.547891 --> 0.462094).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0001287460327148
Epoch: 4, Steps: 46 | Train Loss: 0.4699009 Vali Loss: 0.4284544 Test Loss: 0.4284544
Validation loss decreased (0.462094 --> 0.428454).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0354561805725098
Epoch: 5, Steps: 46 | Train Loss: 0.4670791 Vali Loss: 0.4058720 Test Loss: 0.4058720
Validation loss decreased (0.428454 --> 0.405872).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0171074867248535
Epoch: 6, Steps: 46 | Train Loss: 0.4574534 Vali Loss: 0.4023703 Test Loss: 0.4023703
Validation loss decreased (0.405872 --> 0.402370).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.075277328491211
Epoch: 7, Steps: 46 | Train Loss: 0.4674823 Vali Loss: 0.3963391 Test Loss: 0.3963391
Validation loss decreased (0.402370 --> 0.396339).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0030107498168945
Epoch: 8, Steps: 46 | Train Loss: 0.4327705 Vali Loss: 0.3924140 Test Loss: 0.3924140
Validation loss decreased (0.396339 --> 0.392414).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0010168552398682
Epoch: 9, Steps: 46 | Train Loss: 0.4487850 Vali Loss: 0.3934066 Test Loss: 0.3934066
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0693857669830322
Epoch: 10, Steps: 46 | Train Loss: 0.4467419 Vali Loss: 0.3965218 Test Loss: 0.3965218
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.05693399906158447, mae:0.1775640994310379, smape:17.597052454948425, dtw:not calculated
horizon:2 mse:0.5040289759635925, mae:0.6017246246337891, smape:56.576645374298096, dtw:not calculated
horizon:3 mse:0.26426759362220764, mae:0.41150397062301636, smape:39.61953818798065, dtw:not calculated
horizon:4 mse:0.3610522150993347, mae:0.46719393134117126, smape:44.23017501831055, dtw:not calculated
horizon:5 mse:0.3391859531402588, mae:0.48328903317451477, smape:46.235230565071106, dtw:not calculated
horizon:6 mse:0.36735376715660095, mae:0.45615026354789734, smape:42.94671714305878, dtw:not calculated
horizon:7 mse:0.5598089098930359, mae:0.6076852083206177, smape:56.44504427909851, dtw:not calculated
horizon:8 mse:0.31007593870162964, mae:0.45126351714134216, smape:43.241605162620544, dtw:not calculated
horizon:9 mse:0.58088618516922, mae:0.6464824676513672, smape:60.26316285133362, dtw:not calculated
horizon:10 mse:0.3467658758163452, mae:0.46282824873924255, smape:43.949341773986816, dtw:not calculated
horizon:11 mse:0.5588628649711609, mae:0.629230797290802, smape:58.71891379356384, dtw:not calculated
horizon:12 mse:0.4597455561161041, mae:0.5577664375305176, smape:52.44510769844055, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.315470427274704, mae:0.43290430307388306, smape:41.20089411735535, dtw:not calculated
average metrics: horizon upto:12 mse:0.39241403341293335, mae:0.49605685472488403, smape:46.855711936950684, dtw:not calculated
===============================================================================
average of horizons: mse:0.39241403341293335, mae:0.49605685472488403, smape:46.855711936950684, dtw:not calculated
mean smape over horizons:  46.855711191892624
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5055644512176514
Epoch: 1, Steps: 46 | Train Loss: 0.8579968 Vali Loss: 0.7714946 Test Loss: 0.7714946
Validation loss decreased (inf --> 0.771495).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1282062530517578
Epoch: 2, Steps: 46 | Train Loss: 0.5922531 Vali Loss: 0.5629800 Test Loss: 0.5629800
Validation loss decreased (0.771495 --> 0.562980).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.059088945388794
Epoch: 3, Steps: 46 | Train Loss: 0.5025817 Vali Loss: 0.4736171 Test Loss: 0.4736171
Validation loss decreased (0.562980 --> 0.473617).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0556342601776123
Epoch: 4, Steps: 46 | Train Loss: 0.4642298 Vali Loss: 0.4346329 Test Loss: 0.4346329
Validation loss decreased (0.473617 --> 0.434633).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.025695562362671
Epoch: 5, Steps: 46 | Train Loss: 0.4602316 Vali Loss: 0.4121262 Test Loss: 0.4121262
Validation loss decreased (0.434633 --> 0.412126).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.124063491821289
Epoch: 6, Steps: 46 | Train Loss: 0.4521155 Vali Loss: 0.4088141 Test Loss: 0.4088141
Validation loss decreased (0.412126 --> 0.408814).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0794730186462402
Epoch: 7, Steps: 46 | Train Loss: 0.4626651 Vali Loss: 0.4008970 Test Loss: 0.4008970
Validation loss decreased (0.408814 --> 0.400897).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.1679799556732178
Epoch: 8, Steps: 46 | Train Loss: 0.4263036 Vali Loss: 0.3970389 Test Loss: 0.3970389
Validation loss decreased (0.400897 --> 0.397039).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.012916088104248
Epoch: 9, Steps: 46 | Train Loss: 0.4436775 Vali Loss: 0.3984899 Test Loss: 0.3984899
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9612667560577393
Epoch: 10, Steps: 46 | Train Loss: 0.4408502 Vali Loss: 0.4009370 Test Loss: 0.4009370
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04697318747639656, mae:0.16198401153087616, smape:16.086481511592865, dtw:not calculated
horizon:2 mse:0.5323418378829956, mae:0.6103971600532532, smape:57.12372660636902, dtw:not calculated
horizon:3 mse:0.24593214690685272, mae:0.3930007219314575, smape:37.90035545825958, dtw:not calculated
horizon:4 mse:0.3512340486049652, mae:0.45847412943840027, smape:43.44087243080139, dtw:not calculated
horizon:5 mse:0.3312985897064209, mae:0.475068062543869, smape:45.47343850135803, dtw:not calculated
horizon:6 mse:0.34915199875831604, mae:0.4496866464614868, smape:42.49744713306427, dtw:not calculated
horizon:7 mse:0.6006476283073425, mae:0.6334080100059509, smape:58.604711294174194, dtw:not calculated
horizon:8 mse:0.29205721616744995, mae:0.43347135186195374, smape:41.59313142299652, dtw:not calculated
horizon:9 mse:0.6120578646659851, mae:0.6615744233131409, smape:61.42483949661255, dtw:not calculated
horizon:10 mse:0.38199737668037415, mae:0.4842011034488678, smape:45.738428831100464, dtw:not calculated
horizon:11 mse:0.5307273268699646, mae:0.6187315583229065, smape:57.99112915992737, dtw:not calculated
horizon:12 mse:0.49004799127578735, mae:0.5738269686698914, smape:53.75574827194214, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3094886541366577, mae:0.42476847767829895, smape:40.4203861951828, dtw:not calculated
average metrics: horizon upto:12 mse:0.39703893661499023, mae:0.49615204334259033, smape:46.8025267124176, dtw:not calculated
===============================================================================
average of horizons: mse:0.39703893661499023, mae:0.49615204334259033, smape:46.8025267124176, dtw:not calculated
mean smape over horizons:  46.8025258431832
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=14
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3639800548553467
Epoch: 1, Steps: 46 | Train Loss: 0.8403707 Vali Loss: 0.7889375 Test Loss: 0.7889375
Validation loss decreased (inf --> 0.788938).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.1569252014160156
Epoch: 2, Steps: 46 | Train Loss: 0.5894800 Vali Loss: 0.5740736 Test Loss: 0.5740736
Validation loss decreased (0.788938 --> 0.574074).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0666987895965576
Epoch: 3, Steps: 46 | Train Loss: 0.5004064 Vali Loss: 0.4812565 Test Loss: 0.4812565
Validation loss decreased (0.574074 --> 0.481257).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.063899040222168
Epoch: 4, Steps: 46 | Train Loss: 0.4593759 Vali Loss: 0.4405833 Test Loss: 0.4405833
Validation loss decreased (0.481257 --> 0.440583).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9027304649353027
Epoch: 5, Steps: 46 | Train Loss: 0.4552633 Vali Loss: 0.4185567 Test Loss: 0.4185567
Validation loss decreased (0.440583 --> 0.418557).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0996968746185303
Epoch: 6, Steps: 46 | Train Loss: 0.4483314 Vali Loss: 0.4146122 Test Loss: 0.4146122
Validation loss decreased (0.418557 --> 0.414612).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0481617450714111
Epoch: 7, Steps: 46 | Train Loss: 0.4571671 Vali Loss: 0.4062679 Test Loss: 0.4062679
Validation loss decreased (0.414612 --> 0.406268).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0261507034301758
Epoch: 8, Steps: 46 | Train Loss: 0.4218342 Vali Loss: 0.4016487 Test Loss: 0.4016487
Validation loss decreased (0.406268 --> 0.401649).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0880072116851807
Epoch: 9, Steps: 46 | Train Loss: 0.4401896 Vali Loss: 0.4052134 Test Loss: 0.4052134
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.058706283569336
Epoch: 10, Steps: 46 | Train Loss: 0.4360816 Vali Loss: 0.4059918 Test Loss: 0.4059918
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.038307927548885345, mae:0.14918014407157898, smape:14.844390749931335, dtw:not calculated
horizon:2 mse:0.5382704734802246, mae:0.616620659828186, smape:57.7315092086792, dtw:not calculated
horizon:3 mse:0.2249089926481247, mae:0.37659111618995667, smape:36.43685579299927, dtw:not calculated
horizon:4 mse:0.3497145175933838, mae:0.4615495204925537, smape:43.77581477165222, dtw:not calculated
horizon:5 mse:0.32608190178871155, mae:0.47303587198257446, smape:45.32065689563751, dtw:not calculated
horizon:6 mse:0.3363594710826874, mae:0.44492053985595703, smape:42.173561453819275, dtw:not calculated
horizon:7 mse:0.6558522582054138, mae:0.6670746803283691, smape:61.395496129989624, dtw:not calculated
horizon:8 mse:0.289623498916626, mae:0.4280189275741577, smape:41.05256795883179, dtw:not calculated
horizon:9 mse:0.6296916007995605, mae:0.6691975593566895, smape:61.993348598480225, dtw:not calculated
horizon:10 mse:0.418636292219162, mae:0.5047685503959656, smape:47.41989076137543, dtw:not calculated
horizon:11 mse:0.48284801840782166, mae:0.5919197201728821, smape:55.79615831375122, dtw:not calculated
horizon:12 mse:0.5294898748397827, mae:0.5966892242431641, smape:55.63376545906067, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.30227386951446533, mae:0.42031630873680115, smape:40.04713296890259, dtw:not calculated
average metrics: horizon upto:12 mse:0.40164873003959656, mae:0.49829721450805664, smape:46.96449637413025, dtw:not calculated
===============================================================================
average of horizons: mse:0.40164873003959656, mae:0.49829721450805664, smape:46.96449637413025, dtw:not calculated
mean smape over horizons:  46.96450134118398
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=16
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5609219074249268
Epoch: 1, Steps: 46 | Train Loss: 0.8266640 Vali Loss: 0.8089436 Test Loss: 0.8089436
Validation loss decreased (inf --> 0.808944).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0296707153320312
Epoch: 2, Steps: 46 | Train Loss: 0.5925981 Vali Loss: 0.5889332 Test Loss: 0.5889332
Validation loss decreased (0.808944 --> 0.588933).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.038710594177246
Epoch: 3, Steps: 46 | Train Loss: 0.5063758 Vali Loss: 0.4954230 Test Loss: 0.4954230
Validation loss decreased (0.588933 --> 0.495423).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9466533660888672
Epoch: 4, Steps: 46 | Train Loss: 0.4591019 Vali Loss: 0.4542848 Test Loss: 0.4542848
Validation loss decreased (0.495423 --> 0.454285).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0496058464050293
Epoch: 5, Steps: 46 | Train Loss: 0.4564278 Vali Loss: 0.4337919 Test Loss: 0.4337919
Validation loss decreased (0.454285 --> 0.433792).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0265793800354004
Epoch: 6, Steps: 46 | Train Loss: 0.4478000 Vali Loss: 0.4292292 Test Loss: 0.4292292
Validation loss decreased (0.433792 --> 0.429229).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.076411485671997
Epoch: 7, Steps: 46 | Train Loss: 0.4590808 Vali Loss: 0.4199191 Test Loss: 0.4199191
Validation loss decreased (0.429229 --> 0.419919).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0213282108306885
Epoch: 8, Steps: 46 | Train Loss: 0.4232332 Vali Loss: 0.4162490 Test Loss: 0.4162490
Validation loss decreased (0.419919 --> 0.416249).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.965679407119751
Epoch: 9, Steps: 46 | Train Loss: 0.4427292 Vali Loss: 0.4206009 Test Loss: 0.4206009
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.1036591529846191
Epoch: 10, Steps: 46 | Train Loss: 0.4391178 Vali Loss: 0.4196314 Test Loss: 0.4196314
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03634761646389961, mae:0.14389774203300476, smape:14.321544766426086, dtw:not calculated
horizon:2 mse:0.546575665473938, mae:0.6310936212539673, smape:59.171414375305176, dtw:not calculated
horizon:3 mse:0.22030186653137207, mae:0.3767843246459961, smape:36.507150530815125, dtw:not calculated
horizon:4 mse:0.3556673526763916, mae:0.47148463129997253, smape:44.74310278892517, dtw:not calculated
horizon:5 mse:0.3112833499908447, mae:0.4630647897720337, smape:44.45650577545166, dtw:not calculated
horizon:6 mse:0.3346214294433594, mae:0.4504031836986542, smape:42.76542663574219, dtw:not calculated
horizon:7 mse:0.710729718208313, mae:0.7002919316291809, smape:64.1345202922821, dtw:not calculated
horizon:8 mse:0.2925073802471161, mae:0.4263319969177246, smape:40.85291028022766, dtw:not calculated
horizon:9 mse:0.6602993011474609, mae:0.6909687519073486, smape:63.90395760536194, dtw:not calculated
horizon:10 mse:0.4570797383785248, mae:0.532020628452301, smape:49.77821111679077, dtw:not calculated
horizon:11 mse:0.4782559275627136, mae:0.5891354084014893, smape:55.56135177612305, dtw:not calculated
horizon:12 mse:0.5913195013999939, mae:0.6293587684631348, smape:58.24465751647949, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.30079951882362366, mae:0.4227880537509918, smape:40.32752215862274, dtw:not calculated
average metrics: horizon upto:12 mse:0.4162490665912628, mae:0.5087363123893738, smape:47.87006378173828, dtw:not calculated
===============================================================================
average of horizons: mse:0.4162490665912628, mae:0.5087363123893738, smape:47.87006378173828, dtw:not calculated
mean smape over horizons:  47.87006278832754
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=18
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4721384048461914
Epoch: 1, Steps: 46 | Train Loss: 0.8224613 Vali Loss: 0.8266700 Test Loss: 0.8266700
Validation loss decreased (inf --> 0.826670).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0146348476409912
Epoch: 2, Steps: 46 | Train Loss: 0.5965738 Vali Loss: 0.6034479 Test Loss: 0.6034479
Validation loss decreased (0.826670 --> 0.603448).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.1873021125793457
Epoch: 3, Steps: 46 | Train Loss: 0.5104882 Vali Loss: 0.5089965 Test Loss: 0.5089965
Validation loss decreased (0.603448 --> 0.508996).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.0346691608428955
Epoch: 4, Steps: 46 | Train Loss: 0.4624796 Vali Loss: 0.4676728 Test Loss: 0.4676728
Validation loss decreased (0.508996 --> 0.467673).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 0.9744226932525635
Epoch: 5, Steps: 46 | Train Loss: 0.4600896 Vali Loss: 0.4466622 Test Loss: 0.4466622
Validation loss decreased (0.467673 --> 0.446662).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0498790740966797
Epoch: 6, Steps: 46 | Train Loss: 0.4509644 Vali Loss: 0.4414959 Test Loss: 0.4414959
Validation loss decreased (0.446662 --> 0.441496).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0108351707458496
Epoch: 7, Steps: 46 | Train Loss: 0.4611114 Vali Loss: 0.4329312 Test Loss: 0.4329312
Validation loss decreased (0.441496 --> 0.432931).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0171663761138916
Epoch: 8, Steps: 46 | Train Loss: 0.4283170 Vali Loss: 0.4300309 Test Loss: 0.4300309
Validation loss decreased (0.432931 --> 0.430031).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0259332656860352
Epoch: 9, Steps: 46 | Train Loss: 0.4474029 Vali Loss: 0.4340498 Test Loss: 0.4340498
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.0527851581573486
Epoch: 10, Steps: 46 | Train Loss: 0.4414513 Vali Loss: 0.4320843 Test Loss: 0.4320843
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04181667044758797, mae:0.15808923542499542, smape:15.721924602985382, dtw:not calculated
horizon:2 mse:0.5678204298019409, mae:0.6519190669059753, smape:61.113327741622925, dtw:not calculated
horizon:3 mse:0.2242896556854248, mae:0.3820372223854065, smape:37.00824677944183, dtw:not calculated
horizon:4 mse:0.34923461079597473, mae:0.47486358880996704, smape:45.186758041381836, dtw:not calculated
horizon:5 mse:0.3100915551185608, mae:0.46961674094200134, smape:45.14070451259613, dtw:not calculated
horizon:6 mse:0.3460101783275604, mae:0.464714378118515, smape:44.13385093212128, dtw:not calculated
horizon:7 mse:0.7639634609222412, mae:0.7303401231765747, smape:66.56299829483032, dtw:not calculated
horizon:8 mse:0.29613688588142395, mae:0.4242042303085327, smape:40.586066246032715, dtw:not calculated
horizon:9 mse:0.6733940243721008, mae:0.7063001990318298, smape:65.3735339641571, dtw:not calculated
horizon:10 mse:0.48021841049194336, mae:0.5488073229789734, smape:51.2485146522522, dtw:not calculated
horizon:11 mse:0.4706779420375824, mae:0.5842300057411194, smape:55.13731837272644, dtw:not calculated
horizon:12 mse:0.6367167234420776, mae:0.6593590378761292, smape:60.78055500984192, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.30654385685920715, mae:0.4335400462150574, smape:41.38413667678833, dtw:not calculated
average metrics: horizon upto:12 mse:0.430030882358551, mae:0.5212067365646362, smape:48.99948239326477, dtw:not calculated
===============================================================================
average of horizons: mse:0.430030882358551, mae:0.5212067365646362, smape:48.99948239326477, dtw:not calculated
mean smape over horizons:  48.999483262499176
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=20
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.538921594619751
Epoch: 1, Steps: 46 | Train Loss: 0.8131564 Vali Loss: 0.8394206 Test Loss: 0.8394206
Validation loss decreased (inf --> 0.839421).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.017446517944336
Epoch: 2, Steps: 46 | Train Loss: 0.5961577 Vali Loss: 0.6127885 Test Loss: 0.6127885
Validation loss decreased (0.839421 --> 0.612788).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.113147497177124
Epoch: 3, Steps: 46 | Train Loss: 0.5127153 Vali Loss: 0.5163116 Test Loss: 0.5163116
Validation loss decreased (0.612788 --> 0.516312).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.017232894897461
Epoch: 4, Steps: 46 | Train Loss: 0.4642584 Vali Loss: 0.4756002 Test Loss: 0.4756002
Validation loss decreased (0.516312 --> 0.475600).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0232298374176025
Epoch: 5, Steps: 46 | Train Loss: 0.4608156 Vali Loss: 0.4549632 Test Loss: 0.4549632
Validation loss decreased (0.475600 --> 0.454963).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.011744737625122
Epoch: 6, Steps: 46 | Train Loss: 0.4535471 Vali Loss: 0.4487424 Test Loss: 0.4487424
Validation loss decreased (0.454963 --> 0.448742).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0138499736785889
Epoch: 7, Steps: 46 | Train Loss: 0.4646340 Vali Loss: 0.4409414 Test Loss: 0.4409414
Validation loss decreased (0.448742 --> 0.440941).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0604593753814697
Epoch: 8, Steps: 46 | Train Loss: 0.4303825 Vali Loss: 0.4375190 Test Loss: 0.4375190
Validation loss decreased (0.440941 --> 0.437519).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.9719436168670654
Epoch: 9, Steps: 46 | Train Loss: 0.4522889 Vali Loss: 0.4415664 Test Loss: 0.4415664
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.047799825668335
Epoch: 10, Steps: 46 | Train Loss: 0.4445487 Vali Loss: 0.4392615 Test Loss: 0.4392615
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.046573419123888016, mae:0.17029573023319244, smape:16.93151444196701, dtw:not calculated
horizon:2 mse:0.5964008569717407, mae:0.6677752733230591, smape:62.42132782936096, dtw:not calculated
horizon:3 mse:0.2233106642961502, mae:0.3825068175792694, smape:37.06249296665192, dtw:not calculated
horizon:4 mse:0.3397377133369446, mae:0.4734654128551483, smape:45.178550481796265, dtw:not calculated
horizon:5 mse:0.3156973421573639, mae:0.4754548668861389, smape:45.672497153282166, dtw:not calculated
horizon:6 mse:0.35155779123306274, mae:0.47290584444999695, smape:44.93272304534912, dtw:not calculated
horizon:7 mse:0.7673726081848145, mae:0.7333328127861023, smape:66.84168577194214, dtw:not calculated
horizon:8 mse:0.2945638597011566, mae:0.4220823347568512, smape:40.37708640098572, dtw:not calculated
horizon:9 mse:0.7099592685699463, mae:0.7346412539482117, smape:67.88629293441772, dtw:not calculated
horizon:10 mse:0.49034583568573, mae:0.5590951442718506, smape:52.188706398010254, dtw:not calculated
horizon:11 mse:0.4619867503643036, mae:0.5778924822807312, smape:54.58347797393799, dtw:not calculated
horizon:12 mse:0.6527220606803894, mae:0.6677833795547485, smape:61.45700216293335, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3122129440307617, mae:0.4404006898403168, smape:42.033183574676514, dtw:not calculated
average metrics: horizon upto:12 mse:0.43751901388168335, mae:0.5281026363372803, smape:49.62777495384216, dtw:not calculated
===============================================================================
average of horizons: mse:0.43751901388168335, mae:0.5281026363372803, smape:49.62777495384216, dtw:not calculated
mean smape over horizons:  49.62777979671955
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=22
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4926769733428955
Epoch: 1, Steps: 46 | Train Loss: 0.8137388 Vali Loss: 0.8430091 Test Loss: 0.8430091
Validation loss decreased (inf --> 0.843009).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 1.0579638481140137
Epoch: 2, Steps: 46 | Train Loss: 0.5948347 Vali Loss: 0.6128768 Test Loss: 0.6128768
Validation loss decreased (0.843009 --> 0.612877).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.0560858249664307
Epoch: 3, Steps: 46 | Train Loss: 0.5132018 Vali Loss: 0.5149524 Test Loss: 0.5149524
Validation loss decreased (0.612877 --> 0.514952).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 0.9585444927215576
Epoch: 4, Steps: 46 | Train Loss: 0.4654345 Vali Loss: 0.4737575 Test Loss: 0.4737575
Validation loss decreased (0.514952 --> 0.473757).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.0605278015136719
Epoch: 5, Steps: 46 | Train Loss: 0.4609830 Vali Loss: 0.4537768 Test Loss: 0.4537768
Validation loss decreased (0.473757 --> 0.453777).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.1111915111541748
Epoch: 6, Steps: 46 | Train Loss: 0.4526643 Vali Loss: 0.4467283 Test Loss: 0.4467283
Validation loss decreased (0.453777 --> 0.446728).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 0.9670019149780273
Epoch: 7, Steps: 46 | Train Loss: 0.4636765 Vali Loss: 0.4404828 Test Loss: 0.4404828
Validation loss decreased (0.446728 --> 0.440483).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0069055557250977
Epoch: 8, Steps: 46 | Train Loss: 0.4294520 Vali Loss: 0.4360996 Test Loss: 0.4360996
Validation loss decreased (0.440483 --> 0.436100).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 0.936204195022583
Epoch: 9, Steps: 46 | Train Loss: 0.4524833 Vali Loss: 0.4404407 Test Loss: 0.4404407
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 1.017725944519043
Epoch: 10, Steps: 46 | Train Loss: 0.4428532 Vali Loss: 0.4377394 Test Loss: 0.4377394
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04759081080555916, mae:0.17623381316661835, smape:17.529195547103882, dtw:not calculated
horizon:2 mse:0.6053408980369568, mae:0.6743170619010925, smape:62.97940015792847, dtw:not calculated
horizon:3 mse:0.21425552666187286, mae:0.36675477027893066, smape:35.5273962020874, dtw:not calculated
horizon:4 mse:0.34109407663345337, mae:0.4789375364780426, smape:45.73584794998169, dtw:not calculated
horizon:5 mse:0.2918689548969269, mae:0.45104220509529114, smape:43.402302265167236, dtw:not calculated
horizon:6 mse:0.34655115008354187, mae:0.47211557626724243, smape:44.907596707344055, dtw:not calculated
horizon:7 mse:0.7327711582183838, mae:0.7170343995094299, smape:65.62725901603699, dtw:not calculated
horizon:8 mse:0.30313950777053833, mae:0.4343157708644867, smape:41.54852032661438, dtw:not calculated
horizon:9 mse:0.750537633895874, mae:0.7574604749679565, smape:69.75270509719849, dtw:not calculated
horizon:10 mse:0.5141195058822632, mae:0.5752027034759521, smape:53.579509258270264, dtw:not calculated
horizon:11 mse:0.46478840708732605, mae:0.5798826813697815, smape:54.758405685424805, dtw:not calculated
horizon:12 mse:0.6211380958557129, mae:0.6513707041740417, smape:60.161590576171875, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3077835738658905, mae:0.4365667998790741, smape:41.680288314819336, dtw:not calculated
average metrics: horizon upto:12 mse:0.436099648475647, mae:0.5278889536857605, smape:49.625808000564575, dtw:not calculated
===============================================================================
average of horizons: mse:0.436099648475647, mae:0.5278889536857605, smape:49.625808000564575, dtw:not calculated
mean smape over horizons:  49.62581073244413
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.00001, patch_len=28, stride=24
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      1e-05               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.427905559539795
Epoch: 1, Steps: 46 | Train Loss: 0.8117840 Vali Loss: 0.8429183 Test Loss: 0.8429183
Validation loss decreased (inf --> 0.842918).  Saving model ...
Updating learning rate to 1e-05
Epoch: 2 cost time: 0.998013973236084
Epoch: 2, Steps: 46 | Train Loss: 0.5910454 Vali Loss: 0.6100319 Test Loss: 0.6100319
Validation loss decreased (0.842918 --> 0.610032).  Saving model ...
Updating learning rate to 5e-06
Epoch: 3 cost time: 1.113422155380249
Epoch: 3, Steps: 46 | Train Loss: 0.5105578 Vali Loss: 0.5103474 Test Loss: 0.5103474
Validation loss decreased (0.610032 --> 0.510347).  Saving model ...
Updating learning rate to 2.5e-06
Epoch: 4 cost time: 1.1598827838897705
Epoch: 4, Steps: 46 | Train Loss: 0.4640309 Vali Loss: 0.4679786 Test Loss: 0.4679786
Validation loss decreased (0.510347 --> 0.467979).  Saving model ...
Updating learning rate to 1.25e-06
Epoch: 5 cost time: 1.1211838722229004
Epoch: 5, Steps: 46 | Train Loss: 0.4588826 Vali Loss: 0.4472686 Test Loss: 0.4472686
Validation loss decreased (0.467979 --> 0.447269).  Saving model ...
Updating learning rate to 6.25e-07
Epoch: 6 cost time: 1.0853307247161865
Epoch: 6, Steps: 46 | Train Loss: 0.4524721 Vali Loss: 0.4402070 Test Loss: 0.4402070
Validation loss decreased (0.447269 --> 0.440207).  Saving model ...
Updating learning rate to 3.125e-07
Epoch: 7 cost time: 1.0237655639648438
Epoch: 7, Steps: 46 | Train Loss: 0.4627191 Vali Loss: 0.4337717 Test Loss: 0.4337717
Validation loss decreased (0.440207 --> 0.433772).  Saving model ...
Updating learning rate to 1.5625e-07
Epoch: 8 cost time: 1.0977373123168945
Epoch: 8, Steps: 46 | Train Loss: 0.4256541 Vali Loss: 0.4295569 Test Loss: 0.4295569
Validation loss decreased (0.433772 --> 0.429557).  Saving model ...
Updating learning rate to 7.8125e-08
Epoch: 9 cost time: 1.0210199356079102
Epoch: 9, Steps: 46 | Train Loss: 0.4500831 Vali Loss: 0.4335262 Test Loss: 0.4335262
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-08
Epoch: 10 cost time: 0.9663481712341309
Epoch: 10, Steps: 46 | Train Loss: 0.4414089 Vali Loss: 0.4311276 Test Loss: 0.4311276
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.953125e-08
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04775223881006241, mae:0.1762767881155014, smape:17.531749606132507, dtw:not calculated
horizon:2 mse:0.5910776853561401, mae:0.6667735576629639, smape:62.344563007354736, dtw:not calculated
horizon:3 mse:0.19999627768993378, mae:0.34719276428222656, smape:33.65366756916046, dtw:not calculated
horizon:4 mse:0.36641669273376465, mae:0.49598661065101624, smape:47.20064997673035, dtw:not calculated
horizon:5 mse:0.2732273042201996, mae:0.42885035276412964, smape:41.320690512657166, dtw:not calculated
horizon:6 mse:0.34629395604133606, mae:0.4681914746761322, smape:44.50278580188751, dtw:not calculated
horizon:7 mse:0.6737551689147949, mae:0.6870506405830383, smape:63.31852674484253, dtw:not calculated
horizon:8 mse:0.3139568269252777, mae:0.44967466592788696, smape:43.010157346725464, dtw:not calculated
horizon:9 mse:0.77543705701828, mae:0.7679399847984314, smape:70.5105721950531, dtw:not calculated
horizon:10 mse:0.533324122428894, mae:0.5863468647003174, smape:54.50841188430786, dtw:not calculated
horizon:11 mse:0.43365955352783203, mae:0.5570087432861328, smape:52.74199843406677, dtw:not calculated
horizon:12 mse:0.5997856259346008, mae:0.6408489346504211, smape:59.34472680091858, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.3041273355484009, mae:0.43054530024528503, smape:41.09234809875488, dtw:not calculated
average metrics: horizon upto:12 mse:0.42955687642097473, mae:0.5226784944534302, smape:49.16570782661438, dtw:not calculated
===============================================================================
average of horizons: mse:0.42955687642097473, mae:0.5226784944534302, smape:49.16570782661438, dtw:not calculated
mean smape over horizons:  49.165708323319755
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=8, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4029860496520996
Epoch: 1, Steps: 46 | Train Loss: 0.4581832 Vali Loss: 0.2747694 Test Loss: 0.2747694
Validation loss decreased (inf --> 0.274769).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.0587520599365234
Epoch: 2, Steps: 46 | Train Loss: 0.3837854 Vali Loss: 0.2602149 Test Loss: 0.2602149
Validation loss decreased (0.274769 --> 0.260215).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.040898323059082
Epoch: 3, Steps: 46 | Train Loss: 0.2923900 Vali Loss: 0.1810330 Test Loss: 0.1810330
Validation loss decreased (0.260215 --> 0.181033).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 0.973074197769165
Epoch: 4, Steps: 46 | Train Loss: 0.2283422 Vali Loss: 0.1785763 Test Loss: 0.1785763
Validation loss decreased (0.181033 --> 0.178576).  Saving model ...
Updating learning rate to 0.000125
Epoch: 5 cost time: 0.9975883960723877
Epoch: 5, Steps: 46 | Train Loss: 0.2159561 Vali Loss: 0.1932359 Test Loss: 0.1932359
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.0690040588378906
Epoch: 6, Steps: 46 | Train Loss: 0.2192649 Vali Loss: 0.1667233 Test Loss: 0.1667233
Validation loss decreased (0.178576 --> 0.166723).  Saving model ...
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.0877549648284912
Epoch: 7, Steps: 46 | Train Loss: 0.2145751 Vali Loss: 0.1699178 Test Loss: 0.1699178
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.0783591270446777
Epoch: 8, Steps: 46 | Train Loss: 0.1950337 Vali Loss: 0.1711175 Test Loss: 0.1711175
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-06
Epoch: 9 cost time: 0.9968395233154297
Epoch: 9, Steps: 46 | Train Loss: 0.2056960 Vali Loss: 0.1710906 Test Loss: 0.1710906
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02683795616030693, mae:0.11739751696586609, smape:11.688507348299026, dtw:not calculated
horizon:2 mse:0.05531058460474014, mae:0.17354176938533783, smape:17.194418609142303, dtw:not calculated
horizon:3 mse:0.07747378945350647, mae:0.20394283533096313, smape:20.118294656276703, dtw:not calculated
horizon:4 mse:0.11252910643815994, mae:0.2389945536851883, smape:23.381207883358, dtw:not calculated
horizon:5 mse:0.15195073187351227, mae:0.28324416279792786, smape:27.51573622226715, dtw:not calculated
horizon:6 mse:0.16452789306640625, mae:0.3011794686317444, smape:29.232925176620483, dtw:not calculated
horizon:7 mse:0.18688082695007324, mae:0.32822105288505554, smape:31.8017840385437, dtw:not calculated
horizon:8 mse:0.2269846349954605, mae:0.3680536448955536, smape:35.48893332481384, dtw:not calculated
horizon:9 mse:0.22316855192184448, mae:0.36336416006088257, smape:35.02418398857117, dtw:not calculated
horizon:10 mse:0.23914964497089386, mae:0.36965397000312805, smape:35.469892621040344, dtw:not calculated
horizon:11 mse:0.2765367925167084, mae:0.3958524465560913, smape:37.73763179779053, dtw:not calculated
horizon:12 mse:0.25932925939559937, mae:0.39283862709999084, smape:37.65107989311218, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09810500591993332, mae:0.21971672773361206, smape:21.521848440170288, dtw:not calculated
average metrics: horizon upto:12 mse:0.1667233109474182, mae:0.2946903705596924, smape:28.52538526058197, dtw:not calculated
===============================================================================
average of horizons: mse:0.1667233109474182, mae:0.2946903705596924, smape:28.52538526058197, dtw:not calculated
mean smape over horizons:  28.52538296331962
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=8, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4418330192565918
Epoch: 1, Steps: 46 | Train Loss: 0.4096156 Vali Loss: 0.2783085 Test Loss: 0.2783085
Validation loss decreased (inf --> 0.278309).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 0.9862854480743408
Epoch: 2, Steps: 46 | Train Loss: 0.3375951 Vali Loss: 0.2230364 Test Loss: 0.2230364
Validation loss decreased (0.278309 --> 0.223036).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.022454023361206
Epoch: 3, Steps: 46 | Train Loss: 0.2797221 Vali Loss: 0.1849493 Test Loss: 0.1849493
Validation loss decreased (0.223036 --> 0.184949).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.0729727745056152
Epoch: 4, Steps: 46 | Train Loss: 0.2366223 Vali Loss: 0.1900766 Test Loss: 0.1900766
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.0177056789398193
Epoch: 5, Steps: 46 | Train Loss: 0.2184045 Vali Loss: 0.1972316 Test Loss: 0.1972316
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.0814833641052246
Epoch: 6, Steps: 46 | Train Loss: 0.2152946 Vali Loss: 0.1867210 Test Loss: 0.1867210
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.04351993277668953, mae:0.16454507410526276, smape:16.369836032390594, dtw:not calculated
horizon:2 mse:0.06218225508928299, mae:0.19456233084201813, smape:19.28718239068985, dtw:not calculated
horizon:3 mse:0.12632915377616882, mae:0.27262407541275024, smape:26.69537663459778, dtw:not calculated
horizon:4 mse:0.12594372034072876, mae:0.2561189532279968, smape:25.021442770957947, dtw:not calculated
horizon:5 mse:0.1703118085861206, mae:0.303466796875, smape:29.427215456962585, dtw:not calculated
horizon:6 mse:0.20685535669326782, mae:0.3350933790206909, smape:32.24665820598602, dtw:not calculated
horizon:7 mse:0.21020807325839996, mae:0.3589962422847748, smape:34.75697934627533, dtw:not calculated
horizon:8 mse:0.2489568293094635, mae:0.3818400800228119, smape:36.635613441467285, dtw:not calculated
horizon:9 mse:0.2609292268753052, mae:0.41792139410972595, smape:40.31868278980255, dtw:not calculated
horizon:10 mse:0.24669723212718964, mae:0.3649955689907074, smape:34.87506806850433, dtw:not calculated
horizon:11 mse:0.2585708796977997, mae:0.40877917408943176, smape:39.36900496482849, dtw:not calculated
horizon:12 mse:0.25888699293136597, mae:0.3956594467163086, smape:37.96100914478302, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.1225237026810646, mae:0.2544017732143402, smape:24.84128624200821, dtw:not calculated
average metrics: horizon upto:12 mse:0.18494927883148193, mae:0.321216881275177, smape:31.08033835887909, dtw:not calculated
===============================================================================
average of horizons: mse:0.18494927883148193, mae:0.321216881275177, smape:31.08033835887909, dtw:not calculated
mean smape over horizons:  31.08033910393715
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=8, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3955249786376953
Epoch: 1, Steps: 46 | Train Loss: 0.4133874 Vali Loss: 0.2613493 Test Loss: 0.2613493
Validation loss decreased (inf --> 0.261349).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.0965044498443604
Epoch: 2, Steps: 46 | Train Loss: 0.3022563 Vali Loss: 0.2513328 Test Loss: 0.2513328
Validation loss decreased (0.261349 --> 0.251333).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.0739450454711914
Epoch: 3, Steps: 46 | Train Loss: 0.2699013 Vali Loss: 0.2463326 Test Loss: 0.2463326
Validation loss decreased (0.251333 --> 0.246333).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.0344867706298828
Epoch: 4, Steps: 46 | Train Loss: 0.2236239 Vali Loss: 0.1828362 Test Loss: 0.1828362
Validation loss decreased (0.246333 --> 0.182836).  Saving model ...
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.0378077030181885
Epoch: 5, Steps: 46 | Train Loss: 0.2094349 Vali Loss: 0.1765417 Test Loss: 0.1765417
Validation loss decreased (0.182836 --> 0.176542).  Saving model ...
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.0135810375213623
Epoch: 6, Steps: 46 | Train Loss: 0.2161559 Vali Loss: 0.1830992 Test Loss: 0.1830992
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.1344881057739258
Epoch: 7, Steps: 46 | Train Loss: 0.2125649 Vali Loss: 0.1830691 Test Loss: 0.1830691
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 0.9171655178070068
Epoch: 8, Steps: 46 | Train Loss: 0.2064137 Vali Loss: 0.1792745 Test Loss: 0.1792745
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.026858985424041748, mae:0.12677866220474243, smape:12.645353376865387, dtw:not calculated
horizon:2 mse:0.05834833160042763, mae:0.18339388072490692, smape:18.176719546318054, dtw:not calculated
horizon:3 mse:0.08465693891048431, mae:0.21669605374336243, smape:21.36828303337097, dtw:not calculated
horizon:4 mse:0.1167212501168251, mae:0.2527365982532501, smape:24.75404441356659, dtw:not calculated
horizon:5 mse:0.15082904696464539, mae:0.2946597933769226, smape:28.74058485031128, dtw:not calculated
horizon:6 mse:0.17046697437763214, mae:0.3219341039657593, smape:31.35259747505188, dtw:not calculated
horizon:7 mse:0.18786503374576569, mae:0.34171950817108154, smape:33.2290917634964, dtw:not calculated
horizon:8 mse:0.22665032744407654, mae:0.37553584575653076, smape:36.317822337150574, dtw:not calculated
horizon:9 mse:0.2547827959060669, mae:0.3991292715072632, smape:38.44902515411377, dtw:not calculated
horizon:10 mse:0.28007107973098755, mae:0.4180236756801605, smape:40.10448753833771, dtw:not calculated
horizon:11 mse:0.2795364558696747, mae:0.4142444431781769, smape:39.71437215805054, dtw:not calculated
horizon:12 mse:0.28171268105506897, mae:0.40739819407463074, smape:38.94611299037933, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10131358355283737, mae:0.23269984126091003, smape:22.83959835767746, dtw:not calculated
average metrics: horizon upto:12 mse:0.17654165625572205, mae:0.3126875162124634, smape:30.316543579101562, dtw:not calculated
===============================================================================
average of horizons: mse:0.17654165625572205, mae:0.3126875162124634, smape:30.316543579101562, dtw:not calculated
mean smape over horizons:  30.31654121975104
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=10, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.453735589981079
Epoch: 1, Steps: 46 | Train Loss: 0.4702209 Vali Loss: 0.3178295 Test Loss: 0.3178295
Validation loss decreased (inf --> 0.317830).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 0.9288477897644043
Epoch: 2, Steps: 46 | Train Loss: 0.3337187 Vali Loss: 0.3454845 Test Loss: 0.3454845
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.1439895629882812
Epoch: 3, Steps: 46 | Train Loss: 0.2642365 Vali Loss: 0.3093676 Test Loss: 0.3093676
Validation loss decreased (0.317830 --> 0.309368).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 0.994098424911499
Epoch: 4, Steps: 46 | Train Loss: 0.2366487 Vali Loss: 0.2317750 Test Loss: 0.2317750
Validation loss decreased (0.309368 --> 0.231775).  Saving model ...
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.069674015045166
Epoch: 5, Steps: 46 | Train Loss: 0.2157106 Vali Loss: 0.1982496 Test Loss: 0.1982496
Validation loss decreased (0.231775 --> 0.198250).  Saving model ...
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.1068592071533203
Epoch: 6, Steps: 46 | Train Loss: 0.2127288 Vali Loss: 0.1933461 Test Loss: 0.1933461
Validation loss decreased (0.198250 --> 0.193346).  Saving model ...
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.103442668914795
Epoch: 7, Steps: 46 | Train Loss: 0.2104717 Vali Loss: 0.1952634 Test Loss: 0.1952634
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.054924488067627
Epoch: 8, Steps: 46 | Train Loss: 0.1951205 Vali Loss: 0.1892027 Test Loss: 0.1892027
Validation loss decreased (0.193346 --> 0.189203).  Saving model ...
Updating learning rate to 7.8125e-06
Epoch: 9 cost time: 1.0966520309448242
Epoch: 9, Steps: 46 | Train Loss: 0.2089616 Vali Loss: 0.1899210 Test Loss: 0.1899210
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.90625e-06
Epoch: 10 cost time: 1.010162353515625
Epoch: 10, Steps: 46 | Train Loss: 0.2148947 Vali Loss: 0.1884336 Test Loss: 0.1884336
Validation loss decreased (0.189203 --> 0.188434).  Saving model ...
Updating learning rate to 1.953125e-06
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.028926540166139603, mae:0.1283959150314331, smape:12.796346843242645, dtw:not calculated
horizon:2 mse:0.05881521850824356, mae:0.18467961251735687, smape:18.30976754426956, dtw:not calculated
horizon:3 mse:0.0968734622001648, mae:0.23037858307361603, smape:22.651207447052002, dtw:not calculated
horizon:4 mse:0.12611468136310577, mae:0.2653084695339203, smape:25.93376934528351, dtw:not calculated
horizon:5 mse:0.1595548540353775, mae:0.3020281493663788, smape:29.380899667739868, dtw:not calculated
horizon:6 mse:0.19597797095775604, mae:0.3376874625682831, smape:32.67838954925537, dtw:not calculated
horizon:7 mse:0.22720062732696533, mae:0.3629077076911926, smape:34.905990958213806, dtw:not calculated
horizon:8 mse:0.23047427833080292, mae:0.3695719838142395, smape:35.60940325260162, dtw:not calculated
horizon:9 mse:0.26618635654449463, mae:0.3991803228855133, smape:38.2384330034256, dtw:not calculated
horizon:10 mse:0.2784006595611572, mae:0.40472254157066345, smape:38.66458833217621, dtw:not calculated
horizon:11 mse:0.2952595055103302, mae:0.414643257856369, smape:39.46283757686615, dtw:not calculated
horizon:12 mse:0.29741835594177246, mae:0.4054546058177948, smape:38.42913806438446, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.111043781042099, mae:0.24141301214694977, smape:23.62506240606308, dtw:not calculated
average metrics: horizon upto:12 mse:0.18843355774879456, mae:0.3170798718929291, smape:30.58840036392212, dtw:not calculated
===============================================================================
average of horizons: mse:0.18843355774879456, mae:0.3170798718929291, smape:30.58840036392212, dtw:not calculated
mean smape over horizons:  30.588397632042568
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=10, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5095510482788086
Epoch: 1, Steps: 46 | Train Loss: 0.3921557 Vali Loss: 0.2311975 Test Loss: 0.2311975
Validation loss decreased (inf --> 0.231197).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.0700993537902832
Epoch: 2, Steps: 46 | Train Loss: 0.3300186 Vali Loss: 0.3564249 Test Loss: 0.3564249
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.1868855953216553
Epoch: 3, Steps: 46 | Train Loss: 0.2936883 Vali Loss: 0.2552472 Test Loss: 0.2552472
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.0607504844665527
Epoch: 4, Steps: 46 | Train Loss: 0.2297412 Vali Loss: 0.1835968 Test Loss: 0.1835968
Validation loss decreased (0.231197 --> 0.183597).  Saving model ...
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.043440818786621
Epoch: 5, Steps: 46 | Train Loss: 0.2127695 Vali Loss: 0.1804969 Test Loss: 0.1804969
Validation loss decreased (0.183597 --> 0.180497).  Saving model ...
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.0204777717590332
Epoch: 6, Steps: 46 | Train Loss: 0.2157157 Vali Loss: 0.1903433 Test Loss: 0.1903433
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.022325038909912
Epoch: 7, Steps: 46 | Train Loss: 0.2067472 Vali Loss: 0.1890114 Test Loss: 0.1890114
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 0.9931433200836182
Epoch: 8, Steps: 46 | Train Loss: 0.1889354 Vali Loss: 0.1962563 Test Loss: 0.1962563
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.03891100361943245, mae:0.16277964413166046, smape:16.2160262465477, dtw:not calculated
horizon:2 mse:0.06710416823625565, mae:0.19905509054660797, smape:19.715262949466705, dtw:not calculated
horizon:3 mse:0.095303475856781, mae:0.23275195062160492, smape:22.918933629989624, dtw:not calculated
horizon:4 mse:0.13790662586688995, mae:0.2792692482471466, smape:27.27908492088318, dtw:not calculated
horizon:5 mse:0.1524074226617813, mae:0.2986525297164917, smape:29.13983166217804, dtw:not calculated
horizon:6 mse:0.19156570732593536, mae:0.3352873623371124, smape:32.48621225357056, dtw:not calculated
horizon:7 mse:0.22178108990192413, mae:0.35751837491989136, smape:34.4372421503067, dtw:not calculated
horizon:8 mse:0.22753386199474335, mae:0.37128889560699463, smape:35.81832945346832, dtw:not calculated
horizon:9 mse:0.24783313274383545, mae:0.3939603865146637, smape:37.96125054359436, dtw:not calculated
horizon:10 mse:0.24022415280342102, mae:0.390903502702713, smape:37.764328718185425, dtw:not calculated
horizon:11 mse:0.2751995325088501, mae:0.4094932973384857, smape:39.220231771469116, dtw:not calculated
horizon:12 mse:0.2701927125453949, mae:0.41492170095443726, smape:39.8651659488678, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.1138663962483406, mae:0.25129932165145874, smape:24.62589144706726, dtw:not calculated
average metrics: horizon upto:12 mse:0.18049690127372742, mae:0.32049018144607544, smape:31.06848895549774, dtw:not calculated
===============================================================================
average of horizons: mse:0.18049690127372742, mae:0.32049018144607544, smape:31.06848895549774, dtw:not calculated
mean smape over horizons:  31.068491687377293
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=10, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4371957778930664
Epoch: 1, Steps: 46 | Train Loss: 0.4144682 Vali Loss: 0.3109557 Test Loss: 0.3109557
Validation loss decreased (inf --> 0.310956).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.124929666519165
Epoch: 2, Steps: 46 | Train Loss: 0.3045545 Vali Loss: 0.2086936 Test Loss: 0.2086936
Validation loss decreased (0.310956 --> 0.208694).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.01885986328125
Epoch: 3, Steps: 46 | Train Loss: 0.2742562 Vali Loss: 0.2286614 Test Loss: 0.2286614
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.0565171241760254
Epoch: 4, Steps: 46 | Train Loss: 0.2254679 Vali Loss: 0.1815974 Test Loss: 0.1815974
Validation loss decreased (0.208694 --> 0.181597).  Saving model ...
Updating learning rate to 0.000125
Epoch: 5 cost time: 0.9829754829406738
Epoch: 5, Steps: 46 | Train Loss: 0.2160857 Vali Loss: 0.1788534 Test Loss: 0.1788534
Validation loss decreased (0.181597 --> 0.178853).  Saving model ...
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 0.9987125396728516
Epoch: 6, Steps: 46 | Train Loss: 0.2108170 Vali Loss: 0.1832443 Test Loss: 0.1832443
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.067591667175293
Epoch: 7, Steps: 46 | Train Loss: 0.2194168 Vali Loss: 0.1887848 Test Loss: 0.1887848
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.017334222793579
Epoch: 8, Steps: 46 | Train Loss: 0.1974403 Vali Loss: 0.1879718 Test Loss: 0.1879718
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.025103909894824028, mae:0.11754000186920166, smape:11.71826347708702, dtw:not calculated
horizon:2 mse:0.05110294744372368, mae:0.1718553900718689, smape:17.061924934387207, dtw:not calculated
horizon:3 mse:0.09454147517681122, mae:0.22467200458049774, smape:22.10288792848587, dtw:not calculated
horizon:4 mse:0.1058279424905777, mae:0.23908673226833344, smape:23.461799323558807, dtw:not calculated
horizon:5 mse:0.1525907665491104, mae:0.29091769456863403, smape:28.309649229049683, dtw:not calculated
horizon:6 mse:0.17239142954349518, mae:0.31109902262687683, smape:30.190566182136536, dtw:not calculated
horizon:7 mse:0.19385813176631927, mae:0.3289802372455597, smape:31.81743025779724, dtw:not calculated
horizon:8 mse:0.2377948760986328, mae:0.3688315153121948, smape:35.47407686710358, dtw:not calculated
horizon:9 mse:0.2616465985774994, mae:0.38013744354248047, smape:36.303991079330444, dtw:not calculated
horizon:10 mse:0.24856311082839966, mae:0.36675193905830383, smape:35.07428169250488, dtw:not calculated
horizon:11 mse:0.30168408155441284, mae:0.4012344777584076, smape:37.943604588508606, dtw:not calculated
horizon:12 mse:0.3011360764503479, mae:0.3961045742034912, smape:37.43821084499359, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10025974363088608, mae:0.2258618324995041, smape:22.140847146511078, dtw:not calculated
average metrics: horizon upto:12 mse:0.17885343730449677, mae:0.2997675836086273, smape:28.908061981201172, dtw:not calculated
===============================================================================
average of horizons: mse:0.17885343730449677, mae:0.2997675836086273, smape:28.908061981201172, dtw:not calculated
mean smape over horizons:  28.908057200411957
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=10, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.5066771507263184
Epoch: 1, Steps: 46 | Train Loss: 0.3864931 Vali Loss: 0.2169592 Test Loss: 0.2169592
Validation loss decreased (inf --> 0.216959).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.0919981002807617
Epoch: 2, Steps: 46 | Train Loss: 0.3230859 Vali Loss: 0.2107306 Test Loss: 0.2107306
Validation loss decreased (0.216959 --> 0.210731).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 0.9535562992095947
Epoch: 3, Steps: 46 | Train Loss: 0.2604466 Vali Loss: 0.1807512 Test Loss: 0.1807512
Validation loss decreased (0.210731 --> 0.180751).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.0157959461212158
Epoch: 4, Steps: 46 | Train Loss: 0.2271664 Vali Loss: 0.1587279 Test Loss: 0.1587279
Validation loss decreased (0.180751 --> 0.158728).  Saving model ...
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.0132384300231934
Epoch: 5, Steps: 46 | Train Loss: 0.2113257 Vali Loss: 0.1454085 Test Loss: 0.1454085
Validation loss decreased (0.158728 --> 0.145409).  Saving model ...
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.059056043624878
Epoch: 6, Steps: 46 | Train Loss: 0.2068003 Vali Loss: 0.1448118 Test Loss: 0.1448118
Validation loss decreased (0.145409 --> 0.144812).  Saving model ...
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.0221588611602783
Epoch: 7, Steps: 46 | Train Loss: 0.2029450 Vali Loss: 0.1418649 Test Loss: 0.1418649
Validation loss decreased (0.144812 --> 0.141865).  Saving model ...
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.1351971626281738
Epoch: 8, Steps: 46 | Train Loss: 0.1963002 Vali Loss: 0.1463974 Test Loss: 0.1463974
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-06
Epoch: 9 cost time: 0.9875848293304443
Epoch: 9, Steps: 46 | Train Loss: 0.1981769 Vali Loss: 0.1476940 Test Loss: 0.1476940
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-06
Epoch: 10 cost time: 1.0687317848205566
Epoch: 10, Steps: 46 | Train Loss: 0.2055883 Vali Loss: 0.1471545 Test Loss: 0.1471545
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.020543420687317848, mae:0.10457747429609299, smape:10.43526977300644, dtw:not calculated
horizon:2 mse:0.041604556143283844, mae:0.1500135213136673, smape:14.900347590446472, dtw:not calculated
horizon:3 mse:0.06586907058954239, mae:0.18428125977516174, smape:18.198277056217194, dtw:not calculated
horizon:4 mse:0.09599792212247849, mae:0.22507640719413757, smape:22.11187183856964, dtw:not calculated
horizon:5 mse:0.12105301767587662, mae:0.2604095935821533, smape:25.487497448921204, dtw:not calculated
horizon:6 mse:0.14683791995048523, mae:0.28869378566741943, smape:28.13432812690735, dtw:not calculated
horizon:7 mse:0.16588212549686432, mae:0.31346023082733154, smape:30.514883995056152, dtw:not calculated
horizon:8 mse:0.17698194086551666, mae:0.3299887180328369, smape:32.103654742240906, dtw:not calculated
horizon:9 mse:0.1961812674999237, mae:0.3459412157535553, smape:33.56396555900574, dtw:not calculated
horizon:10 mse:0.21744543313980103, mae:0.35942304134368896, smape:34.718143939971924, dtw:not calculated
horizon:11 mse:0.22713448107242584, mae:0.3708757758140564, smape:35.836607217788696, dtw:not calculated
horizon:12 mse:0.22684814035892487, mae:0.36856329441070557, smape:35.55845022201538, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.08198432624340057, mae:0.2021753191947937, smape:19.877932965755463, dtw:not calculated
average metrics: horizon upto:12 mse:0.14186494052410126, mae:0.27510866522789, smape:26.796942949295044, dtw:not calculated
===============================================================================
average of horizons: mse:0.14186494052410126, mae:0.27510866522789, smape:26.796942949295044, dtw:not calculated
mean smape over horizons:  26.796941459178925
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=12, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.4238088130950928
Epoch: 1, Steps: 46 | Train Loss: 0.4379758 Vali Loss: 0.3752271 Test Loss: 0.3752271
Validation loss decreased (inf --> 0.375227).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 0.9940986633300781
Epoch: 2, Steps: 46 | Train Loss: 0.3109927 Vali Loss: 0.2985780 Test Loss: 0.2985780
Validation loss decreased (0.375227 --> 0.298578).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.0762240886688232
Epoch: 3, Steps: 46 | Train Loss: 0.2464967 Vali Loss: 0.1719549 Test Loss: 0.1719549
Validation loss decreased (0.298578 --> 0.171955).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 0.9416158199310303
Epoch: 4, Steps: 46 | Train Loss: 0.2217132 Vali Loss: 0.1904802 Test Loss: 0.1904802
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.1156320571899414
Epoch: 5, Steps: 46 | Train Loss: 0.2113431 Vali Loss: 0.1712721 Test Loss: 0.1712721
Validation loss decreased (0.171955 --> 0.171272).  Saving model ...
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.1191959381103516
Epoch: 6, Steps: 46 | Train Loss: 0.2045338 Vali Loss: 0.1720292 Test Loss: 0.1720292
EarlyStopping counter: 1 out of 3
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.0521094799041748
Epoch: 7, Steps: 46 | Train Loss: 0.1993865 Vali Loss: 0.1728164 Test Loss: 0.1728164
EarlyStopping counter: 2 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.1012439727783203
Epoch: 8, Steps: 46 | Train Loss: 0.1935575 Vali Loss: 0.1736783 Test Loss: 0.1736783
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.0272165909409523, mae:0.12796054780483246, smape:12.760725617408752, dtw:not calculated
horizon:2 mse:0.05238359048962593, mae:0.17139944434165955, smape:17.004524171352386, dtw:not calculated
horizon:3 mse:0.08564043045043945, mae:0.22614619135856628, smape:22.325417399406433, dtw:not calculated
horizon:4 mse:0.1210244745016098, mae:0.2687341570854187, smape:26.367276906967163, dtw:not calculated
horizon:5 mse:0.1468779593706131, mae:0.2893929183483124, smape:28.246459364891052, dtw:not calculated
horizon:6 mse:0.18104258179664612, mae:0.33763593435287476, smape:32.87835121154785, dtw:not calculated
horizon:7 mse:0.20695480704307556, mae:0.3391515016555786, smape:32.735493779182434, dtw:not calculated
horizon:8 mse:0.23183749616146088, mae:0.36665239930152893, smape:35.32773554325104, dtw:not calculated
horizon:9 mse:0.24056129157543182, mae:0.3719951808452606, smape:35.76676547527313, dtw:not calculated
horizon:10 mse:0.26588571071624756, mae:0.3765241205692291, smape:35.91276705265045, dtw:not calculated
horizon:11 mse:0.24781225621700287, mae:0.37286707758903503, smape:35.75282692909241, dtw:not calculated
horizon:12 mse:0.24802789092063904, mae:0.3701877295970917, smape:35.47178506851196, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10236427187919617, mae:0.2368782013654709, smape:23.26379120349884, dtw:not calculated
average metrics: horizon upto:12 mse:0.17127208411693573, mae:0.3015539348125458, smape:29.212510585784912, dtw:not calculated
===============================================================================
average of horizons: mse:0.17127208411693573, mae:0.3015539348125458, smape:29.212510585784912, dtw:not calculated
mean smape over horizons:  29.212510709961254
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=12, stride=6
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3781943321228027
Epoch: 1, Steps: 46 | Train Loss: 0.4503775 Vali Loss: 0.2591816 Test Loss: 0.2591816
Validation loss decreased (inf --> 0.259182).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.001265048980713
Epoch: 2, Steps: 46 | Train Loss: 0.3336866 Vali Loss: 0.2830028 Test Loss: 0.2830028
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.0236515998840332
Epoch: 3, Steps: 46 | Train Loss: 0.2577089 Vali Loss: 0.2022922 Test Loss: 0.2022922
Validation loss decreased (0.259182 --> 0.202292).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.010697364807129
Epoch: 4, Steps: 46 | Train Loss: 0.2155181 Vali Loss: 0.2043066 Test Loss: 0.2043066
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000125
Epoch: 5 cost time: 0.9930171966552734
Epoch: 5, Steps: 46 | Train Loss: 0.2097895 Vali Loss: 0.2203927 Test Loss: 0.2203927
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.0689566135406494
Epoch: 6, Steps: 46 | Train Loss: 0.2111040 Vali Loss: 0.1897728 Test Loss: 0.1897728
Validation loss decreased (0.202292 --> 0.189773).  Saving model ...
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.036834955215454
Epoch: 7, Steps: 46 | Train Loss: 0.1960159 Vali Loss: 0.2016798 Test Loss: 0.2016798
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.029745101928711
Epoch: 8, Steps: 46 | Train Loss: 0.1923870 Vali Loss: 0.1975120 Test Loss: 0.1975120
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-06
Epoch: 9 cost time: 1.0131292343139648
Epoch: 9, Steps: 46 | Train Loss: 0.1983563 Vali Loss: 0.2000294 Test Loss: 0.2000294
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.029393808916211128, mae:0.13343720138072968, smape:13.299886882305145, dtw:not calculated
horizon:2 mse:0.054092321544885635, mae:0.1799294352531433, smape:17.86198318004608, dtw:not calculated
horizon:3 mse:0.08193797618150711, mae:0.21802684664726257, smape:21.52377963066101, dtw:not calculated
horizon:4 mse:0.1228405013680458, mae:0.25809574127197266, smape:25.246211886405945, dtw:not calculated
horizon:5 mse:0.1477113515138626, mae:0.2866736948490143, smape:27.92721688747406, dtw:not calculated
horizon:6 mse:0.19119857251644135, mae:0.321225106716156, smape:31.03318214416504, dtw:not calculated
horizon:7 mse:0.23199716210365295, mae:0.3562755286693573, smape:34.198468923568726, dtw:not calculated
horizon:8 mse:0.2344398945569992, mae:0.3669995367527008, smape:35.3085458278656, dtw:not calculated
horizon:9 mse:0.27551719546318054, mae:0.403746634721756, smape:38.623008131980896, dtw:not calculated
horizon:10 mse:0.28868648409843445, mae:0.4067581593990326, smape:38.755664229393005, dtw:not calculated
horizon:11 mse:0.3098721206188202, mae:0.4070957601070404, smape:38.43733072280884, dtw:not calculated
horizon:12 mse:0.3095865249633789, mae:0.4126824736595154, smape:39.06716704368591, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.10452909022569656, mae:0.23289799690246582, smape:22.81537652015686, dtw:not calculated
average metrics: horizon upto:12 mse:0.189772829413414, mae:0.31257882714271545, smape:30.106869339942932, dtw:not calculated
===============================================================================
average of horizons: mse:0.189772829413414, mae:0.31257882714271545, smape:30.106869339942932, dtw:not calculated
mean smape over horizons:  30.10687045753002
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=12, stride=8
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.411257266998291
Epoch: 1, Steps: 46 | Train Loss: 0.4077044 Vali Loss: 0.2366996 Test Loss: 0.2366996
Validation loss decreased (inf --> 0.236700).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.116440773010254
Epoch: 2, Steps: 46 | Train Loss: 0.3235041 Vali Loss: 0.1860404 Test Loss: 0.1860404
Validation loss decreased (0.236700 --> 0.186040).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.0438532829284668
Epoch: 3, Steps: 46 | Train Loss: 0.2604032 Vali Loss: 0.1756994 Test Loss: 0.1756994
Validation loss decreased (0.186040 --> 0.175699).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.032301902770996
Epoch: 4, Steps: 46 | Train Loss: 0.2217596 Vali Loss: 0.1880690 Test Loss: 0.1880690
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000125
Epoch: 5 cost time: 0.9745757579803467
Epoch: 5, Steps: 46 | Train Loss: 0.2010135 Vali Loss: 0.1584783 Test Loss: 0.1584783
Validation loss decreased (0.175699 --> 0.158478).  Saving model ...
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.0457966327667236
Epoch: 6, Steps: 46 | Train Loss: 0.2073650 Vali Loss: 0.1582287 Test Loss: 0.1582287
Validation loss decreased (0.158478 --> 0.158229).  Saving model ...
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.0859262943267822
Epoch: 7, Steps: 46 | Train Loss: 0.2105065 Vali Loss: 0.1669134 Test Loss: 0.1669134
EarlyStopping counter: 1 out of 3
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.0443828105926514
Epoch: 8, Steps: 46 | Train Loss: 0.1972840 Vali Loss: 0.1615837 Test Loss: 0.1615837
EarlyStopping counter: 2 out of 3
Updating learning rate to 7.8125e-06
Epoch: 9 cost time: 1.0043158531188965
Epoch: 9, Steps: 46 | Train Loss: 0.2062587 Vali Loss: 0.1647410 Test Loss: 0.1647410
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.023345617577433586, mae:0.11985902488231659, smape:11.964182555675507, dtw:not calculated
horizon:2 mse:0.05021730437874794, mae:0.1688464730978012, smape:16.763557493686676, dtw:not calculated
horizon:3 mse:0.07848194241523743, mae:0.21082742512226105, smape:20.81761211156845, dtw:not calculated
horizon:4 mse:0.10607250779867172, mae:0.2357024997472763, smape:23.110434412956238, dtw:not calculated
horizon:5 mse:0.13826020061969757, mae:0.28134649991989136, smape:27.47964859008789, dtw:not calculated
horizon:6 mse:0.16713419556617737, mae:0.2973126769065857, smape:28.817132115364075, dtw:not calculated
horizon:7 mse:0.17462405562400818, mae:0.3120113015174866, smape:30.274662375450134, dtw:not calculated
horizon:8 mse:0.20236049592494965, mae:0.33715203404426575, smape:32.55200386047363, dtw:not calculated
horizon:9 mse:0.22773587703704834, mae:0.35629966855049133, smape:34.227967262268066, dtw:not calculated
horizon:10 mse:0.2563232481479645, mae:0.37505555152893066, smape:35.81936061382294, dtw:not calculated
horizon:11 mse:0.2318892776966095, mae:0.36596229672431946, smape:35.1927787065506, dtw:not calculated
horizon:12 mse:0.24229951202869415, mae:0.35633549094200134, smape:33.987778425216675, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09391863644123077, mae:0.21898241341114044, smape:21.492093801498413, dtw:not calculated
average metrics: horizon upto:12 mse:0.15822866559028625, mae:0.2847259044647217, smape:27.583923935890198, dtw:not calculated
===============================================================================
average of horizons: mse:0.15822866559028625, mae:0.2847259044647217, smape:27.583923935890198, dtw:not calculated
mean smape over horizons:  27.583926543593407
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=12, stride=10
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3490681648254395
Epoch: 1, Steps: 46 | Train Loss: 0.3670577 Vali Loss: 0.2201340 Test Loss: 0.2201340
Validation loss decreased (inf --> 0.220134).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 0.9823253154754639
Epoch: 2, Steps: 46 | Train Loss: 0.3038047 Vali Loss: 0.2026664 Test Loss: 0.2026664
Validation loss decreased (0.220134 --> 0.202666).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 0.9965128898620605
Epoch: 3, Steps: 46 | Train Loss: 0.2522058 Vali Loss: 0.1964297 Test Loss: 0.1964297
Validation loss decreased (0.202666 --> 0.196430).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 0.9133272171020508
Epoch: 4, Steps: 46 | Train Loss: 0.2100598 Vali Loss: 0.1738574 Test Loss: 0.1738574
Validation loss decreased (0.196430 --> 0.173857).  Saving model ...
Updating learning rate to 0.000125
Epoch: 5 cost time: 0.9925382137298584
Epoch: 5, Steps: 46 | Train Loss: 0.1999146 Vali Loss: 0.1838632 Test Loss: 0.1838632
EarlyStopping counter: 1 out of 3
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.0546016693115234
Epoch: 6, Steps: 46 | Train Loss: 0.2040470 Vali Loss: 0.1789632 Test Loss: 0.1789632
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.0218467712402344
Epoch: 7, Steps: 46 | Train Loss: 0.2074319 Vali Loss: 0.1773435 Test Loss: 0.1773435
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.027159199118614197, mae:0.12258217483758926, smape:12.217269092798233, dtw:not calculated
horizon:2 mse:0.04481633007526398, mae:0.156683549284935, smape:15.558025240898132, dtw:not calculated
horizon:3 mse:0.08307255059480667, mae:0.2040419578552246, smape:20.092031359672546, dtw:not calculated
horizon:4 mse:0.09836222976446152, mae:0.2216046154499054, smape:21.738718450069427, dtw:not calculated
horizon:5 mse:0.13035158812999725, mae:0.2677917778491974, smape:26.16875171661377, dtw:not calculated
horizon:6 mse:0.16972409188747406, mae:0.3072459399700165, smape:29.818177223205566, dtw:not calculated
horizon:7 mse:0.1824609488248825, mae:0.319009006023407, smape:30.925309658050537, dtw:not calculated
horizon:8 mse:0.24794995784759521, mae:0.36573824286460876, smape:34.99610722064972, dtw:not calculated
horizon:9 mse:0.2489195168018341, mae:0.3719397783279419, smape:35.64434349536896, dtw:not calculated
horizon:10 mse:0.2696112096309662, mae:0.3912680149078369, smape:37.409377098083496, dtw:not calculated
horizon:11 mse:0.279979407787323, mae:0.3914593756198883, smape:37.26657330989838, dtw:not calculated
horizon:12 mse:0.3038814961910248, mae:0.412492573261261, smape:39.114975929260254, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09224765747785568, mae:0.21332499384880066, smape:20.932160317897797, dtw:not calculated
average metrics: horizon upto:12 mse:0.17385737597942352, mae:0.2943214178085327, smape:28.41247022151947, dtw:not calculated
===============================================================================
average of horizons: mse:0.17385737597942352, mae:0.2943214178085327, smape:28.41247022151947, dtw:not calculated
mean smape over horizons:  28.412471649547417
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=12, stride=12
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.428386926651001
Epoch: 1, Steps: 46 | Train Loss: 0.3774972 Vali Loss: 0.2134715 Test Loss: 0.2134715
Validation loss decreased (inf --> 0.213471).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.0467402935028076
Epoch: 2, Steps: 46 | Train Loss: 0.2968467 Vali Loss: 0.2046592 Test Loss: 0.2046592
Validation loss decreased (0.213471 --> 0.204659).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.0501368045806885
Epoch: 3, Steps: 46 | Train Loss: 0.2500007 Vali Loss: 0.1796678 Test Loss: 0.1796678
Validation loss decreased (0.204659 --> 0.179668).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 0.9485664367675781
Epoch: 4, Steps: 46 | Train Loss: 0.2161589 Vali Loss: 0.1940581 Test Loss: 0.1940581
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.034869909286499
Epoch: 5, Steps: 46 | Train Loss: 0.2138905 Vali Loss: 0.1877752 Test Loss: 0.1877752
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.012120008468628
Epoch: 6, Steps: 46 | Train Loss: 0.2115831 Vali Loss: 0.1791861 Test Loss: 0.1791861
Validation loss decreased (0.179668 --> 0.179186).  Saving model ...
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.08231520652771
Epoch: 7, Steps: 46 | Train Loss: 0.2091511 Vali Loss: 0.1764984 Test Loss: 0.1764984
Validation loss decreased (0.179186 --> 0.176498).  Saving model ...
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 1.053013801574707
Epoch: 8, Steps: 46 | Train Loss: 0.1952248 Vali Loss: 0.1800698 Test Loss: 0.1800698
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-06
Epoch: 9 cost time: 1.1437098979949951
Epoch: 9, Steps: 46 | Train Loss: 0.2005211 Vali Loss: 0.1801912 Test Loss: 0.1801912
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-06
Epoch: 10 cost time: 1.0823755264282227
Epoch: 10, Steps: 46 | Train Loss: 0.2067232 Vali Loss: 0.1809848 Test Loss: 0.1809848
EarlyStopping counter: 3 out of 3
Early stopping
>>>>>>>testing : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
test shape: (182, 12, 1) (182, 12, 1)
test shape: (182, 12, 1) (182, 12, 1)
horizon:1 mse:0.02057368867099285, mae:0.11065194010734558, smape:11.044424027204514, dtw:not calculated
horizon:2 mse:0.04874265938997269, mae:0.1649007350206375, smape:16.36689007282257, dtw:not calculated
horizon:3 mse:0.07860361039638519, mae:0.20868857204914093, smape:20.58938592672348, dtw:not calculated
horizon:4 mse:0.1094757467508316, mae:0.2369982898235321, smape:23.209531605243683, dtw:not calculated
horizon:5 mse:0.1451101154088974, mae:0.2749198377132416, smape:26.754996180534363, dtw:not calculated
horizon:6 mse:0.18032243847846985, mae:0.3109077513217926, smape:30.08561134338379, dtw:not calculated
horizon:7 mse:0.210840106010437, mae:0.35248053073883057, smape:34.06450152397156, dtw:not calculated
horizon:8 mse:0.21912679076194763, mae:0.36581289768218994, smape:35.34044623374939, dtw:not calculated
horizon:9 mse:0.25898754596710205, mae:0.3973742425441742, smape:38.173362612724304, dtw:not calculated
horizon:10 mse:0.2703956663608551, mae:0.3983416259288788, smape:38.10487389564514, dtw:not calculated
horizon:11 mse:0.29661500453948975, mae:0.41324976086616516, smape:39.273226261138916, dtw:not calculated
horizon:12 mse:0.27918705344200134, mae:0.39814767241477966, smape:37.938740849494934, dtw:not calculated
===============================================================================
average metrics: horizon upto:6 mse:0.09713803976774216, mae:0.21784451603889465, smape:21.341806650161743, dtw:not calculated
average metrics: horizon upto:12 mse:0.17649836838245392, mae:0.30270615220069885, smape:29.245498776435852, dtw:not calculated
===============================================================================
average of horizons: mse:0.17649836838245392, mae:0.30270615220069885, smape:29.245498776435852, dtw:not calculated
mean smape over horizons:  29.245499211053055
Running with parameters: d_model=256, n_heads=1, e_layers=2, d_ff=256, batch_size=16, learning_rate=0.001, patch_len=14, stride=4
True
Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           tmmodelm2           Model:              PatchTST            

[1mData Loader[0m
  Data:               custom              Root Path:          ./dataset/illness/  
  Data Path:          national_illness_3cols.csvFeatures:           MS                  
  Target:             ILITOTAL            Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          0                   
  Pred Len:           12                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            256                 
  n heads:            1                   e layers:           2                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                

[1mRun Parameters[0m
  Num Workers:        20                  Itr:                1                   
  Train Epochs:       10                  Batch Size:         16                  
  Patience:           3                   Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_tmmodelm2_PatchTST_custom_ftMS_sl36_ll0_pl12_dm256_nh1_el2_dl1_df256_expand2_dc4_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
train 725
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
num_train: 772
num_test: 193
border1s: [0, 736, 737]
border2s: [772, 773, 966]
test 182
Epoch: 1 cost time: 1.3855903148651123
Epoch: 1, Steps: 46 | Train Loss: 0.4290511 Vali Loss: 0.3842795 Test Loss: 0.3842795
Validation loss decreased (inf --> 0.384280).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 0.916163444519043
Epoch: 2, Steps: 46 | Train Loss: 0.3385739 Vali Loss: 0.2394714 Test Loss: 0.2394714
Validation loss decreased (0.384280 --> 0.239471).  Saving model ...
Updating learning rate to 0.0005
Epoch: 3 cost time: 1.0055344104766846
Epoch: 3, Steps: 46 | Train Loss: 0.2605341 Vali Loss: 0.1775205 Test Loss: 0.1775205
Validation loss decreased (0.239471 --> 0.177520).  Saving model ...
Updating learning rate to 0.00025
Epoch: 4 cost time: 1.0170955657958984
Epoch: 4, Steps: 46 | Train Loss: 0.2317797 Vali Loss: 0.2157681 Test Loss: 0.2157681
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000125
Epoch: 5 cost time: 1.0002970695495605
Epoch: 5, Steps: 46 | Train Loss: 0.2191054 Vali Loss: 0.1879721 Test Loss: 0.1879721
EarlyStopping counter: 2 out of 3
Updating learning rate to 6.25e-05
Epoch: 6 cost time: 1.1002144813537598
Epoch: 6, Steps: 46 | Train Loss: 0.2133097 Vali Loss: 0.1725115 Test Loss: 0.1725115
Validation loss decreased (0.177520 --> 0.172512).  Saving model ...
Updating learning rate to 3.125e-05
Epoch: 7 cost time: 1.0669291019439697
Epoch: 7, Steps: 46 | Train Loss: 0.2061528 Vali Loss: 0.1655497 Test Loss: 0.1655497
Validation loss decreased (0.172512 --> 0.165550).  Saving model ...
Updating learning rate to 1.5625e-05
Epoch: 8 cost time: 0.9509835243225098
Epoch: 8, Steps: 46 | Train Loss: 0.2036370 Vali Loss: 0.1695441 Test Loss: 0.1695441
EarlyStopping counter: 1 out of 3
Updating learning rate to 7.8125e-06
Epoch: 9 cost time: 1.0138359069824219
Epoch: 9, Steps: 46 | Train Loss: 0.2015129 Vali Loss: 0.1689844 Test Loss: 0.1689844
EarlyStopping counter: 2 out of 3
Updating learning rate to 3.90625e-06
